{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Background Operation ID is a ministry that exists to serve the need of individuals who need to obtain identification documents to support their application for a variety of services from housing to job application. The ministry is volunteer driven and has been in place for over thirty years. During this time thousands of individuals have received help to support their identification needs. Help is provided in the form of vouchers that can be used to pay for services rendered by either the Texas Department of Public Safety for a Texas ID or by the Bureau of Vital Statistics of Texas or another state to acquire a certified copy of a birth certificate. Over the years of Operation ID's existence various paper driven pipelines of client processing in providing identification services have been used. The currently existing pipeline consists of four stages which are executed sequentially. Screening Check In Interviewing Back Office Voucher Writing In the Screening stage, a client is greeted at the front desk by a volunteer who checks that the client's referral letter is valid and that the documents the client possesses support the services they are seeking. The referral letter is provided by the agency representing the client to Operation ID and this agency performs the first level of screening. If this first level of screening is done carefully, the Screening stage at Operation ID can proceed very quickly, allowing the client to proceed to the Check In stage. The referral letter must specify the services that a client is seeking at Operation ID. In the Check In stage (also performed by a volunteer at the front desk) the client is entered into the Operation ID database if he/she is not already in the database. The database is maintained by the Apricot database driven application software provided by the vendor Social Solutions . Apricot is used to capture a client's demographic data together with a history of a client's previous visits to Operation ID. The demographic data includes a client's name, date of birth and age (a calculated field based on date of birth). The demographic data also includes a link to a household table, if a client is part of a household being processed together. Apricot stores basic demographic data in a table it refers to as the Client Tracking table and stores client visits in a related table called the Client Tracking Document Folder. (In terms of database technology, the Client Tracking Document Folder is related to the Client Tracking table by a foreign key.) Operation ID makes use of the Client Tracking Document Folder to record services provided to a client during previous visits. Whenever a service is provided to a client (example: writing a check for a birth certificate), the service is recorded by date together with the number of any issued check. At a date after the date of service, the disposition of an issued check is recorded. Most often this disposition will be either Cleared, indicating that the check has been used by the client or Voided, indicating that the check is no longer valid. During the Check In stage at the front desk, a client's history of previous visits (if any) is manually copied onto a small slip of paper which is then stapled to the client's referral letter. Previous visits may make a client ineligible for a service being requested. Operation ID enforces a twice-in-a-lifetime policy for birth certificate or ID services. If a client's history of previous visits reveals that they have twice previously been issued checks for a birth certificate and both checks have been used, then the client is not eligible for another check for a birth certificate. Similarly, if a client has twice previously been issued checks for an ID (either a Texas ID or a Texas Driver's License) and both checks have been used, then they are not eligible for another check for an ID. If a client is ineligible for a requested service, then they are so informed at the front desk. A client ineligible for a requested service may appeal their ineligibility and may, at the discretion of Operation ID management, be granted an exception. Most clients are either first time clients (no previous visit history) or clients who have not yet exhausted eligibility for a requested service. In either case, such a client is ready for the Interviewing stage. A client ready to be interviewed may have to wait for a (volunteer) interviewer to become available. Clients are processed on a first-come-first-served basis and frequently have to wait for an available interviewer. When an interviewer picks up a waiting client, the client's requested services are reviewed and paperwork in support of these services is completed. If the client is representing themselves alone, the paperwork generation may go fairly quickly. If the client is requesting an out-of-state birth certificate, more time is required, because the paperwork required to acquire a certified birth certificate varies from state to state. If the client represents a household, then paperwork for each member of the household is required and this may slow down the interviewing process considerably. In any event, under the current client processing pipeline, all paperwork required is completed during the interviewing process before the back office can generate any voucher for a client. When paperwork filled out during the Interviewing stage is sent to the back office, the Back Office Voucher Writing stage may begin. This stage may not begin immediately upon the delivery of the completed paperwork for a given client, because a backlog of previous clients may need to be processed first. When a client's paperwork is eventually processed by the back office, what remains to be done is straightforward. A voucher for each service requested by a client is first generated by use of Quickbooks (operated by a volunteer). Then the voucher number of each generated voucher is recorded in the Apricot database (by another volunteer) in the client's visit history. This record keeping step is what enables enforcement of the twice-in-a-lifetime service policy mentioned above. After vouchers have been generated and recorded, they are delivered to the client and the pipeline of client processing is completed. What has been described above may be referred to as the same-day-service model of client processing. An alternative model, Remote Operation ID, will be described in a section below. Remote Operation ID will be described as a virtual platform providing the same client services without the need for a client to appear at Operation ID. The OPIDDaily Website The OPIDDaily website was designed to partially automate the pipeline of client processing at Operation ID by allowing the back office production of vouchers to proceed in parallel with the interviewing of clients to generate paperwork supporting their needs. OPIDDaily takes advantage of the fact that a client's voucher needs are already known during the Screening and Check In stages. This allows the front desk volunteers to send the back office volunteers information which allows the Back Office Voucher Writing stage to begin, possibly even before the Interviewing Stage has started. In support of parallel processing using OPIDDaily a Service Ticket describing a client's previous visits (if any) together with the services the client is seeking is printed on a printer at the front desk by the front desk check in volunteer. It has been suggested that in straightforward cases this Service Ticket be sent directly to the back office so that back office volunteers could begin processing the service requested on the Service Ticket in advance of the Interviewing stage being completed. In effect, the Interviewing stage and the Back Office Voucher Writing stage would be run in parallel. This has the potential of delivering substantial time savings over the currently existing sequential pipeline. The Screening and Check In stages would require a small amount of additional time in order to generate the Service Ticket for a client, but this extra time would be more than compensated for by the ability to run the Interviewing and Back Office Voucher Writing stages in parallel. In practice, a Service Ticket is not issued for each client appearing at the front desk on a day of service. Instead, in order to keep the line moving, a Service Ticket is only generated for clients with a history of previous visits. Clients with no previous visits are quickly moved through the front desk after their name and date of birth have been recorded in the Apricot database. The Service Ticket generated for a client with a visit history is attached to the client's referral letter rather than being sent to the back office. This does not take advantage of the parallel processing mentioned above, but it has nevertheless proved to be valuable. A client's referral letter must contain the services the client is seeking. A Service Ticket for a client with previous history summarizes the requested services in a tabular format. Remote Operation ID In March 2020 Operation ID was shutdown due to the coronavirus pandemic. At the time of this writing (April 2020) Operation ID is still shut down and it is not known when it will reopen. When the shutdown began, work was begun on a simple extension of OPIDDaily that would allow for providing identification services remotely. Remote operation is the analog of the same-day-service model of identification services that have historically been provided by Operation ID by a virtual platform that allows case managers to submit service requests to Operation ID through the OPIDDaily website. The service request is an electronic replacement for the referral letter used in the same-day-service model described above. Under the same-day-service model, there is only a single logged in user, the TicketMaster . The TicketMaster is a volunteer who sits at the front desk and uses OPIDDaily to generate a Service Ticket for each client with a history of previous visits. In the remote operations model, case managers who work at Operation ID partner agencies login to the OPIDDaily website to submit service requests on behalf of their clients. The last step of generating a service request for a client is the printing and signing of the Case Manager Voucher . The Case Manager Voucher is a piece of paper describing a service request. It includes a disclaimer that makes the client aware that requested services may be denied because of previous visits to Operation ID for the same services. The Case Manager Voucher must be signed by both the client and the case manager. The voucher will be retained by the case manager, who will later redeem it for checks covering the expenses of the requested services. A service request submitted by a case manager appears on a service requests dashboard monitored by an Operation ID employee. The dashboard is simply a list of service requests placed by case managers working at different agencies. When a new service request appears on the dashboard, its arrival is first acknowledged by checking a checkbox on the request. This acknowledgment will be seen by the case manager who placed the request, informing him/her that Operation ID is aware of the request. The service request will next be used to generate a Service Ticket which will be printed and added to a collection of outstanding Service Tickets. When Service Tickets representing enough clients have been accumulated, the status of each represented client will be changed to BackOffice, indicating that the service requests have been passed to the back office at Operation ID for processing. The respective case managers will see this change in status for their clients. Processing a Service Ticket in the back office is exactly the same process that takes place in the same-day-service model. It involves using Quickbooks to cut checks for services specified by a Service Ticket and recording the check numbers in the corresponding client database entry in the Apricot database. This processing will be handled by an Operation ID volunteer, as is done in the same-day-service model. Once the check(s) have been cut for a client, the Operation ID employee monitoring the dashboard changes the client's status to Done, indicating to the client's case manager that processing has completed and checks may be picked up at the Operation ID office. The checks will be released to a case manager as redemption of the corresponding Case Manager Voucher. The process of remote operations described above involves only the case manager and Operation ID personnel. A client works with his/her case manager to generate a service request and then waits for the checks to be picked up at Operation ID by the case manager. Unlike the same-day-service model, the client never appears at Operation ID. This will be especially important as long as the same-day-service operation remains shut down. The Remote Operation ID process clearly requires more time to complete than the same-day-service model. As a practical matter, a Case Manager Voucher will indicate a 30 day period of validity. This is actually a time limit on how long Operation ID is given to respond to the corresponding service request once it appears on the service requests dashboard. The dashboard list will be sorted in chronological order of service request expiration dates. A service request will automatically roll off the dashboard once its expiration date has passed. Service History A Service Ticket lists not only the services a client is requesting but also the services the client has received in prior visits (if any). Listing previous services rendered is made possible by including a table of previously issued checks in the OPIDDaily database. Each record in the table of checks contains a check number, the check purpose, the name of the client to whom the check was issued and the disposition of the check, whether it was Cleared by the bank, Voided by the bank or one of several dispositions private to Operation ID. A check record does not include who the payee of the check is or the amount of the check. The check is referenced to the Apricot database simply by its unique number, not by the name of the client to whom the check was given. Periodically a pair of back office reports is processed by OPIDDaily to update its table of checks with the bank declared disposition of checks. These reports are generated by Quickbooks. One report lists check numbers of Cleared checks and the other report lists check numbers of Voided checks. This updating of the OPIDDaily table of checks implies that check numbers are recorded in the OPIDDaily database before their dispositions are known. This is indeed the case. To effect this, a report is periodically exported from the Apricot database and processed by OPIDDaily to import issued check into the OPIDDaily table of checks. By consulting the table of checks in the OPIDDaily database, it is possible to produce a service history for a client. A client with a service history is referred to as an Existing Client and a client with no previous service history is referred to as an Express Client. The term express client was chosen to imply that such a client's service request would be both easier and faster to process in the historic same-day-service model. As explained above, in practice a Service Ticket for an Express Client is not generated. In the remote operation model, a Service Ticket will be generated at Operation ID for both Existing Clients and Express Clients. Since there will be no referral letter under this model, the Service Ticket will be the only means of specifying requested services. Frequently a client will return to Operation ID requesting service before the disposition of a check previously issued to the client is known. In such a case, a back office volunteer will need to consult a Quickbooks ledger to determine the check's disposition. If the disposition can be determined, it may affect the service request by the client. In the same-day-service model, a client may need to be informed the he/she is not eligible for a requested service, because they have exhausted their twice-in-a-lifetime eligibility. This is sometimes a difficult conversation, especially if the client has endured a long line only to learn they are not eligible for the service they are requesting. In the remote operation model, Operation ID will convey ineligibility through the OPIDDaily website to a client's case manager. The case manager will then need to remind the client of the disclaimer about potential service denial that the client was made aware of when they signed their Case Manager Voucher. This Document OPIDDaily is available as a password protected website to registered users. This document describes the design and implementation of OPIDDaily. The Infrastructure tab describes how project OPIDDaily is maintained on a desktop host and how it is deployed to the .NET hosting service AppHarbor. The Database tab describes how the OPIDDaily database is managed on the desktop and at AppHarbor. The Implementation tab provides some details concerning the implementation of OPIDDaily.","title":"Background"},{"location":"#background","text":"Operation ID is a ministry that exists to serve the need of individuals who need to obtain identification documents to support their application for a variety of services from housing to job application. The ministry is volunteer driven and has been in place for over thirty years. During this time thousands of individuals have received help to support their identification needs. Help is provided in the form of vouchers that can be used to pay for services rendered by either the Texas Department of Public Safety for a Texas ID or by the Bureau of Vital Statistics of Texas or another state to acquire a certified copy of a birth certificate. Over the years of Operation ID's existence various paper driven pipelines of client processing in providing identification services have been used. The currently existing pipeline consists of four stages which are executed sequentially. Screening Check In Interviewing Back Office Voucher Writing In the Screening stage, a client is greeted at the front desk by a volunteer who checks that the client's referral letter is valid and that the documents the client possesses support the services they are seeking. The referral letter is provided by the agency representing the client to Operation ID and this agency performs the first level of screening. If this first level of screening is done carefully, the Screening stage at Operation ID can proceed very quickly, allowing the client to proceed to the Check In stage. The referral letter must specify the services that a client is seeking at Operation ID. In the Check In stage (also performed by a volunteer at the front desk) the client is entered into the Operation ID database if he/she is not already in the database. The database is maintained by the Apricot database driven application software provided by the vendor Social Solutions . Apricot is used to capture a client's demographic data together with a history of a client's previous visits to Operation ID. The demographic data includes a client's name, date of birth and age (a calculated field based on date of birth). The demographic data also includes a link to a household table, if a client is part of a household being processed together. Apricot stores basic demographic data in a table it refers to as the Client Tracking table and stores client visits in a related table called the Client Tracking Document Folder. (In terms of database technology, the Client Tracking Document Folder is related to the Client Tracking table by a foreign key.) Operation ID makes use of the Client Tracking Document Folder to record services provided to a client during previous visits. Whenever a service is provided to a client (example: writing a check for a birth certificate), the service is recorded by date together with the number of any issued check. At a date after the date of service, the disposition of an issued check is recorded. Most often this disposition will be either Cleared, indicating that the check has been used by the client or Voided, indicating that the check is no longer valid. During the Check In stage at the front desk, a client's history of previous visits (if any) is manually copied onto a small slip of paper which is then stapled to the client's referral letter. Previous visits may make a client ineligible for a service being requested. Operation ID enforces a twice-in-a-lifetime policy for birth certificate or ID services. If a client's history of previous visits reveals that they have twice previously been issued checks for a birth certificate and both checks have been used, then the client is not eligible for another check for a birth certificate. Similarly, if a client has twice previously been issued checks for an ID (either a Texas ID or a Texas Driver's License) and both checks have been used, then they are not eligible for another check for an ID. If a client is ineligible for a requested service, then they are so informed at the front desk. A client ineligible for a requested service may appeal their ineligibility and may, at the discretion of Operation ID management, be granted an exception. Most clients are either first time clients (no previous visit history) or clients who have not yet exhausted eligibility for a requested service. In either case, such a client is ready for the Interviewing stage. A client ready to be interviewed may have to wait for a (volunteer) interviewer to become available. Clients are processed on a first-come-first-served basis and frequently have to wait for an available interviewer. When an interviewer picks up a waiting client, the client's requested services are reviewed and paperwork in support of these services is completed. If the client is representing themselves alone, the paperwork generation may go fairly quickly. If the client is requesting an out-of-state birth certificate, more time is required, because the paperwork required to acquire a certified birth certificate varies from state to state. If the client represents a household, then paperwork for each member of the household is required and this may slow down the interviewing process considerably. In any event, under the current client processing pipeline, all paperwork required is completed during the interviewing process before the back office can generate any voucher for a client. When paperwork filled out during the Interviewing stage is sent to the back office, the Back Office Voucher Writing stage may begin. This stage may not begin immediately upon the delivery of the completed paperwork for a given client, because a backlog of previous clients may need to be processed first. When a client's paperwork is eventually processed by the back office, what remains to be done is straightforward. A voucher for each service requested by a client is first generated by use of Quickbooks (operated by a volunteer). Then the voucher number of each generated voucher is recorded in the Apricot database (by another volunteer) in the client's visit history. This record keeping step is what enables enforcement of the twice-in-a-lifetime service policy mentioned above. After vouchers have been generated and recorded, they are delivered to the client and the pipeline of client processing is completed. What has been described above may be referred to as the same-day-service model of client processing. An alternative model, Remote Operation ID, will be described in a section below. Remote Operation ID will be described as a virtual platform providing the same client services without the need for a client to appear at Operation ID.","title":"Background"},{"location":"#the-opiddaily-website","text":"The OPIDDaily website was designed to partially automate the pipeline of client processing at Operation ID by allowing the back office production of vouchers to proceed in parallel with the interviewing of clients to generate paperwork supporting their needs. OPIDDaily takes advantage of the fact that a client's voucher needs are already known during the Screening and Check In stages. This allows the front desk volunteers to send the back office volunteers information which allows the Back Office Voucher Writing stage to begin, possibly even before the Interviewing Stage has started. In support of parallel processing using OPIDDaily a Service Ticket describing a client's previous visits (if any) together with the services the client is seeking is printed on a printer at the front desk by the front desk check in volunteer. It has been suggested that in straightforward cases this Service Ticket be sent directly to the back office so that back office volunteers could begin processing the service requested on the Service Ticket in advance of the Interviewing stage being completed. In effect, the Interviewing stage and the Back Office Voucher Writing stage would be run in parallel. This has the potential of delivering substantial time savings over the currently existing sequential pipeline. The Screening and Check In stages would require a small amount of additional time in order to generate the Service Ticket for a client, but this extra time would be more than compensated for by the ability to run the Interviewing and Back Office Voucher Writing stages in parallel. In practice, a Service Ticket is not issued for each client appearing at the front desk on a day of service. Instead, in order to keep the line moving, a Service Ticket is only generated for clients with a history of previous visits. Clients with no previous visits are quickly moved through the front desk after their name and date of birth have been recorded in the Apricot database. The Service Ticket generated for a client with a visit history is attached to the client's referral letter rather than being sent to the back office. This does not take advantage of the parallel processing mentioned above, but it has nevertheless proved to be valuable. A client's referral letter must contain the services the client is seeking. A Service Ticket for a client with previous history summarizes the requested services in a tabular format.","title":"The OPIDDaily Website"},{"location":"#remote-operation-id","text":"In March 2020 Operation ID was shutdown due to the coronavirus pandemic. At the time of this writing (April 2020) Operation ID is still shut down and it is not known when it will reopen. When the shutdown began, work was begun on a simple extension of OPIDDaily that would allow for providing identification services remotely. Remote operation is the analog of the same-day-service model of identification services that have historically been provided by Operation ID by a virtual platform that allows case managers to submit service requests to Operation ID through the OPIDDaily website. The service request is an electronic replacement for the referral letter used in the same-day-service model described above. Under the same-day-service model, there is only a single logged in user, the TicketMaster . The TicketMaster is a volunteer who sits at the front desk and uses OPIDDaily to generate a Service Ticket for each client with a history of previous visits. In the remote operations model, case managers who work at Operation ID partner agencies login to the OPIDDaily website to submit service requests on behalf of their clients. The last step of generating a service request for a client is the printing and signing of the Case Manager Voucher . The Case Manager Voucher is a piece of paper describing a service request. It includes a disclaimer that makes the client aware that requested services may be denied because of previous visits to Operation ID for the same services. The Case Manager Voucher must be signed by both the client and the case manager. The voucher will be retained by the case manager, who will later redeem it for checks covering the expenses of the requested services. A service request submitted by a case manager appears on a service requests dashboard monitored by an Operation ID employee. The dashboard is simply a list of service requests placed by case managers working at different agencies. When a new service request appears on the dashboard, its arrival is first acknowledged by checking a checkbox on the request. This acknowledgment will be seen by the case manager who placed the request, informing him/her that Operation ID is aware of the request. The service request will next be used to generate a Service Ticket which will be printed and added to a collection of outstanding Service Tickets. When Service Tickets representing enough clients have been accumulated, the status of each represented client will be changed to BackOffice, indicating that the service requests have been passed to the back office at Operation ID for processing. The respective case managers will see this change in status for their clients. Processing a Service Ticket in the back office is exactly the same process that takes place in the same-day-service model. It involves using Quickbooks to cut checks for services specified by a Service Ticket and recording the check numbers in the corresponding client database entry in the Apricot database. This processing will be handled by an Operation ID volunteer, as is done in the same-day-service model. Once the check(s) have been cut for a client, the Operation ID employee monitoring the dashboard changes the client's status to Done, indicating to the client's case manager that processing has completed and checks may be picked up at the Operation ID office. The checks will be released to a case manager as redemption of the corresponding Case Manager Voucher. The process of remote operations described above involves only the case manager and Operation ID personnel. A client works with his/her case manager to generate a service request and then waits for the checks to be picked up at Operation ID by the case manager. Unlike the same-day-service model, the client never appears at Operation ID. This will be especially important as long as the same-day-service operation remains shut down. The Remote Operation ID process clearly requires more time to complete than the same-day-service model. As a practical matter, a Case Manager Voucher will indicate a 30 day period of validity. This is actually a time limit on how long Operation ID is given to respond to the corresponding service request once it appears on the service requests dashboard. The dashboard list will be sorted in chronological order of service request expiration dates. A service request will automatically roll off the dashboard once its expiration date has passed.","title":"Remote Operation ID"},{"location":"#service-history","text":"A Service Ticket lists not only the services a client is requesting but also the services the client has received in prior visits (if any). Listing previous services rendered is made possible by including a table of previously issued checks in the OPIDDaily database. Each record in the table of checks contains a check number, the check purpose, the name of the client to whom the check was issued and the disposition of the check, whether it was Cleared by the bank, Voided by the bank or one of several dispositions private to Operation ID. A check record does not include who the payee of the check is or the amount of the check. The check is referenced to the Apricot database simply by its unique number, not by the name of the client to whom the check was given. Periodically a pair of back office reports is processed by OPIDDaily to update its table of checks with the bank declared disposition of checks. These reports are generated by Quickbooks. One report lists check numbers of Cleared checks and the other report lists check numbers of Voided checks. This updating of the OPIDDaily table of checks implies that check numbers are recorded in the OPIDDaily database before their dispositions are known. This is indeed the case. To effect this, a report is periodically exported from the Apricot database and processed by OPIDDaily to import issued check into the OPIDDaily table of checks. By consulting the table of checks in the OPIDDaily database, it is possible to produce a service history for a client. A client with a service history is referred to as an Existing Client and a client with no previous service history is referred to as an Express Client. The term express client was chosen to imply that such a client's service request would be both easier and faster to process in the historic same-day-service model. As explained above, in practice a Service Ticket for an Express Client is not generated. In the remote operation model, a Service Ticket will be generated at Operation ID for both Existing Clients and Express Clients. Since there will be no referral letter under this model, the Service Ticket will be the only means of specifying requested services. Frequently a client will return to Operation ID requesting service before the disposition of a check previously issued to the client is known. In such a case, a back office volunteer will need to consult a Quickbooks ledger to determine the check's disposition. If the disposition can be determined, it may affect the service request by the client. In the same-day-service model, a client may need to be informed the he/she is not eligible for a requested service, because they have exhausted their twice-in-a-lifetime eligibility. This is sometimes a difficult conversation, especially if the client has endured a long line only to learn they are not eligible for the service they are requesting. In the remote operation model, Operation ID will convey ineligibility through the OPIDDaily website to a client's case manager. The case manager will then need to remind the client of the disclaimer about potential service denial that the client was made aware of when they signed their Case Manager Voucher.","title":"Service History"},{"location":"#this-document","text":"OPIDDaily is available as a password protected website to registered users. This document describes the design and implementation of OPIDDaily. The Infrastructure tab describes how project OPIDDaily is maintained on a desktop host and how it is deployed to the .NET hosting service AppHarbor. The Database tab describes how the OPIDDaily database is managed on the desktop and at AppHarbor. The Implementation tab provides some details concerning the implementation of OPIDDaily.","title":"This Document"},{"location":"Checks/","text":"Checks Application OPID Daily maintains 3 tables of check records that have been issued to clients. The tables, called RChecks , AncientChecks and PocketChecks , can be found on the database diagram of the Database tab. The name RChecks stands for recent checks. The RChecks table contains checks from the years 2018 - 2020. The name AncientChecks refers to the fact that this table contains older checks, specifically checks from the years 2016 - 2019. The two tables have exactly the same data fields and could in fact be represented by a single table. A single table was used initially, but as this table grew larger it became more difficult to update. So an implementation decision was made to split it into 2 tables. The codebase performs all check searches and CRUD operations with respect to both tables. Each check stored in the tables is referenced back to the Apricot database by its RecordID and InterviewRecordID data fields. The master table of checks is maintained by Apricot. The collection of checks that have been modified (either created or updated) in the Apricot database is periodically imported into OPID Daily. This happens by running the OPID Daily report in Apricot to create an Excel spreadsheet of checks, which is then imported into the OPID Daily application to update its database tables. The PocketChecks table was added in June 2020 to provide a means of entering check data directly into OPID Daily instead of having to wait for checks to be imported from Apricot. A pocket check is created when adding a new check to the visit history of an existing client or when adding a new check to an express client. Only users in role Interviewer or Back Office can create a pocket check. A pocket check is marked as active when it is first created and is marked inactive when a check with the same check number is imported from Apricot. Pocket checks enable a seamless transition to operations under OPID Daily alone. Once imports from Apricot are no longer performed the checks in the RChecks and AncientChecks tables will eventually be older than the data retention guidelines. The OPID Daily application also has its check tables updated by Excel spreadsheets generated by Quickbooks. An Excel spreadsheet coming from Quickbooks shows the disposition (either Cleared or Voided) of a check as recorded at the bank used by Operation ID. In addition to updating its own tables, application OPID Daily generates an Excel spreadsheet for importing into the Apricot database to keep this database in synch with the OPID Daily database. Keeping 2 databases in synch with each other is a cumbersome process. The Apricot database came first and it has been in use to support the same-day-service model of Operation ID for the past several years. However, now that application OPID Daily has become available, it is worth considering whether it should become the sole source of check data, eliminating the need to maintain the Apricot database for Operation ID. The PocketChecks table makes this possible. If Operation ID would adopt a data retention policy that required only 5 years worth of check data, then the twice-in-a-lifetime service policy could be changed to a twice in 5 years policy which could easily by enforced by application OPID Daily. The Apricot database contains check dating back to the year 2013. Moving forward it will become less likely that a client from that year (or even years 2015 and 2016) will request service from Operation ID. At the end of 2020, 5 full years of check data will be in the OPID Daily database and a twice-in-five-years policy could be enacted. The OPID Daily Report When a check for a client is issued by Quickbooks, the number of the check is stored in the record representing the client in the Apricot database. The check number is recorded based on the type of check, for example, a check for a Texas driver's license or a Texas birth certificate. This check information must be recorded in the OPID Daily database as well in order to keep the two in synch. For this purpose, it is necessary to periodically run the OPID Daily report in Apricot. This report takes a date as an input parameter and generates a spreadsheet of checks that have been modified since that date. Importing this spreadsheet into the OPID Daily application brings its database of checks into synch with the database of checks in Apricot. Only a user in the BackOffice role can import the OPID Daily report. The interface for importing the file is found under the Merge item of the BackOffice menubar. Each record in the spreadsheet contains a RecordID and InterviewRecordID used to reference the check back to the Apricot database. Unless a check updates a check in the AncientChecks table, it is imported into the RChecks table. Thus, the RChecks table contains checks from years other than 2018 - 2020. When application OPID Daily searches for a client check, it does so by using the client's last name together with the client's date of birth. This is not necessarily a unique pair of keys to search by; however, it has been unique in practice. The search is always conducted with respect to both tables RChecks and AncientChecks . The search may also return a check from the PocketChecks table. The need to run the OPID Daily report is caused by the need to keep 2 databases in synch with each other. As already mentioned, this need could be eliminated by enacting a data retention policy and providing a check recording interface in application OPID Daily. The ImportMe Spreadsheet Periodically, reports indicating the disposition of checks at the Operation ID bank become available by running Quickbook reports. For important reasons these reports must be edited by the Operation ID Finance Office before they can be processed. The result of editing is a pair of Excel spreadsheets indicating check numbers of checks that have been Voided by the bank and check numbers of checks that have been Cleared by the bank. Neither of the reports can be directly imported into the Apricot database. Each report must be processed in order to create a spreadsheet suitable for importing into Apricot for the purpose of updating its database. The processing of each Voided checks and Cleared checks is done by application OPID Daily. The number of a check provides a unique search key for locating a check in the OPID Daily database. When a check has been located by number, its disposition is updated in the OPID Daily database as either Voided or Cleared, depending on the imported spreadsheet that it belongs to. Processing a spreadsheet of Voided checks will generate a set of resolved voided checks, that is a set of checks that are now known to be voided. Similarly, processing a set of Cleared checks will generate a set of resolved Cleared checks, that is a set of checks that are now known to be cleared. Since each check stored in the OPID Daily database contains both a RecordID and an InterviewRecordID, these keys together with the check's number and new disposition can be used to update the check in the Apricot database. A spreadsheet of resolved checks called an ImportMe file is created by application OPID Daily to perform this updating. It is not quite a simple matter of using the indicated data keys to update a record in Apricot. This is because the record in Apricot contains all the check data for a given client in one record pointed to by the RecordID and InterviewRecordID data keys. Thus, if a birth certificate check for a client is resolved as voided, all the other checks issued for the client must be included in the record imported for the one resolved check. Otherwise, the import will overwrite other checks for the client with NULL values! This requires that application OPID Daily carefully construct an ImportMe spreadsheet so as not to lose data. In addition to resolving a single check for a client, OPID Daily must search its database to collect all other checks stored for the client in order to construct an import record that will not overwrite data. Updating the OPID Daily database via a spreadsheet of Voided checks or Cleared checks is easy. It can be performed only by a user in the BackOffice role. The interface for doing so is found under the Merge menu item of the BackOffice menubar. As described, updating the OPID Daily database has the side effect of creating a set of resolved checks. This set can be viewed in a table stored under the Resolved tab of the menubar. Resolved checks must be downloaded to create an ImportMe spreadsheet to import into Apricot. As already noted, this is a cumbersome process that could be eliminated by switching all Operation ID processing to application OPID Daily.","title":"Checks"},{"location":"Checks/#checks","text":"Application OPID Daily maintains 3 tables of check records that have been issued to clients. The tables, called RChecks , AncientChecks and PocketChecks , can be found on the database diagram of the Database tab. The name RChecks stands for recent checks. The RChecks table contains checks from the years 2018 - 2020. The name AncientChecks refers to the fact that this table contains older checks, specifically checks from the years 2016 - 2019. The two tables have exactly the same data fields and could in fact be represented by a single table. A single table was used initially, but as this table grew larger it became more difficult to update. So an implementation decision was made to split it into 2 tables. The codebase performs all check searches and CRUD operations with respect to both tables. Each check stored in the tables is referenced back to the Apricot database by its RecordID and InterviewRecordID data fields. The master table of checks is maintained by Apricot. The collection of checks that have been modified (either created or updated) in the Apricot database is periodically imported into OPID Daily. This happens by running the OPID Daily report in Apricot to create an Excel spreadsheet of checks, which is then imported into the OPID Daily application to update its database tables. The PocketChecks table was added in June 2020 to provide a means of entering check data directly into OPID Daily instead of having to wait for checks to be imported from Apricot. A pocket check is created when adding a new check to the visit history of an existing client or when adding a new check to an express client. Only users in role Interviewer or Back Office can create a pocket check. A pocket check is marked as active when it is first created and is marked inactive when a check with the same check number is imported from Apricot. Pocket checks enable a seamless transition to operations under OPID Daily alone. Once imports from Apricot are no longer performed the checks in the RChecks and AncientChecks tables will eventually be older than the data retention guidelines. The OPID Daily application also has its check tables updated by Excel spreadsheets generated by Quickbooks. An Excel spreadsheet coming from Quickbooks shows the disposition (either Cleared or Voided) of a check as recorded at the bank used by Operation ID. In addition to updating its own tables, application OPID Daily generates an Excel spreadsheet for importing into the Apricot database to keep this database in synch with the OPID Daily database. Keeping 2 databases in synch with each other is a cumbersome process. The Apricot database came first and it has been in use to support the same-day-service model of Operation ID for the past several years. However, now that application OPID Daily has become available, it is worth considering whether it should become the sole source of check data, eliminating the need to maintain the Apricot database for Operation ID. The PocketChecks table makes this possible. If Operation ID would adopt a data retention policy that required only 5 years worth of check data, then the twice-in-a-lifetime service policy could be changed to a twice in 5 years policy which could easily by enforced by application OPID Daily. The Apricot database contains check dating back to the year 2013. Moving forward it will become less likely that a client from that year (or even years 2015 and 2016) will request service from Operation ID. At the end of 2020, 5 full years of check data will be in the OPID Daily database and a twice-in-five-years policy could be enacted.","title":"Checks"},{"location":"Checks/#the-opid-daily-report","text":"When a check for a client is issued by Quickbooks, the number of the check is stored in the record representing the client in the Apricot database. The check number is recorded based on the type of check, for example, a check for a Texas driver's license or a Texas birth certificate. This check information must be recorded in the OPID Daily database as well in order to keep the two in synch. For this purpose, it is necessary to periodically run the OPID Daily report in Apricot. This report takes a date as an input parameter and generates a spreadsheet of checks that have been modified since that date. Importing this spreadsheet into the OPID Daily application brings its database of checks into synch with the database of checks in Apricot. Only a user in the BackOffice role can import the OPID Daily report. The interface for importing the file is found under the Merge item of the BackOffice menubar. Each record in the spreadsheet contains a RecordID and InterviewRecordID used to reference the check back to the Apricot database. Unless a check updates a check in the AncientChecks table, it is imported into the RChecks table. Thus, the RChecks table contains checks from years other than 2018 - 2020. When application OPID Daily searches for a client check, it does so by using the client's last name together with the client's date of birth. This is not necessarily a unique pair of keys to search by; however, it has been unique in practice. The search is always conducted with respect to both tables RChecks and AncientChecks . The search may also return a check from the PocketChecks table. The need to run the OPID Daily report is caused by the need to keep 2 databases in synch with each other. As already mentioned, this need could be eliminated by enacting a data retention policy and providing a check recording interface in application OPID Daily.","title":"The OPID Daily Report"},{"location":"Checks/#the-importme-spreadsheet","text":"Periodically, reports indicating the disposition of checks at the Operation ID bank become available by running Quickbook reports. For important reasons these reports must be edited by the Operation ID Finance Office before they can be processed. The result of editing is a pair of Excel spreadsheets indicating check numbers of checks that have been Voided by the bank and check numbers of checks that have been Cleared by the bank. Neither of the reports can be directly imported into the Apricot database. Each report must be processed in order to create a spreadsheet suitable for importing into Apricot for the purpose of updating its database. The processing of each Voided checks and Cleared checks is done by application OPID Daily. The number of a check provides a unique search key for locating a check in the OPID Daily database. When a check has been located by number, its disposition is updated in the OPID Daily database as either Voided or Cleared, depending on the imported spreadsheet that it belongs to. Processing a spreadsheet of Voided checks will generate a set of resolved voided checks, that is a set of checks that are now known to be voided. Similarly, processing a set of Cleared checks will generate a set of resolved Cleared checks, that is a set of checks that are now known to be cleared. Since each check stored in the OPID Daily database contains both a RecordID and an InterviewRecordID, these keys together with the check's number and new disposition can be used to update the check in the Apricot database. A spreadsheet of resolved checks called an ImportMe file is created by application OPID Daily to perform this updating. It is not quite a simple matter of using the indicated data keys to update a record in Apricot. This is because the record in Apricot contains all the check data for a given client in one record pointed to by the RecordID and InterviewRecordID data keys. Thus, if a birth certificate check for a client is resolved as voided, all the other checks issued for the client must be included in the record imported for the one resolved check. Otherwise, the import will overwrite other checks for the client with NULL values! This requires that application OPID Daily carefully construct an ImportMe spreadsheet so as not to lose data. In addition to resolving a single check for a client, OPID Daily must search its database to collect all other checks stored for the client in order to construct an import record that will not overwrite data. Updating the OPID Daily database via a spreadsheet of Voided checks or Cleared checks is easy. It can be performed only by a user in the BackOffice role. The interface for doing so is found under the Merge menu item of the BackOffice menubar. As described, updating the OPID Daily database has the side effect of creating a set of resolved checks. This set can be viewed in a table stored under the Resolved tab of the menubar. Resolved checks must be downloaded to create an ImportMe spreadsheet to import into Apricot. As already noted, this is a cumbersome process that could be eliminated by switching all Operation ID processing to application OPID Daily.","title":"The ImportMe Spreadsheet"},{"location":"Database/","text":"Database OPIDDaily is a database driven application built using SQL Server technology. In the desktop environment OPIDDaily is built using the Sql Server Express database engine. In the online environment at AppHarbor a full SQL Server is used. The two versions are compatible with each other with respect to the database features used. SQL Server Management Studio v18.0 (SSMS) is used to manage both database engines. In the desktop environment, Windows Authentication is used to connect to the Sql Server Express database. In the online environment, SQL Server Authentication is used to connect to the SQL Server database. When application OPIDDaily was created at AppHarbor, a free version of SQL Server was added on through the AppHarbor interface. The connection string to this SQL Server is found by selecting the SQL Server add-on and following the Go to SQL Server link on the page that appears. The value of this connection string is stored as the value of Config.WorkingProductionConnection string on file Config.cs. The 3 components of the connection string, HostName, UserName and Password, are also displayed on this page. The components may be used to configure a SQL Server Authentication connection to the AppHarbor database through SSMS. The same connection string displayed at AppHarbor is retrieved at runtime by accessing Config.ConnectionString, which returns the value of OpidDailyConnectionString configured in the <connectionStrings> section of file Web.config. The statically configured value on file Web.Config points to the OpidDailyDB on the desktop SQL Server Express. At runtime, AppHarbor will overwrite this statically configured value with the value displayed at AppHarbor. See the section on the Connection String on this tab. Two Databases and a Hybrid System The Apricot database is intended to be the repository of all truth about clients of Operation ID. It does well with the record keeping aspects of a repository but falls short on two of the primary goals of the OPID Daily application: updating of the disposition of checks in the Apricot database by processing a Quickbooks report generating consolidated Service Tickets Neither of these can be accomplished by Apricot alone. The updating of check dispositions must be done through the OPID Daily application, because Apricot does not have the power of a general purpose programming language which is needed for this task. The generation of consolidated Service Tickets, an important addition to the Operation ID workflow, can also not be performed by Apricot alone. Again, it is the lack of a general purpose programming language that prevents Apricot from performing this task. The consequence of these 2 shortcomings of Apricot has led to Operation ID being run as a hybrid system between the OPID Daily application and the Apricot record keeping system. Each system maintains its own database and bidirectional updating is needed to keep the two databases in synch to ensure the proper functioning of Operation ID. Without going into detail, the OPID application could take over the record keeping performed by Apricot and become the single repository of truth about all clients of Operation ID. But it was found that Apricot could not be conveniently extended beyond record keeping to be the single repository it was intended to be. Apricot supports the generation of filtered reports exportable as Excel spreadsheets. In terms of database technology, an Apricot report is a database view, a SQL query which when executed returns a collection of records of data drawn from the underlying Apricot database. A filtered report is a database query which includes a SQL WHERE-clause. Filtered reports were made available to Apricot users to provide convenient windows upon the data in their Apricot databases and to support construction of their own custom reports based on this data. Filtered reports are also the key to initially populating and ongoingly updating the OPID Daily application to keep it in synch with the record keeping performed in the Apricot database. A filtered report called the Bounded Research Report was used to initially populate the OPID Daily database of checks. The internal database of checks in application OPID Daily is what enables the generation of Service Tickets revealing the past visit history of a client. The Bounded Research Report was used to create Excel spreadsheets containing a single year's worth of check data at a time. The Bounded Research Report for a given (past) year was consumed by the OPID Daily application to enter check data belonging to that year into the OPID Daily database. Since the construction of the database of checks from previous years, a second filtered report called the OPID Daily Report is now used to incrementally update the OPID Daily database with new checks that have been recorded in Apricot. The OPID Daily report makes use of the modification date stored with records in the Apricot database to retrieve the delta , that is only the records that have been modified since a specific date. The specific date supplied to the OPID Daily report is its filter over all modified records. If record keeping were performed inside the OPID Daily application, then this updating would be unnecessary. The Apricot database would only be needed to initially populate the OPID Daily database of checks. The initial population of the OPID Daily database and its incremental updating by the use of the OPID Daily Report are how the OPID Daily database is kept in synch by the Apricot database. Updating in the other direction is equally important. At the time that check numbers a recorded in the Apricot database, the disposition of those checks (whether Cleared or Voided) is yet to be determined. The determination of the disposition of a check is referred to as resolving the check. Checks are resolved by a pair of Quickbooks reports created as Excel spreadsheets after the bank used by Operation ID has reported on cleared checks. The first report, called the Cleared Checks Report, lists checks cleared by the bank by check number. The second report, called the Voided Checks Report lists by check number checks that are more than 90 days old and have been voided by Operation ID in instructions sent by Quickbooks to the bank. Neither the Cleared Checks Report nor the Voided Checks Report in a format that can be consumed by Apricot to update its database. Apricot is very stringent about the format a file must be in in order to be used to update its database. OPID Daily is used to consume the Cleared Checks Report and the Voided Checks Report to resolve checks in its own database. After resolving checks listed on these reports, application OPID Daily constructs an Excel spreadsheet in the stringent format required by Apricot in order to update its database. This spreadsheet is referred to as an importme file, because it is imported into Apricot through the Apricot Administrator interface. This is how the Apricot database is kept in synch with the OPID Daily database. Again, if record keeping functions were provided in OPID Daily, then keeping the Apricot database in synch with the OPID Daily database would not be necessary. Bidirectional updating is the means that has been worked out in order to realize the benefits of both Apricot and OPID Daily. This has resulted in a hybrid system. Connection String In the desktop environment, SSMS was used to create an empty project database by executing the SQL query create database OpidDailyDB The Visual Studio Server Explorer (found under the OPIDDaily project View menu) was then used to discover the connection string to database OpidDailyDB by creating a new Data Connection to it and copying the Connection String property of the data connection into the <connectionStrings> section of Web.config. <add name=\"OpidDailyConnectionString\" connectionString=\"data source=DESKTOP-0U83VML\\SqlExpress;initial catalog=OPIDDailyDB;Integrated Security=True\" providerName=\"System.Data.SqlClient\" /> This value is accessed on files IdentityDB.cs and OpidDailyDB.cs by reading the value of the static variable Config.ConnectionString: public static string ConnectionString { get { // The value of OpidDailyConnectionString configured on Web.config is overwritten at AppHarbor deployment time. return ConfigurationManager.ConnectionStrings[\"OpidDailyConnectionString\"].ToString(); } } The value of OpidDailyConnectionString is overwritten at application deployment time at AppHarbor. This is accomplished by setting the connection string alias for the SQL Server add-on at AppHarbor to be OpidDailyConnectionString . To set this alias, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link Go to SQL Server on the page that appears. Click the button labeled \"Edit database configuration\" to set OpidDailyConnectionString as the alias value for the connection string. When this is done, OpidDailyConnectionString will appear as the value of SQLSERVER_CONNECTION_STRING_ALIAS in the Configuration variables section of application OPIDDaily at AppHarbor. When application OPIDDaily is deployed at AppHarbor, this alias will overwrite the configured value of OpidDailyConnectionString on file Web.config by the value of the connection string for the AppHarbor database. If you run the OPIDDaily application on the desktop and see the error the error The wait operation timed out it may not point to an error with the connection string but to a problem with SQL Server itself. Try opening the Windows 10 Services App and restarting the SQL Server (SQLEXPRESS) service. If all else fails try restarting the computer. This sometimes clears up connection problems. The online version of OPIDDaily is hosted as an application at AppHarbor and it uses a database server provided as an add-on. The add-on database server includes a database which serves as the application database, so it is not necessary to create the application database as was done above for the desktop version. Database Diagram The diagram was created by SSMS, copied to the clipboard (using the \"Copy Diagram to Clipboard\" command found on the freespace context menu) and then pasted into the Paint tool. Inside of Paint it is saved as a .PNG file. Version 18.0 of SSMS does not allow database diagrams to be created. Newer releases have restored this capability. But the diagram seen here was created by an earlier release of SSMS, which did have the ability to create database diagrams. The main tables of application OPIDDaily are the tables Clients , TextMsgs , RChecks and AncientChecks . The table Clients stores clients, their service requests and their supporting documents. This is the reason that table Clients has so many data fields. Some of this data could have been factored out by the use of many-to-many relationship tables, but it was decided that a single table would be simpler. Notice that many of the data fields in table Clients are bit fields. As a result, a record in the table does not consume much database storage. The table TextMsgs is related to the Clients table by the foreign key Id as there is a one-to-many relationship between a client and the messages that have been written concerning the client. The table RChecks is used to store check data from Apricot. The RecordIID and InterviewRecordID data fields identify a client in the Apricot database and a particular visit the client has made to Operation ID. The service history of a client consists of the checks that have been issued to the client. To retrieve the service history from the table RChecks the client is looked up by last name and DOB. This is not guaranteed to be a unique lookup, but it almost always is. The name RChecks is short for research checks . The RChecks table was so named because checks whose disposition is unknown are said to be under research until their disposition is resolved. The AncientChecks table was added to relieve the overcrowding of the RChecks table. It has exactly the same data fields as table RChecks . Originally table RChecks contained all the checks that have been issued by Operation ID. The table was built by adding one year's worth of checks at a time, which resulted in gateway timeouts at AppHarbor during update operations as the table grew larger. So the table was split up into 2 tables by years. Currently the RChecks table contains checks from the years 2018-2020 and the AncientChecks table contains checks from the years 2016 and 2017. The PocketChecks table was added in June 2020 to provide a means of entering check data directly into OPID Daily instead of having to wait for checks to be imported from Apricot. A pocket check is created when adding a new check to the visit history of an existing client or when adding a new check to an express client. Only users in role Interviewer or Back Office can create a pocket check. A pocket check is marked as active when it is first created and is marked inactive when a check with the same check number is imported from Apricot. Pocket checks enable a seamless transition to operations under OPID Daily alone. Once imports from Apricot are no longer performed the checks in the RChecks and AncientChecks tables will eventually be older than the data retention guidelines. There exist checks going back to the year 2013 when a Microsoft Access database was used to manage clients; however, the disposition of checks from these early years was not stored along with the check numbers in the Access database. These check numbers were migrated to the Apricot database as part of a client's service history, but the check numbers from the years 2013-2015 were not entered into the OPIDDaily database because it was believed that clients from these years would rarely return to Operation ID. This has saved valuable storage space in the OPIDDaily database. The free deployment of the OPIDDaily website is given 20MB of data space on a shared SQL Server. According to the SSMS Disk Usage report, 12.31MB of the current 15.25MB allocated data space is in use. Thus, 80% of the current allocation is in use and only 61% of the allowed 20MB allocation is in use. So there is still adequate free space for more check data. At some point it will be useful to reconsider the twice-in-a-lifetime policy for client service. Ten years from now it is unlikely that clients from fifteen years in the past will come to Operation ID seeking service. So there is no advantage to storing checks from the distant past. A data retention policy should be formulated to purge the database of old records. Social Solutions does not allow customers to delete records from their own Apricot database. To delete records a customer must request that Social Solutions do it for them. The SuperAdmin user of the OPID Daily website has the ability to delete a year's worth of checks at a time. If a data retention policy of 5 years were put in place, then the OPID Daily website would easily be able to enforce it. Furthermore, it would simplify part of the user interface as there would be no need to consult Apricot for information not in the OPID Daily database: all the check data for the past 5 years could be stored. And instead of a twice-in-a-lifetime policy, Operation ID could support a twice-in-five-years policy. This would make OPIDDaily a self-contained website. When a client appears for service, a check is first made in the Apricot database to see if the client has a visit history. If the client does have a visit history and this history does not include any checks before the year 2016, then a Service Ticket including all previous service for the client can be generated at the front desk. If a client's visit history includes visits prior to 2016, then these visits will be included on the Service Ticket by consulting the Apricot database. In most cases, the checks from visits prior to 2015 will not include a disposition and their disposition will need to be determined in the back office by consulting the Quickbooks ledger. The inconvenience of this is outweighed by its rarity and the desire to stay below the 20MB free limit of data storage at AppHarbor. The table ELMAH_Error stores error messages generated by uncaught application errors. See the section on ELMAH on the Infrastructure tab. The table AppLog contains the log messages generated by the application using log4net. See the section on log4net on the Infrastructure tab. The _MigrationHistory table stores the Entity Framework Code First migrations that have been applied to the database. This table is defined by and managed by Entity Framework Code First. The 3 AspNet tables in the center the above diagram are created by ASP.NET Identity 2.0 to manage registered users of OPIDDaily. The 3 tables are managed by their own data context which cannot be augmented by additional tables. However, data fields can be added to table AspNetUsers if necessary. The data field AgencyId was added to table AspNetUsers to store the unique AgencyId stored in table Agencies by the Superadmin user. (The AgencyId of an Operation ID user - example the TicketMaster user - is always 0.) The non-ASP.NET Identity tables in the diagram are managed by a separate data context. The Visual Studio project OPIDDaily has 2 data contexts called IdentityDb and OpidDailyDB. (See the section Entity Framework Code First of the Infrastructure tab.) The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video . Adding migrations at AppHarbor via script To generate a script for the Up methods of the most recent migration(s), go back in the migration history to where the recent migrations start. For example, the migration preceding the migration ExpressClient was the migration PXXA. Therefore, to get a script for migration ExpressClient, execute the command update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:PXXA The generated script can be run against the database at AppHarbor using SSMS. The script should be run before the code is updated at AppHarbor. The script will also update the _MigrationHistory table. If the script involves adding a table, then running it before the code is updated at AppHarbor will ensure that the table is ready for use when the code that references it is deployed. Backing out migrations via script To generate a script to run the Down methods of multiple down-migrations, do, for example, PM> Update-Database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -TargetMigration: ExpressClient This command will create a script to execute in SSMS the Down methods of all migrations since (and not including) migration ExpressClient. It is important to be able to generate this script if changes need to be backed out, because the deployed versions of application OPIDDaily cannot be managed by the Package Manager (PowerShell) window available in Visual Studio. The script to execute the Down methods must be run in SSMS. After the script has been executed, the reverted migration may be safely deleted from the set of migrations maintained for the data context. Creating a Cloned Copy of OPIDDaily To create a cloned copy of OPIDDaily from the GitHub repository, supply the repository address https://github.com/tmhsplb/opiddaily.git to Visual Studio. Before running the project for the first time, use SSMS to setup a database called OpidDailyDB, as described in the SQL Server Express and SSMS section of the Infrastructure tab. It will be necessary to follow the instructions for creating the Invitations table given in the section titled The Staging and Training Versions of OPIDDaily on the Infrastructure tab. The missing migrations mentioned in that section can be applied to the database of the cloned version only by receiving an emailed copy of the referenced update-database script. It will be necessary to be made a collaborator on the project in order to commit changes in the cloned copy to the GitHub repository for OPIDDaily. Database Duplicates There are still many duplicates in the Apricot database as a result of the initial build from the legacy Access database. As an example, there are two records for the client John Harmon, DOB 9/11/1983: John Harmon, Record ID: 271978 John Marshall Harmon, Record ID: 4299 Since Service Tickets are built based on last name and DOB, both records will be used in building the Service Ticket for John Harmon. This can cause some confusion! Duplicate records are merged when they are discovered. The records for John Harmon may have been merged when the reader reads this documentation. Managing Users OPIDDaily is a role-based database application administered by a Superadmin user. The Superadmin user has the responsibilty of establishing a login account for each OPIDDaily user, which includes the user's role. This is done to prevent a user from specifying his/her own role when logging in and to force the user into his/her assigned role instead. See the introduction and Role Controllers sections of the Implementation tab for a discussion of roles. The Superadmin will be given a user name and email address for a new user. For example, if Mary Atwood would like to use the user name Mary and email address maryatwood@gmail.com, this request would be given to the Superadmin user. Provided that the user name Mary is not already in use, the Superadmin user would use a private interface to enter Mary Atwood in the Invitations table under UserName Mary (with FullName Mary Atwood) and Email Address maryatwood@gmail.com. The Superadmin would also use the OPIDDaily interface to assign a role to user Mary Atwood in the Invitations table. The record in the Invitations table is in effect an invitation for Mary Atwood to register under user name Mary and email address maryatwood@gmail.com in the assigned role. The Superadmin will notify Mary that her account has been created and that she may register with application OPIDDaily using the credentials she has supplied together with a password of her own choosing. When Mary registers, the user name and email address she provides will be checked against the Invitations table. If this pair of credentials is not found in the Invitations table, Mary's attempt to register will be rejected. If they are found, a record will be created for her in the AspNetUsers table using the password she has specified and using the role assigned by the Superadmin, which has been stored in the Invitations table. On subsequent visits to OPIDDaily, Mary may simply login with the credentials established by her registration. When logged in she will be recognized in her assigned role. In practice, the Superadmin has created all login accounts and registered all users. This implies that passwords are not secret. However, this has not been an issue. For the convenience of not having to self-register users have been willing to sacrifice a little privacy. User email addresses do not need to be unique per account. This is not the default behavior; it is enabled by the setting RequireUniqueEmail = false in method ApplicationUserManager.Create on file App_Start/IdentityConfig.cs There are two special accounts reserved for usage by the two users who serve at the front desk on any given day of operation. Each of these accounts has the pre-assigned role called FrontDesk. The users are the Screener and the TicketMaster which correspond to the pipeline stages Screening and Checkin , respectively. (See the Background tab for information about the pipeline stages.) Having dedicated accounts avoids the need to create unique accounts in the role of FrontDesk. There are also two additional special users called Client1 and Client2 corresponding to the pipeline stages Screening and CheckIn , respectively. (See the Background tab for information about the pipeline stages.) During the screening stage, the Screener user will enter the name and date of birth of an entering client into the OPIDDaily database. To ensure that this information has been correctly entered, the Screener may click a button to have this information appear on a small tablet computer which will be handed to the client for verification. This small tablet computer will be logged into the OPIDDaily application as Client1 . During the Checkin stage, by consultng the Apricot database, the TicketMaster user will record any previous visit history by a screened client in the OPIDDaily database. If previous visits indicate that the screened client is ineligible for a service being sought, the TicketMaster may click a button to have the visit history appear on a second small tablet computer which will be handed to the client. This second small tablet computer will be logged into the OPIDDaily application as Client2 . Both users Client1 and Client2 are assigned the role FrontDesk. Database Utilization AppHarbor allows 20MB of database storage with the free SQL Server. The current utilization can be checked at AppHarbor by selecting the SQL Server used by application OPIDDaily and following the \"Go to server\" link. As of February 8, 2020 10.2MB out of the free 20MB are in use. This database contains 32,500 checks which were cut since 2013. Of these, between 24,00 and 28,000 checks were cut over the past 4 years. This suggests that there is enough capacity in the free database to run for at least 3 more years before approaching the 20MB limit. The disk utilization should be monitored periodically to make sure the limit is not exceeded. Currently an upgrade to 10GB of space would cost $10/month. This should be an affordable expense when the time comes that the additional space is needed. SSMS can be used to check on the utilization of a database. To do so: Right click a database name Navigate to Reports > Standard Reports > Disk Usage Navigate to Reports > Standard Reports > Disk Usage By Table Although this can be done, the single number reported by AppHarbor is easier to understand. Rebuilding the Research Table In February 2020 application OPIDChecks was merged into application OPIDDaily for the sake of having a single application that performs both functions. The merge required the Research Table to be added to application OPIDDaily. The merger of the two applications had the additional benefit of allowing application OPIDDaily to use the Research Table to populate the visit history of clients with previous visits. The Research Table was rebuilt by loading files created by two different reports in Apricot: The report Bounded Research Table was used to create files: Bounded Research Table 2017 Bounded Research Table 2018 Bounded Research Table 2019 Bounded Research Table 2020 The report Imported from Access DB was used to create files: Imported from Access DB 2010 Imported from Access DB 2011 Imported from Access DB 2012 Imported from Access DB 2013 Imported from Access DB 2014 Imported from Access DB 2015 Imported from Access DB 2016 The files Imported from Access DB 2010 Imported from Access DB 2011 Imported from Access DB 2012 Imported from Access DB 2013 Imported from Access DB 2014 Imported from Access DB 2015 contain very little (and incomplete) data and were not loaded to rebuild the Research Table. The Research Table acutally consist of 2 tables: RChecks and AncientChecks . See the section on the Database Diagram for more information. Only the SuperAdmin user can build the database, using the Rebuild menu item on the Superadmin mneubar. Files Bounded Research Table 2018 Bounded Research Table 2019 Bounded Research Table 2020 are loaded into table RChecks under the Merge Bounded Research File section and files Bounded Research Table 2017 Imported from Access DB 2016 are loaded into table AncientChecks under the Merge Ancient Checks File section. At the end of year 2021, there will be 6 full years of checks in the Research Table(s). Between now (May 2020) and then a 5 year data retention policy should be put in place. This will allow OPIDDaily to easily enforce a twice-in-5-years check policy without having to consult the Apricot database. The current twice-in-a-lifetime check policy requires consulting Apricot for checks dating back to when record keeping started and will become an unecessary burden as the years pass. Updating the OPIDDaily Database It is necessary to keep the Research Table updated. This is done on a weekly basis by running the Apricot report called OPID Daily and providing the date of the Monday beginning a week of operation after the week is complete. For example the OPID Daily report should be run using Monday February 10, 2020 as the filter date on any day following Thursday February 13, 2020. This will create a file containing all the records modified during the week starting on Monday February 10. By merging the created file into application OPIDDaily, the Research Table will be brought up to date.","title":"Database"},{"location":"Database/#database","text":"OPIDDaily is a database driven application built using SQL Server technology. In the desktop environment OPIDDaily is built using the Sql Server Express database engine. In the online environment at AppHarbor a full SQL Server is used. The two versions are compatible with each other with respect to the database features used. SQL Server Management Studio v18.0 (SSMS) is used to manage both database engines. In the desktop environment, Windows Authentication is used to connect to the Sql Server Express database. In the online environment, SQL Server Authentication is used to connect to the SQL Server database. When application OPIDDaily was created at AppHarbor, a free version of SQL Server was added on through the AppHarbor interface. The connection string to this SQL Server is found by selecting the SQL Server add-on and following the Go to SQL Server link on the page that appears. The value of this connection string is stored as the value of Config.WorkingProductionConnection string on file Config.cs. The 3 components of the connection string, HostName, UserName and Password, are also displayed on this page. The components may be used to configure a SQL Server Authentication connection to the AppHarbor database through SSMS. The same connection string displayed at AppHarbor is retrieved at runtime by accessing Config.ConnectionString, which returns the value of OpidDailyConnectionString configured in the <connectionStrings> section of file Web.config. The statically configured value on file Web.Config points to the OpidDailyDB on the desktop SQL Server Express. At runtime, AppHarbor will overwrite this statically configured value with the value displayed at AppHarbor. See the section on the Connection String on this tab.","title":"Database"},{"location":"Database/#two-databases-and-a-hybrid-system","text":"The Apricot database is intended to be the repository of all truth about clients of Operation ID. It does well with the record keeping aspects of a repository but falls short on two of the primary goals of the OPID Daily application: updating of the disposition of checks in the Apricot database by processing a Quickbooks report generating consolidated Service Tickets Neither of these can be accomplished by Apricot alone. The updating of check dispositions must be done through the OPID Daily application, because Apricot does not have the power of a general purpose programming language which is needed for this task. The generation of consolidated Service Tickets, an important addition to the Operation ID workflow, can also not be performed by Apricot alone. Again, it is the lack of a general purpose programming language that prevents Apricot from performing this task. The consequence of these 2 shortcomings of Apricot has led to Operation ID being run as a hybrid system between the OPID Daily application and the Apricot record keeping system. Each system maintains its own database and bidirectional updating is needed to keep the two databases in synch to ensure the proper functioning of Operation ID. Without going into detail, the OPID application could take over the record keeping performed by Apricot and become the single repository of truth about all clients of Operation ID. But it was found that Apricot could not be conveniently extended beyond record keeping to be the single repository it was intended to be. Apricot supports the generation of filtered reports exportable as Excel spreadsheets. In terms of database technology, an Apricot report is a database view, a SQL query which when executed returns a collection of records of data drawn from the underlying Apricot database. A filtered report is a database query which includes a SQL WHERE-clause. Filtered reports were made available to Apricot users to provide convenient windows upon the data in their Apricot databases and to support construction of their own custom reports based on this data. Filtered reports are also the key to initially populating and ongoingly updating the OPID Daily application to keep it in synch with the record keeping performed in the Apricot database. A filtered report called the Bounded Research Report was used to initially populate the OPID Daily database of checks. The internal database of checks in application OPID Daily is what enables the generation of Service Tickets revealing the past visit history of a client. The Bounded Research Report was used to create Excel spreadsheets containing a single year's worth of check data at a time. The Bounded Research Report for a given (past) year was consumed by the OPID Daily application to enter check data belonging to that year into the OPID Daily database. Since the construction of the database of checks from previous years, a second filtered report called the OPID Daily Report is now used to incrementally update the OPID Daily database with new checks that have been recorded in Apricot. The OPID Daily report makes use of the modification date stored with records in the Apricot database to retrieve the delta , that is only the records that have been modified since a specific date. The specific date supplied to the OPID Daily report is its filter over all modified records. If record keeping were performed inside the OPID Daily application, then this updating would be unnecessary. The Apricot database would only be needed to initially populate the OPID Daily database of checks. The initial population of the OPID Daily database and its incremental updating by the use of the OPID Daily Report are how the OPID Daily database is kept in synch by the Apricot database. Updating in the other direction is equally important. At the time that check numbers a recorded in the Apricot database, the disposition of those checks (whether Cleared or Voided) is yet to be determined. The determination of the disposition of a check is referred to as resolving the check. Checks are resolved by a pair of Quickbooks reports created as Excel spreadsheets after the bank used by Operation ID has reported on cleared checks. The first report, called the Cleared Checks Report, lists checks cleared by the bank by check number. The second report, called the Voided Checks Report lists by check number checks that are more than 90 days old and have been voided by Operation ID in instructions sent by Quickbooks to the bank. Neither the Cleared Checks Report nor the Voided Checks Report in a format that can be consumed by Apricot to update its database. Apricot is very stringent about the format a file must be in in order to be used to update its database. OPID Daily is used to consume the Cleared Checks Report and the Voided Checks Report to resolve checks in its own database. After resolving checks listed on these reports, application OPID Daily constructs an Excel spreadsheet in the stringent format required by Apricot in order to update its database. This spreadsheet is referred to as an importme file, because it is imported into Apricot through the Apricot Administrator interface. This is how the Apricot database is kept in synch with the OPID Daily database. Again, if record keeping functions were provided in OPID Daily, then keeping the Apricot database in synch with the OPID Daily database would not be necessary. Bidirectional updating is the means that has been worked out in order to realize the benefits of both Apricot and OPID Daily. This has resulted in a hybrid system.","title":"Two Databases and a Hybrid System"},{"location":"Database/#connection-string","text":"In the desktop environment, SSMS was used to create an empty project database by executing the SQL query create database OpidDailyDB The Visual Studio Server Explorer (found under the OPIDDaily project View menu) was then used to discover the connection string to database OpidDailyDB by creating a new Data Connection to it and copying the Connection String property of the data connection into the <connectionStrings> section of Web.config. <add name=\"OpidDailyConnectionString\" connectionString=\"data source=DESKTOP-0U83VML\\SqlExpress;initial catalog=OPIDDailyDB;Integrated Security=True\" providerName=\"System.Data.SqlClient\" /> This value is accessed on files IdentityDB.cs and OpidDailyDB.cs by reading the value of the static variable Config.ConnectionString: public static string ConnectionString { get { // The value of OpidDailyConnectionString configured on Web.config is overwritten at AppHarbor deployment time. return ConfigurationManager.ConnectionStrings[\"OpidDailyConnectionString\"].ToString(); } } The value of OpidDailyConnectionString is overwritten at application deployment time at AppHarbor. This is accomplished by setting the connection string alias for the SQL Server add-on at AppHarbor to be OpidDailyConnectionString . To set this alias, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link Go to SQL Server on the page that appears. Click the button labeled \"Edit database configuration\" to set OpidDailyConnectionString as the alias value for the connection string. When this is done, OpidDailyConnectionString will appear as the value of SQLSERVER_CONNECTION_STRING_ALIAS in the Configuration variables section of application OPIDDaily at AppHarbor. When application OPIDDaily is deployed at AppHarbor, this alias will overwrite the configured value of OpidDailyConnectionString on file Web.config by the value of the connection string for the AppHarbor database. If you run the OPIDDaily application on the desktop and see the error the error The wait operation timed out it may not point to an error with the connection string but to a problem with SQL Server itself. Try opening the Windows 10 Services App and restarting the SQL Server (SQLEXPRESS) service. If all else fails try restarting the computer. This sometimes clears up connection problems. The online version of OPIDDaily is hosted as an application at AppHarbor and it uses a database server provided as an add-on. The add-on database server includes a database which serves as the application database, so it is not necessary to create the application database as was done above for the desktop version.","title":"Connection String"},{"location":"Database/#database-diagram","text":"The diagram was created by SSMS, copied to the clipboard (using the \"Copy Diagram to Clipboard\" command found on the freespace context menu) and then pasted into the Paint tool. Inside of Paint it is saved as a .PNG file. Version 18.0 of SSMS does not allow database diagrams to be created. Newer releases have restored this capability. But the diagram seen here was created by an earlier release of SSMS, which did have the ability to create database diagrams. The main tables of application OPIDDaily are the tables Clients , TextMsgs , RChecks and AncientChecks . The table Clients stores clients, their service requests and their supporting documents. This is the reason that table Clients has so many data fields. Some of this data could have been factored out by the use of many-to-many relationship tables, but it was decided that a single table would be simpler. Notice that many of the data fields in table Clients are bit fields. As a result, a record in the table does not consume much database storage. The table TextMsgs is related to the Clients table by the foreign key Id as there is a one-to-many relationship between a client and the messages that have been written concerning the client. The table RChecks is used to store check data from Apricot. The RecordIID and InterviewRecordID data fields identify a client in the Apricot database and a particular visit the client has made to Operation ID. The service history of a client consists of the checks that have been issued to the client. To retrieve the service history from the table RChecks the client is looked up by last name and DOB. This is not guaranteed to be a unique lookup, but it almost always is. The name RChecks is short for research checks . The RChecks table was so named because checks whose disposition is unknown are said to be under research until their disposition is resolved. The AncientChecks table was added to relieve the overcrowding of the RChecks table. It has exactly the same data fields as table RChecks . Originally table RChecks contained all the checks that have been issued by Operation ID. The table was built by adding one year's worth of checks at a time, which resulted in gateway timeouts at AppHarbor during update operations as the table grew larger. So the table was split up into 2 tables by years. Currently the RChecks table contains checks from the years 2018-2020 and the AncientChecks table contains checks from the years 2016 and 2017. The PocketChecks table was added in June 2020 to provide a means of entering check data directly into OPID Daily instead of having to wait for checks to be imported from Apricot. A pocket check is created when adding a new check to the visit history of an existing client or when adding a new check to an express client. Only users in role Interviewer or Back Office can create a pocket check. A pocket check is marked as active when it is first created and is marked inactive when a check with the same check number is imported from Apricot. Pocket checks enable a seamless transition to operations under OPID Daily alone. Once imports from Apricot are no longer performed the checks in the RChecks and AncientChecks tables will eventually be older than the data retention guidelines. There exist checks going back to the year 2013 when a Microsoft Access database was used to manage clients; however, the disposition of checks from these early years was not stored along with the check numbers in the Access database. These check numbers were migrated to the Apricot database as part of a client's service history, but the check numbers from the years 2013-2015 were not entered into the OPIDDaily database because it was believed that clients from these years would rarely return to Operation ID. This has saved valuable storage space in the OPIDDaily database. The free deployment of the OPIDDaily website is given 20MB of data space on a shared SQL Server. According to the SSMS Disk Usage report, 12.31MB of the current 15.25MB allocated data space is in use. Thus, 80% of the current allocation is in use and only 61% of the allowed 20MB allocation is in use. So there is still adequate free space for more check data. At some point it will be useful to reconsider the twice-in-a-lifetime policy for client service. Ten years from now it is unlikely that clients from fifteen years in the past will come to Operation ID seeking service. So there is no advantage to storing checks from the distant past. A data retention policy should be formulated to purge the database of old records. Social Solutions does not allow customers to delete records from their own Apricot database. To delete records a customer must request that Social Solutions do it for them. The SuperAdmin user of the OPID Daily website has the ability to delete a year's worth of checks at a time. If a data retention policy of 5 years were put in place, then the OPID Daily website would easily be able to enforce it. Furthermore, it would simplify part of the user interface as there would be no need to consult Apricot for information not in the OPID Daily database: all the check data for the past 5 years could be stored. And instead of a twice-in-a-lifetime policy, Operation ID could support a twice-in-five-years policy. This would make OPIDDaily a self-contained website. When a client appears for service, a check is first made in the Apricot database to see if the client has a visit history. If the client does have a visit history and this history does not include any checks before the year 2016, then a Service Ticket including all previous service for the client can be generated at the front desk. If a client's visit history includes visits prior to 2016, then these visits will be included on the Service Ticket by consulting the Apricot database. In most cases, the checks from visits prior to 2015 will not include a disposition and their disposition will need to be determined in the back office by consulting the Quickbooks ledger. The inconvenience of this is outweighed by its rarity and the desire to stay below the 20MB free limit of data storage at AppHarbor. The table ELMAH_Error stores error messages generated by uncaught application errors. See the section on ELMAH on the Infrastructure tab. The table AppLog contains the log messages generated by the application using log4net. See the section on log4net on the Infrastructure tab. The _MigrationHistory table stores the Entity Framework Code First migrations that have been applied to the database. This table is defined by and managed by Entity Framework Code First. The 3 AspNet tables in the center the above diagram are created by ASP.NET Identity 2.0 to manage registered users of OPIDDaily. The 3 tables are managed by their own data context which cannot be augmented by additional tables. However, data fields can be added to table AspNetUsers if necessary. The data field AgencyId was added to table AspNetUsers to store the unique AgencyId stored in table Agencies by the Superadmin user. (The AgencyId of an Operation ID user - example the TicketMaster user - is always 0.) The non-ASP.NET Identity tables in the diagram are managed by a separate data context. The Visual Studio project OPIDDaily has 2 data contexts called IdentityDb and OpidDailyDB. (See the section Entity Framework Code First of the Infrastructure tab.) The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video .","title":"Database Diagram"},{"location":"Database/#adding-migrations-at-appharbor-via-script","text":"To generate a script for the Up methods of the most recent migration(s), go back in the migration history to where the recent migrations start. For example, the migration preceding the migration ExpressClient was the migration PXXA. Therefore, to get a script for migration ExpressClient, execute the command update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:PXXA The generated script can be run against the database at AppHarbor using SSMS. The script should be run before the code is updated at AppHarbor. The script will also update the _MigrationHistory table. If the script involves adding a table, then running it before the code is updated at AppHarbor will ensure that the table is ready for use when the code that references it is deployed.","title":"Adding migrations at AppHarbor via script"},{"location":"Database/#backing-out-migrations-via-script","text":"To generate a script to run the Down methods of multiple down-migrations, do, for example, PM> Update-Database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -TargetMigration: ExpressClient This command will create a script to execute in SSMS the Down methods of all migrations since (and not including) migration ExpressClient. It is important to be able to generate this script if changes need to be backed out, because the deployed versions of application OPIDDaily cannot be managed by the Package Manager (PowerShell) window available in Visual Studio. The script to execute the Down methods must be run in SSMS. After the script has been executed, the reverted migration may be safely deleted from the set of migrations maintained for the data context.","title":"Backing out migrations via script"},{"location":"Database/#creating-a-cloned-copy-of-opiddaily","text":"To create a cloned copy of OPIDDaily from the GitHub repository, supply the repository address https://github.com/tmhsplb/opiddaily.git to Visual Studio. Before running the project for the first time, use SSMS to setup a database called OpidDailyDB, as described in the SQL Server Express and SSMS section of the Infrastructure tab. It will be necessary to follow the instructions for creating the Invitations table given in the section titled The Staging and Training Versions of OPIDDaily on the Infrastructure tab. The missing migrations mentioned in that section can be applied to the database of the cloned version only by receiving an emailed copy of the referenced update-database script. It will be necessary to be made a collaborator on the project in order to commit changes in the cloned copy to the GitHub repository for OPIDDaily.","title":"Creating a Cloned Copy of OPIDDaily"},{"location":"Database/#database-duplicates","text":"There are still many duplicates in the Apricot database as a result of the initial build from the legacy Access database. As an example, there are two records for the client John Harmon, DOB 9/11/1983: John Harmon, Record ID: 271978 John Marshall Harmon, Record ID: 4299 Since Service Tickets are built based on last name and DOB, both records will be used in building the Service Ticket for John Harmon. This can cause some confusion! Duplicate records are merged when they are discovered. The records for John Harmon may have been merged when the reader reads this documentation.","title":"Database Duplicates"},{"location":"Database/#managing-users","text":"OPIDDaily is a role-based database application administered by a Superadmin user. The Superadmin user has the responsibilty of establishing a login account for each OPIDDaily user, which includes the user's role. This is done to prevent a user from specifying his/her own role when logging in and to force the user into his/her assigned role instead. See the introduction and Role Controllers sections of the Implementation tab for a discussion of roles. The Superadmin will be given a user name and email address for a new user. For example, if Mary Atwood would like to use the user name Mary and email address maryatwood@gmail.com, this request would be given to the Superadmin user. Provided that the user name Mary is not already in use, the Superadmin user would use a private interface to enter Mary Atwood in the Invitations table under UserName Mary (with FullName Mary Atwood) and Email Address maryatwood@gmail.com. The Superadmin would also use the OPIDDaily interface to assign a role to user Mary Atwood in the Invitations table. The record in the Invitations table is in effect an invitation for Mary Atwood to register under user name Mary and email address maryatwood@gmail.com in the assigned role. The Superadmin will notify Mary that her account has been created and that she may register with application OPIDDaily using the credentials she has supplied together with a password of her own choosing. When Mary registers, the user name and email address she provides will be checked against the Invitations table. If this pair of credentials is not found in the Invitations table, Mary's attempt to register will be rejected. If they are found, a record will be created for her in the AspNetUsers table using the password she has specified and using the role assigned by the Superadmin, which has been stored in the Invitations table. On subsequent visits to OPIDDaily, Mary may simply login with the credentials established by her registration. When logged in she will be recognized in her assigned role. In practice, the Superadmin has created all login accounts and registered all users. This implies that passwords are not secret. However, this has not been an issue. For the convenience of not having to self-register users have been willing to sacrifice a little privacy. User email addresses do not need to be unique per account. This is not the default behavior; it is enabled by the setting RequireUniqueEmail = false in method ApplicationUserManager.Create on file App_Start/IdentityConfig.cs There are two special accounts reserved for usage by the two users who serve at the front desk on any given day of operation. Each of these accounts has the pre-assigned role called FrontDesk. The users are the Screener and the TicketMaster which correspond to the pipeline stages Screening and Checkin , respectively. (See the Background tab for information about the pipeline stages.) Having dedicated accounts avoids the need to create unique accounts in the role of FrontDesk. There are also two additional special users called Client1 and Client2 corresponding to the pipeline stages Screening and CheckIn , respectively. (See the Background tab for information about the pipeline stages.) During the screening stage, the Screener user will enter the name and date of birth of an entering client into the OPIDDaily database. To ensure that this information has been correctly entered, the Screener may click a button to have this information appear on a small tablet computer which will be handed to the client for verification. This small tablet computer will be logged into the OPIDDaily application as Client1 . During the Checkin stage, by consultng the Apricot database, the TicketMaster user will record any previous visit history by a screened client in the OPIDDaily database. If previous visits indicate that the screened client is ineligible for a service being sought, the TicketMaster may click a button to have the visit history appear on a second small tablet computer which will be handed to the client. This second small tablet computer will be logged into the OPIDDaily application as Client2 . Both users Client1 and Client2 are assigned the role FrontDesk.","title":"Managing Users"},{"location":"Database/#database-utilization","text":"AppHarbor allows 20MB of database storage with the free SQL Server. The current utilization can be checked at AppHarbor by selecting the SQL Server used by application OPIDDaily and following the \"Go to server\" link. As of February 8, 2020 10.2MB out of the free 20MB are in use. This database contains 32,500 checks which were cut since 2013. Of these, between 24,00 and 28,000 checks were cut over the past 4 years. This suggests that there is enough capacity in the free database to run for at least 3 more years before approaching the 20MB limit. The disk utilization should be monitored periodically to make sure the limit is not exceeded. Currently an upgrade to 10GB of space would cost $10/month. This should be an affordable expense when the time comes that the additional space is needed. SSMS can be used to check on the utilization of a database. To do so: Right click a database name Navigate to Reports > Standard Reports > Disk Usage Navigate to Reports > Standard Reports > Disk Usage By Table Although this can be done, the single number reported by AppHarbor is easier to understand.","title":"Database Utilization"},{"location":"Database/#rebuilding-the-research-table","text":"In February 2020 application OPIDChecks was merged into application OPIDDaily for the sake of having a single application that performs both functions. The merge required the Research Table to be added to application OPIDDaily. The merger of the two applications had the additional benefit of allowing application OPIDDaily to use the Research Table to populate the visit history of clients with previous visits. The Research Table was rebuilt by loading files created by two different reports in Apricot: The report Bounded Research Table was used to create files: Bounded Research Table 2017 Bounded Research Table 2018 Bounded Research Table 2019 Bounded Research Table 2020 The report Imported from Access DB was used to create files: Imported from Access DB 2010 Imported from Access DB 2011 Imported from Access DB 2012 Imported from Access DB 2013 Imported from Access DB 2014 Imported from Access DB 2015 Imported from Access DB 2016 The files Imported from Access DB 2010 Imported from Access DB 2011 Imported from Access DB 2012 Imported from Access DB 2013 Imported from Access DB 2014 Imported from Access DB 2015 contain very little (and incomplete) data and were not loaded to rebuild the Research Table. The Research Table acutally consist of 2 tables: RChecks and AncientChecks . See the section on the Database Diagram for more information. Only the SuperAdmin user can build the database, using the Rebuild menu item on the Superadmin mneubar. Files Bounded Research Table 2018 Bounded Research Table 2019 Bounded Research Table 2020 are loaded into table RChecks under the Merge Bounded Research File section and files Bounded Research Table 2017 Imported from Access DB 2016 are loaded into table AncientChecks under the Merge Ancient Checks File section. At the end of year 2021, there will be 6 full years of checks in the Research Table(s). Between now (May 2020) and then a 5 year data retention policy should be put in place. This will allow OPIDDaily to easily enforce a twice-in-5-years check policy without having to consult the Apricot database. The current twice-in-a-lifetime check policy requires consulting Apricot for checks dating back to when record keeping started and will become an unecessary burden as the years pass.","title":"Rebuilding the Research Table"},{"location":"Database/#updating-the-opiddaily-database","text":"It is necessary to keep the Research Table updated. This is done on a weekly basis by running the Apricot report called OPID Daily and providing the date of the Monday beginning a week of operation after the week is complete. For example the OPID Daily report should be run using Monday February 10, 2020 as the filter date on any day following Thursday February 13, 2020. This will create a file containing all the records modified during the week starting on Monday February 10. By merging the created file into application OPIDDaily, the Research Table will be brought up to date.","title":"Updating the OPIDDaily Database"},{"location":"Implementation/","text":"Implementation Application OPIDDaily is implemented as an ASP.NET Framework application using the ASP.NET MVC 5 project template provided by Visual Studio 2019 (Community Edition). OPIDDaily is implemented as an ASP.NET Foundation application and as such is hosted by IIS. Since cross platform availability is not a goal of OPIDDaily, there is no need to port it to ASP.NET Core. However, it may be useful to provide an alternative to AppHarbor as a hosting service. OPIDDaily uses ASP.NET Identity 2.0 to define a set of user roles. Each user role is associated with a separate MVC controller. A key feature is the use of controller inheritance (see the SharedController section below) to share editing functionality across user roles. Most of the OPIDaily interface is built around the JavaScript component jqGrid. This component provides a powerful user interface for CRUD operations, relieving the application of the burden of providing such an interface. (See the 2 sections on the jqGrid and the dashboard below.) The graphical user interface of OPIDDaily is styled using Bootstrap 3.0.0. Each user role is associated with its own layout file which defines a Bootstrap navbar containing links to the OPIDDaily features available to users in the role. The ASP.NET Identity system ensures that a user in a specified role cannot visit any pages outside of those allowed to users in that role. (See the section on Role Controllers.) Broadcast communication in OPIDDaily is made possible by the use of SignalR. Architecture Application OPIDDaily uses the well-defined architecture of an MVC application. The presentation layer is defined by the collection of controllers and corresponding views. Models The Models folder contains the models used by application OPIDDaily. Each model name with suffix ViewModel is a view model used to pass data from a controller to a view. Data Access Layer The collection of classes in the DAL folder comprise the data access layer. Controllers retrieve database data by invoking methods of the classes in the data access layer. The data access layer contains many methods that convert back and forth between entities and view models. For example, method ClientEntityToClientViewModel returns a ClientViewModel representing the ClientEntity which it receives as an argument. And method ClientViewModelToClientEntity converts a ClientViewModel passsed as its first argument to a Client entity passed as its second argument. This back and forth conversion enforces a separation of concerns. Database Entities The Entities folder contains classes representing the database entities. There is one database entity corresponding to each application database in the OPIDDaily database. Data Contexts Application OPIDDDaily uses 2 data contexts: IdentityDB and OpidDailyDB. These contexts are defined as classes in the DataContexts folder. The OpidDailyDB class creates the OpidDailyDB context and Entity Framework Code First uses it to create tables in the database corresponding to the entity classes in the Entities folder. Thus, for example, the class variable declaration public DbSet<Client> Clients { get; set; } causes Entity Framework to create table Clients in the database. Scripts The Scripts folder contains the JavaScript scripts used by application OPIDDaily. The MVC application template provisions the application with a set of generally useful scripts in an MVC application. Other scripts in the Scripts folder are added by adding packages through the NuGet package manager. The scripts in the subfolders ClientHistory Clients Conversation Dashboards PocketChecks ReviewClients all define jqGrids used by views in the application. The large number of jqGrids is evidence of the importance of this component to application OPIDDaily. Users OPIDDaily is a role-based system. Each registered user will be assigned a user role by the OPIDDaily administrator. The role that a user is assigned will determine the OPIDDaily features available to the user. A user's assigned role will depend upon wheterh thr user volunteers at the front desk, is a volunteer interviewer or is a back office volunteer. There will also be a SuperAdmin role. The single user in this role, the Superadmin, will have access to features necessary for the maintenance of application OPIDDaily. The credentials for this account will only be available to a select few individuals. The SuperAdmin User OPIDDaily defines a pre-registered SuperAdmin user who has privileges to create new roles invite new users to register in a pre-determined role add new agencies The credentials for the SuperAdmin user are configured on file Startup.cs. File Startup.cs is where the Superadmin role is created. There is only a single user, the SuperAdmin, with role of Superadmin. MVC Routing Application OPIDDaily uses only the default routing rule supplied by the Visual Studio MVC 5 template. This default routing rule is found in `.../App_Start/RouteConfig.cs`. routes.MapRoute( name: \"Default\", url: \"{controller}/{action}/{id}\", defaults: new { controller = \"Users\", action = \"Index\", id = UrlParameter.Optional } ); For the sake of simplicity, future development of application OPIDDaily should strive to keep this as the one and only routing rule. Role Controllers There is an MVC controller defined for each role defined by the superadmin user. Each controller defined for a role inherits from SharedController to implement shared functionality. The role controllers manage the views of application OPIDDaily. The implementation of each role controller defines methods that are accessible through the menubar defined on the layout file for the role. For example, the FrontDeskController - which implements the FrontDesk role - contains methods ExpressClient and ExistingClient (found on the SharedController) invoked from the menubar defined on file ~/Shared/_FrontDesk.cshtml. This is the layout file for the FrontDesk role. Each view returned by the FrontDeskController includes this layout file, thereby ensuring that a user in the role of FrontDesk will only invoke methods defined by the FrontDeskController. Each other role controllers is implemented the same way: each has a defined layout file that is included in each view returned by the controller. The layout file defines a menubar that specifies the methods that users in the role can invoke. As protection against unauthorized access to methods of a role controller, use of each role controller is limited to users in the role associated with the controller. For example, the FrontDeskController is protected by the annotation [Authorize(Roles = \"FrontDesk\")] Access is then restricted by the functionality of ASP.NET Identity to authenticated users in role FrontDesk. A quirk of role-based authorization of controllers occurs when a new user is registered in a role. The AccountController register method redirects a newly registered user to the Index method of the UsersController. This method then in turn attempts to redirect the user to the Home method of the appropriate controller, based on the newly registered users role. For example, a newly registered user in the role BackOffice will be redirected to the Home method of the BackOfficeController. Here lies the rub. The BackOfficeController is decorated with the attribute [Authorize(Roles = \"BackOffice\")] but the newly registered user, even though he/she is in role BackOffice as determined by the UsersController is blocked by the attribute! The attribute causes the user to be redirected to the Login method of the AccountController. This may be by design and logging in with the same credentials just supplied in the registration process works. But it would seem that the act of registering should cause the new user to be logged in upon registration. When a new session is started, a user registered in role BackOffice is not blocked by the Authorize attribute even though the login workflow is basically the same as the registration workflow. Go figure! In any event, registration of new users for application OPIDDaily has been handled by the SuperAdmin user who therefore knows the password of each user. But this is not a problem because users do not have access to private data. User accounts are simply established to separate users from each other and to restrict their access to portions of the website. The SharedController Each role controller derives from SharedController. The SharedController implements the shared editor functionality available to the different roles. The FrontDeskController, the BackOfficeController, the InterviewerController and the CaseManagerController all inherit from the SharedController. Deriving role controllers from a shared controller is an extremely useful technique for building a role-based editor. The UsersController The UsersController controls access to the ASP.NET Identity tables used to store registered users and the roles they are in. The method UsersController.Index is the entry point for an authenticated user. The role an authenticated user is in determines the method the user will be redirected to from this entry point. jqGrid The entire implementation of application OPIDDaily is structured around instances of jqGrid appearing in MVC Views. Each jqGrid is initially populated by a call to an MVC action made through the url property of the grid. For example, the clientsGrid on view FrontDesk/Clients.cshtml is initially populated by the call \"@Url.Action(\"GetClients\", \"FrontDesk\")\" which is the value of the url argument to grid clientsGrid. (Method GetClients is found on the SharedController.) The clientsGrid is updated by SingalR without requiring a page refresh! This enhances the interactivity of the OPIDDaily application. Each instance of a jqGrid defines a pager , which defines the CRUD operations supported by the grid. Each CRUD operation is implemented by an MVC action of the role controller associated with the grid. There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily. Dashboard The jqGrid used to represent the dashboard of service requests supports filtered server side searching. Performing filtered searches on the server side is necessary in order to preserve pagination, which will be needed if there are more than 25 service requests active at any given time. The dashboard is sorted by ascending order of service request expiry. A service request rolls off the dashboard once its expiry has passed. The dashboard is made searchable via the configuration: jQuery(\"#dashboardGrid\").jqGrid('filterToolbar', { searchOperators: true }); Each column which supports filtered searching (example Agency Name) includes the parameter search: true in its column declaration. Filtered searching is implemented by the method SharedController/GetDashboard. The parameter sps of type SearchParameters is populated by the POST that calls method GetDashboard. The POST will include the search string as the value of the jqGrid column name, for example AgencyName or LastName. Method GetDashboard calls Clients.GetDashboardClients which in turn calls GetFilteredDashboardClients in case a filtered search has been requested. This is determined by the boolean variable _search. The value of _search will be true if a filtered search has been requested and false otherwise. The value of _search is false on the initial call to GetDashboard and on each call to refresh the dashboard. The parameter setting loadonce : false as part of the jqGrid declaration enables the server side filtering. Styling a jqGrid using jquery-ui The default jquery ui theme that comes with a Visual Studio 2019 project is referenced by adding the lines \"~/Content/themes/base/jquery-ui.css\", \"~/Content/themes/base/jquery.ui.theme.css\" to the Content/css bundle on AppStart/BundleConfig.cs and adding the link <link href=\"@Url.Content(\"~/Content/themes/base/jquery.ui.all.css\")\" rel=\"stylesheet\" /> to each file that contains a jqGrid. The file jquery.ui.all.css contains styling for all the jquery ui components. But this is not necessary for the OPIDDaily application, because the only jquery component contained on any page is the jqGrid component. Instead of linking in jquery.ui.all.css by the above link, replace this link by the link <link href=\"@Url.Content(\"~/Content/jquery.jqGrid/ui.jqgrid.css\")\" rel=\"stylesheet\" /> on each page that uses a jqGrid. This links in file ui.jqgrid.css that comes with the jqGrid download. This file has a problem with correctly displaying the caption of a jqGrid. A new version was downloaded from the jqDemos page and copying the displayed stylesheet to the file copied.ui.jqgrid.css. When this is linked in instead of the original ui.jqgrid.css the caption is displayed correctly. However, it still uses the default theme referenced above. To change the theme, go to the Download tab on jqueryui.com. Uncheck all components, since only the jqGrid needs to be styled and this is done by ui.jqgrid.css which came with the jqGrid download. At the bottom of the page, select a theme (the jqDemos page uses the Redmond theme) and click the Download button to get a .zip file. The .zip file includes files named jquery-ui.css and jquery-ui.theme.css. Rename them according to the selected theme (example: redmond-jquery-ui.css and redmond-jquery-ui.theme.css) and copy them to the project. The .zip file also contains an images folder which contains images referenced by the theme. Rename the existing Content/themes/base/images folder to be old-images and create a new images folder. Copy the images from the .zip file to the new images folder. Finally, on file AppStart/BundleConfig.cs, replace the lines providing the default theme (see above) by the lines \"~/Content/themes/base/redmond-jquery-ui.css\", \"~/Content/themes/base/redmond-jquery-ui.theme.css\" This will cause the styling on a page containing a jqGrid to be done according to the Redmond theme. jquery Datatables The Research Table is implemented as a server side datatable. It is dependent on 2 packages downloaded using the NuGet package manager: jquery.datatables v1.10.15 datatables.mvc5 v0.1.0 The first of the 2 packages adds folders: ~/Content/DataTables ~/Scripts/DataTables The packages should be loaded in the given order. NowServing By convention, wherever the code makes use of a variable with the name nowServing , its value is the id of a client that has been selected from a jqGrid. Understanding this convention greatly simplifies understanding the code base. When a row is selected from a client grid or dashboard the id of the selected row is the id of the client referred to by the row. This id is stored in the session context for ease access by any method in the code. This technique eliminates the need to pass the id as an argument to each method that needs it. It is in effect a means of managing session state across pages in the OPID Daily website in the same way that cookies are designed to manage state across web pages in a large website. See the NowConversing section for additional details. NowConversing When a row in the clientsGrid defined on FrontDeskClients.js is selected, the JavaScript function that is the value of the onSelectRow property of the grid is invoked. When the function is invoked, the value passed to its nowServing argument is the id associated with the client represented by the selected row. The function posts to the server side method NowConversing found on SharedController, passing the JavaScript variable nowServing in the post as a result of the line postData: { nowServing: nowServing } Method SharedController/NowConversing has an argument called nowServing. MVC data binding will cause this variable to be bound to the JavaScript variable in the post. The method NowConversing was originally called NowServing, but was changed when it was understood that it needs to return a JsonResult to the grid in order to support pagination. The name NowConversing was chose, because the call frequently initiates a conversation between two OPID Daily users. The JsonResult returned by NowConversing is the set of records that are the current focus of the jqGrid. When the JsonResult is returned, the code .trigger('reloadGrid', { fromServer: true }); chained to the call to NowConversing then forces the grid to reload with the grid data records in the JsonResult. It would be natural to follow this chained call by a call to set the selection: .trigger('reloadGrid', { fromServer: true }).jqGrid('setSelection', nowServing); but this DOES NOT WORK. Instead, it is necessary to wait for the reload to complete before the selection is set. loadComplete: function () { jquery(\"#dashboardGrid\").jqGrid('setSelection', lastServed); } The global JavaScript variable lastServed referenced here is set to the value of nowServing . There is a complicated interplay between the server side and the client side regarding the selection of a client being served. This interplay is at the heart of how the OPIDDaily application works. For this reason it is important to study method NowConversing and the several jqGrids which call it. One thing that will be noticed is that the JsonResult returned by method NowConversing depends upon wheter it is being called by an agency or by OPID. If it is being called by OPID, it returns records for a dashboard display, which may include records from clients being served by several different agencies. When it is called by an agency, the JsonResult includes only records from clients being served by that agency. Households Table Clients is a list of clients and their dependents. Most clients represent only themselves but some have dependents. A client together with any dependents he or she may have is referred to as a household . The head of a household is a top level member of the table Clients . The HH field of each top level client C is set to 0, that is C.HH = 0. If client D is a dependent of client C then the HH field of client D points back to client C through the setting D.HH = C.Id. In fact, dependent D belongs to a subgrid of the jqGrid used to render the Clients table. Dependent D is added to the subgrid whose parent entry is the entry for client C. If client C is has dependents, then C.HeadOfHousehold will be set to true and the row rendering client C in the jqGrid will indicate that client C is the head of a household. Conversations The OPID Daily application supports conversations between users through a sequence of text messages. There is a one-to-many relationship between each client and the messages concerning that client. Selecting a client from a client grid causes the conversation of text messages to open. Each message is tagged with a date, a sender and a receiver. The entire conversation is stored in the table TextMsgs which uses a client Id as a foreign key. (See the database diagram on the Database tab.). When a client is selected from a grid and a text message is sent, the client's row in the grid turns red, indicating waiting on receipt of a reply. The recipient of the message sees the row representing the client in the grid he/she is monitoring turn green, indicating that a message is waiting to be read. This simple mechanism enables communication between users of OPID Daily without the need for time consuming phone calls or difficult to manage emails. The AgencyId data field in table AspNetUsers Application OPIDDaily is used by users at Operation ID and by users representing agencies that refer clients to Operation ID. The AgencyId field in table AspNetUsrs will be set to 0 for Operation ID users (for example the TicketMaster user) and will be set to the AgencyId of their referring agency for other users. The AgencyId data field in the Clients table is set in the same way. So, the AgencyId of a client entered into the Clients table at Operation ID on a day of operation will be set to 0. The AgencyId of a client entered by a user representing a referring agency will be set to the AgencyId of the referring agency. Clients are tagged in this way to allow Operation ID users to see all clients and to allow a users from a referring agency to see only the clients referred by that agency. Date Management Dates are treated differently at AppHarbor from the way they are treated on the desktop. In both environments, a date is stored as the midnight hour. For example, March 26, 2020 is stored as 2020-03-26:00:00:00:000 in the desktop database as well as in the AppHarbor database. But if this date is stored in a ClientViewModel object and reported as the value of a jqGrid column using the date formatter, then the desktop reports it as 03/26/2020 whereas AppHarbor reports it as 03/25/2020. It is not clear why this happens, but a way to make both environments report 03/26/2020 is to offset the value stored in the view model by 12 hours to noon (2020-03-26:12:00:00:000). This is for done several DateTime data fields in Clients.ClientEntityToClientViewModel. (A 1 hour offset was tried first, but it did not work.) The same 12 hour offsetting was done with the Date field in a VisitViewModel. This can be found on file DAL/Visits.cs. When a jqGrid row containing a date is edited the date posted to the server is the midnight time, not the offset to noon time. When the row is retrieved in a ClientViewModel, the time will again be offset by 12 hour to noon and will thus display as intended in the jqGrid. The ServiceDate and Expiry data fields in table Clients When the TicketMaster adds a new client to the Clients* table, the date the client is entered is recorded in the ServiceDate data field of table Clients . The same date is recorded in the Expiry data field. When a Case Manager user enters a new client into the Clients** table, the ServiceDate data field is set to the date the client is entered, but the Expiry data field is set at least 30 days into the future. This expiry date will be printed on the Case Manager Voucher given to a client by his/her Case Manager. Application OPIDDaily will preload the Dashboard table seen by the TicketMaster with any clients whose Expiry date has not yet past. This will allow a client with a Case Manager Voucher to be tracked on the day he/she appears at Operation ID. It is the client's responsibility to appear at Operation ID on or before the expiry date printed on their voucher. SessionHelper The SessionHelper class found on fie DAL/SessionHelper.cs is the key method for managing state in application OPIDDaily. This class is used to store key value pairs in the sesssion context private to each authenticated user. Managing the value of NowServing is a key use of the SessionHelper. Instead of having methods called GetNowServing and SetNowServing, the SharedController uses polymorphism to define two methods called NowServing with different signatures. The NowServing method with zero arguments invokes method SessionHelper.Get and the NowServing method with optional argument nowServing invokes method SessionHelper.Set. These two NowServing methods are invoked by many methods on SharedController to implement editing functionality private to an authenticated user. Service Tickets A primary goal of application OPIDDaily is the production of Service Tickets . A service ticket is a single piece of paper that shows the services requested by a given client together with the documents the client is supplying in support of his/her service request. In addition, a service ticket provides a history of service requests from previous visits by the client, if any. Service Tickets are produced by the TicketMaster at the front desk for same-day-service clients. In practice, the TicketMaster only creates a Service Ticket for a client with a history of previous visits. This Service Ticket is attached to the referral letter for a same-day-service client and the client is picked up by an interviewer. The Service Ticket contains a section of supporting documents which an interviewer can use as a worksheet while interviewing a client. The supporting documents section is not filled out by the front desk admin. The interviewer will assist the client in filling out paperwork in support of service(s) sought. It has been suggested that a client's Service Ticket be forwarded to a back office admin before work on a client's paperwork begins. By passing the Service Ticket to a back office admin, a client's service voucher(s) can be cut and recorded in Apricot while the client is busy with paperwork. In effect a client's BackOffice stage can proceed in parallel with his/her Interviewing stage once the Service Ticket has been produced. A Service Ticket is specified by a Case Manager before production of a Case Manager Voucher. See the Remote Operation ID section of the Background tab for information about this. A Service Ticket for Back Office use is printed by an Operation ID employee monitoring the remote service requests that appear on the Back Office service request dashboard display. See the Remote Operation ID section of the Background tab for information about this. SignalR Application OPIDDaily uses version 2.4.1 of Microsoft's SignalR framework for real-time communications. SignalR is installed using the NuGet package manager. SignalR is a push-technology used by OPIDDaily to update displays without requiring a page refresh. The SignalR connection hub is defined on file DAL/DailyHub.cs. Invoking a method of DailyHub in the server side code causes a push-notification to go out to all clients registered to the hub. For example, when a case manager adds a new client to the jqGrid managed by the CaseManagerController the method CaseManagerController.AddMyClient will invoke method DailyHub.Refresh. This method executes hubContext.Clients.All.refreshPage(); which sends a push notification to all registered clients of the hub. The registered clients include all other users logged in any role. They will each see their view dashbords update without the need to refresh the dashboard. In this way all grids are kept in synch with the grid to which the client was added. SignalR is also used as the means to update a progress bar display used when processing external Excel files containing check data. The code for doing so is found in this excellent Code Project article . Application OPIDDaily makes use of this code. On the client side, the progress bar is included as a partial view by the statement @Html.Partial(\"_ModalProgressBar\") See for example file ~/Views/BackOffice/Merge.cshtml. On the server side the progress bar is updated by executing a call like DailyHub.SendProgress(\"Merge in progress...\", i, checkCount); found on file DAL/Merger.cs. SignalR is also used by both the special users Client1 and Client2 to refresh the tablet computer displays used by these users. (See the section Managing Users on the Database tab.) ExcelDataReader Application OPIDDaily uses version 3.60 of the ExcelDataReader package downloaded using the NuGet package manager. The package is used to read externally generated Excel data files containing check data that is imported into OPIDDaily. The code that makes use of the package is found in files Utils/ExcelData.cs and Utils/MyExcelDataReader.cs","title":"Implementation"},{"location":"Implementation/#implementation","text":"Application OPIDDaily is implemented as an ASP.NET Framework application using the ASP.NET MVC 5 project template provided by Visual Studio 2019 (Community Edition). OPIDDaily is implemented as an ASP.NET Foundation application and as such is hosted by IIS. Since cross platform availability is not a goal of OPIDDaily, there is no need to port it to ASP.NET Core. However, it may be useful to provide an alternative to AppHarbor as a hosting service. OPIDDaily uses ASP.NET Identity 2.0 to define a set of user roles. Each user role is associated with a separate MVC controller. A key feature is the use of controller inheritance (see the SharedController section below) to share editing functionality across user roles. Most of the OPIDaily interface is built around the JavaScript component jqGrid. This component provides a powerful user interface for CRUD operations, relieving the application of the burden of providing such an interface. (See the 2 sections on the jqGrid and the dashboard below.) The graphical user interface of OPIDDaily is styled using Bootstrap 3.0.0. Each user role is associated with its own layout file which defines a Bootstrap navbar containing links to the OPIDDaily features available to users in the role. The ASP.NET Identity system ensures that a user in a specified role cannot visit any pages outside of those allowed to users in that role. (See the section on Role Controllers.) Broadcast communication in OPIDDaily is made possible by the use of SignalR.","title":"Implementation"},{"location":"Implementation/#architecture","text":"Application OPIDDaily uses the well-defined architecture of an MVC application. The presentation layer is defined by the collection of controllers and corresponding views.","title":"Architecture"},{"location":"Implementation/#models","text":"The Models folder contains the models used by application OPIDDaily. Each model name with suffix ViewModel is a view model used to pass data from a controller to a view.","title":"Models"},{"location":"Implementation/#data-access-layer","text":"The collection of classes in the DAL folder comprise the data access layer. Controllers retrieve database data by invoking methods of the classes in the data access layer. The data access layer contains many methods that convert back and forth between entities and view models. For example, method ClientEntityToClientViewModel returns a ClientViewModel representing the ClientEntity which it receives as an argument. And method ClientViewModelToClientEntity converts a ClientViewModel passsed as its first argument to a Client entity passed as its second argument. This back and forth conversion enforces a separation of concerns.","title":"Data Access Layer"},{"location":"Implementation/#database-entities","text":"The Entities folder contains classes representing the database entities. There is one database entity corresponding to each application database in the OPIDDaily database.","title":"Database Entities"},{"location":"Implementation/#data-contexts","text":"Application OPIDDDaily uses 2 data contexts: IdentityDB and OpidDailyDB. These contexts are defined as classes in the DataContexts folder. The OpidDailyDB class creates the OpidDailyDB context and Entity Framework Code First uses it to create tables in the database corresponding to the entity classes in the Entities folder. Thus, for example, the class variable declaration public DbSet<Client> Clients { get; set; } causes Entity Framework to create table Clients in the database.","title":"Data Contexts"},{"location":"Implementation/#scripts","text":"The Scripts folder contains the JavaScript scripts used by application OPIDDaily. The MVC application template provisions the application with a set of generally useful scripts in an MVC application. Other scripts in the Scripts folder are added by adding packages through the NuGet package manager. The scripts in the subfolders ClientHistory Clients Conversation Dashboards PocketChecks ReviewClients all define jqGrids used by views in the application. The large number of jqGrids is evidence of the importance of this component to application OPIDDaily.","title":"Scripts"},{"location":"Implementation/#users","text":"OPIDDaily is a role-based system. Each registered user will be assigned a user role by the OPIDDaily administrator. The role that a user is assigned will determine the OPIDDaily features available to the user. A user's assigned role will depend upon wheterh thr user volunteers at the front desk, is a volunteer interviewer or is a back office volunteer. There will also be a SuperAdmin role. The single user in this role, the Superadmin, will have access to features necessary for the maintenance of application OPIDDaily. The credentials for this account will only be available to a select few individuals.","title":"Users"},{"location":"Implementation/#the-superadmin-user","text":"OPIDDaily defines a pre-registered SuperAdmin user who has privileges to create new roles invite new users to register in a pre-determined role add new agencies The credentials for the SuperAdmin user are configured on file Startup.cs. File Startup.cs is where the Superadmin role is created. There is only a single user, the SuperAdmin, with role of Superadmin.","title":"The SuperAdmin User"},{"location":"Implementation/#mvc-routing","text":"Application OPIDDaily uses only the default routing rule supplied by the Visual Studio MVC 5 template. This default routing rule is found in `.../App_Start/RouteConfig.cs`. routes.MapRoute( name: \"Default\", url: \"{controller}/{action}/{id}\", defaults: new { controller = \"Users\", action = \"Index\", id = UrlParameter.Optional } ); For the sake of simplicity, future development of application OPIDDaily should strive to keep this as the one and only routing rule.","title":"MVC Routing"},{"location":"Implementation/#role-controllers","text":"There is an MVC controller defined for each role defined by the superadmin user. Each controller defined for a role inherits from SharedController to implement shared functionality. The role controllers manage the views of application OPIDDaily. The implementation of each role controller defines methods that are accessible through the menubar defined on the layout file for the role. For example, the FrontDeskController - which implements the FrontDesk role - contains methods ExpressClient and ExistingClient (found on the SharedController) invoked from the menubar defined on file ~/Shared/_FrontDesk.cshtml. This is the layout file for the FrontDesk role. Each view returned by the FrontDeskController includes this layout file, thereby ensuring that a user in the role of FrontDesk will only invoke methods defined by the FrontDeskController. Each other role controllers is implemented the same way: each has a defined layout file that is included in each view returned by the controller. The layout file defines a menubar that specifies the methods that users in the role can invoke. As protection against unauthorized access to methods of a role controller, use of each role controller is limited to users in the role associated with the controller. For example, the FrontDeskController is protected by the annotation [Authorize(Roles = \"FrontDesk\")] Access is then restricted by the functionality of ASP.NET Identity to authenticated users in role FrontDesk. A quirk of role-based authorization of controllers occurs when a new user is registered in a role. The AccountController register method redirects a newly registered user to the Index method of the UsersController. This method then in turn attempts to redirect the user to the Home method of the appropriate controller, based on the newly registered users role. For example, a newly registered user in the role BackOffice will be redirected to the Home method of the BackOfficeController. Here lies the rub. The BackOfficeController is decorated with the attribute [Authorize(Roles = \"BackOffice\")] but the newly registered user, even though he/she is in role BackOffice as determined by the UsersController is blocked by the attribute! The attribute causes the user to be redirected to the Login method of the AccountController. This may be by design and logging in with the same credentials just supplied in the registration process works. But it would seem that the act of registering should cause the new user to be logged in upon registration. When a new session is started, a user registered in role BackOffice is not blocked by the Authorize attribute even though the login workflow is basically the same as the registration workflow. Go figure! In any event, registration of new users for application OPIDDaily has been handled by the SuperAdmin user who therefore knows the password of each user. But this is not a problem because users do not have access to private data. User accounts are simply established to separate users from each other and to restrict their access to portions of the website.","title":"Role Controllers"},{"location":"Implementation/#the-sharedcontroller","text":"Each role controller derives from SharedController. The SharedController implements the shared editor functionality available to the different roles. The FrontDeskController, the BackOfficeController, the InterviewerController and the CaseManagerController all inherit from the SharedController. Deriving role controllers from a shared controller is an extremely useful technique for building a role-based editor.","title":"The SharedController"},{"location":"Implementation/#the-userscontroller","text":"The UsersController controls access to the ASP.NET Identity tables used to store registered users and the roles they are in. The method UsersController.Index is the entry point for an authenticated user. The role an authenticated user is in determines the method the user will be redirected to from this entry point.","title":"The UsersController"},{"location":"Implementation/#jqgrid","text":"The entire implementation of application OPIDDaily is structured around instances of jqGrid appearing in MVC Views. Each jqGrid is initially populated by a call to an MVC action made through the url property of the grid. For example, the clientsGrid on view FrontDesk/Clients.cshtml is initially populated by the call \"@Url.Action(\"GetClients\", \"FrontDesk\")\" which is the value of the url argument to grid clientsGrid. (Method GetClients is found on the SharedController.) The clientsGrid is updated by SingalR without requiring a page refresh! This enhances the interactivity of the OPIDDaily application. Each instance of a jqGrid defines a pager , which defines the CRUD operations supported by the grid. Each CRUD operation is implemented by an MVC action of the role controller associated with the grid. There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily.","title":"jqGrid"},{"location":"Implementation/#dashboard","text":"The jqGrid used to represent the dashboard of service requests supports filtered server side searching. Performing filtered searches on the server side is necessary in order to preserve pagination, which will be needed if there are more than 25 service requests active at any given time. The dashboard is sorted by ascending order of service request expiry. A service request rolls off the dashboard once its expiry has passed. The dashboard is made searchable via the configuration: jQuery(\"#dashboardGrid\").jqGrid('filterToolbar', { searchOperators: true }); Each column which supports filtered searching (example Agency Name) includes the parameter search: true in its column declaration. Filtered searching is implemented by the method SharedController/GetDashboard. The parameter sps of type SearchParameters is populated by the POST that calls method GetDashboard. The POST will include the search string as the value of the jqGrid column name, for example AgencyName or LastName. Method GetDashboard calls Clients.GetDashboardClients which in turn calls GetFilteredDashboardClients in case a filtered search has been requested. This is determined by the boolean variable _search. The value of _search will be true if a filtered search has been requested and false otherwise. The value of _search is false on the initial call to GetDashboard and on each call to refresh the dashboard. The parameter setting loadonce : false as part of the jqGrid declaration enables the server side filtering.","title":"Dashboard"},{"location":"Implementation/#styling-a-jqgrid-using-jquery-ui","text":"The default jquery ui theme that comes with a Visual Studio 2019 project is referenced by adding the lines \"~/Content/themes/base/jquery-ui.css\", \"~/Content/themes/base/jquery.ui.theme.css\" to the Content/css bundle on AppStart/BundleConfig.cs and adding the link <link href=\"@Url.Content(\"~/Content/themes/base/jquery.ui.all.css\")\" rel=\"stylesheet\" /> to each file that contains a jqGrid. The file jquery.ui.all.css contains styling for all the jquery ui components. But this is not necessary for the OPIDDaily application, because the only jquery component contained on any page is the jqGrid component. Instead of linking in jquery.ui.all.css by the above link, replace this link by the link <link href=\"@Url.Content(\"~/Content/jquery.jqGrid/ui.jqgrid.css\")\" rel=\"stylesheet\" /> on each page that uses a jqGrid. This links in file ui.jqgrid.css that comes with the jqGrid download. This file has a problem with correctly displaying the caption of a jqGrid. A new version was downloaded from the jqDemos page and copying the displayed stylesheet to the file copied.ui.jqgrid.css. When this is linked in instead of the original ui.jqgrid.css the caption is displayed correctly. However, it still uses the default theme referenced above. To change the theme, go to the Download tab on jqueryui.com. Uncheck all components, since only the jqGrid needs to be styled and this is done by ui.jqgrid.css which came with the jqGrid download. At the bottom of the page, select a theme (the jqDemos page uses the Redmond theme) and click the Download button to get a .zip file. The .zip file includes files named jquery-ui.css and jquery-ui.theme.css. Rename them according to the selected theme (example: redmond-jquery-ui.css and redmond-jquery-ui.theme.css) and copy them to the project. The .zip file also contains an images folder which contains images referenced by the theme. Rename the existing Content/themes/base/images folder to be old-images and create a new images folder. Copy the images from the .zip file to the new images folder. Finally, on file AppStart/BundleConfig.cs, replace the lines providing the default theme (see above) by the lines \"~/Content/themes/base/redmond-jquery-ui.css\", \"~/Content/themes/base/redmond-jquery-ui.theme.css\" This will cause the styling on a page containing a jqGrid to be done according to the Redmond theme.","title":"Styling a jqGrid using jquery-ui"},{"location":"Implementation/#jquery-datatables","text":"The Research Table is implemented as a server side datatable. It is dependent on 2 packages downloaded using the NuGet package manager: jquery.datatables v1.10.15 datatables.mvc5 v0.1.0 The first of the 2 packages adds folders: ~/Content/DataTables ~/Scripts/DataTables The packages should be loaded in the given order.","title":"jquery Datatables"},{"location":"Implementation/#nowserving","text":"By convention, wherever the code makes use of a variable with the name nowServing , its value is the id of a client that has been selected from a jqGrid. Understanding this convention greatly simplifies understanding the code base. When a row is selected from a client grid or dashboard the id of the selected row is the id of the client referred to by the row. This id is stored in the session context for ease access by any method in the code. This technique eliminates the need to pass the id as an argument to each method that needs it. It is in effect a means of managing session state across pages in the OPID Daily website in the same way that cookies are designed to manage state across web pages in a large website. See the NowConversing section for additional details.","title":"NowServing"},{"location":"Implementation/#nowconversing","text":"When a row in the clientsGrid defined on FrontDeskClients.js is selected, the JavaScript function that is the value of the onSelectRow property of the grid is invoked. When the function is invoked, the value passed to its nowServing argument is the id associated with the client represented by the selected row. The function posts to the server side method NowConversing found on SharedController, passing the JavaScript variable nowServing in the post as a result of the line postData: { nowServing: nowServing } Method SharedController/NowConversing has an argument called nowServing. MVC data binding will cause this variable to be bound to the JavaScript variable in the post. The method NowConversing was originally called NowServing, but was changed when it was understood that it needs to return a JsonResult to the grid in order to support pagination. The name NowConversing was chose, because the call frequently initiates a conversation between two OPID Daily users. The JsonResult returned by NowConversing is the set of records that are the current focus of the jqGrid. When the JsonResult is returned, the code .trigger('reloadGrid', { fromServer: true }); chained to the call to NowConversing then forces the grid to reload with the grid data records in the JsonResult. It would be natural to follow this chained call by a call to set the selection: .trigger('reloadGrid', { fromServer: true }).jqGrid('setSelection', nowServing); but this DOES NOT WORK. Instead, it is necessary to wait for the reload to complete before the selection is set. loadComplete: function () { jquery(\"#dashboardGrid\").jqGrid('setSelection', lastServed); } The global JavaScript variable lastServed referenced here is set to the value of nowServing . There is a complicated interplay between the server side and the client side regarding the selection of a client being served. This interplay is at the heart of how the OPIDDaily application works. For this reason it is important to study method NowConversing and the several jqGrids which call it. One thing that will be noticed is that the JsonResult returned by method NowConversing depends upon wheter it is being called by an agency or by OPID. If it is being called by OPID, it returns records for a dashboard display, which may include records from clients being served by several different agencies. When it is called by an agency, the JsonResult includes only records from clients being served by that agency.","title":"NowConversing"},{"location":"Implementation/#households","text":"Table Clients is a list of clients and their dependents. Most clients represent only themselves but some have dependents. A client together with any dependents he or she may have is referred to as a household . The head of a household is a top level member of the table Clients . The HH field of each top level client C is set to 0, that is C.HH = 0. If client D is a dependent of client C then the HH field of client D points back to client C through the setting D.HH = C.Id. In fact, dependent D belongs to a subgrid of the jqGrid used to render the Clients table. Dependent D is added to the subgrid whose parent entry is the entry for client C. If client C is has dependents, then C.HeadOfHousehold will be set to true and the row rendering client C in the jqGrid will indicate that client C is the head of a household.","title":"Households"},{"location":"Implementation/#conversations","text":"The OPID Daily application supports conversations between users through a sequence of text messages. There is a one-to-many relationship between each client and the messages concerning that client. Selecting a client from a client grid causes the conversation of text messages to open. Each message is tagged with a date, a sender and a receiver. The entire conversation is stored in the table TextMsgs which uses a client Id as a foreign key. (See the database diagram on the Database tab.). When a client is selected from a grid and a text message is sent, the client's row in the grid turns red, indicating waiting on receipt of a reply. The recipient of the message sees the row representing the client in the grid he/she is monitoring turn green, indicating that a message is waiting to be read. This simple mechanism enables communication between users of OPID Daily without the need for time consuming phone calls or difficult to manage emails.","title":"Conversations"},{"location":"Implementation/#the-agencyid-data-field-in-table-aspnetusers","text":"Application OPIDDaily is used by users at Operation ID and by users representing agencies that refer clients to Operation ID. The AgencyId field in table AspNetUsrs will be set to 0 for Operation ID users (for example the TicketMaster user) and will be set to the AgencyId of their referring agency for other users. The AgencyId data field in the Clients table is set in the same way. So, the AgencyId of a client entered into the Clients table at Operation ID on a day of operation will be set to 0. The AgencyId of a client entered by a user representing a referring agency will be set to the AgencyId of the referring agency. Clients are tagged in this way to allow Operation ID users to see all clients and to allow a users from a referring agency to see only the clients referred by that agency.","title":"The AgencyId data field in table AspNetUsers"},{"location":"Implementation/#date-management","text":"Dates are treated differently at AppHarbor from the way they are treated on the desktop. In both environments, a date is stored as the midnight hour. For example, March 26, 2020 is stored as 2020-03-26:00:00:00:000 in the desktop database as well as in the AppHarbor database. But if this date is stored in a ClientViewModel object and reported as the value of a jqGrid column using the date formatter, then the desktop reports it as 03/26/2020 whereas AppHarbor reports it as 03/25/2020. It is not clear why this happens, but a way to make both environments report 03/26/2020 is to offset the value stored in the view model by 12 hours to noon (2020-03-26:12:00:00:000). This is for done several DateTime data fields in Clients.ClientEntityToClientViewModel. (A 1 hour offset was tried first, but it did not work.) The same 12 hour offsetting was done with the Date field in a VisitViewModel. This can be found on file DAL/Visits.cs. When a jqGrid row containing a date is edited the date posted to the server is the midnight time, not the offset to noon time. When the row is retrieved in a ClientViewModel, the time will again be offset by 12 hour to noon and will thus display as intended in the jqGrid.","title":"Date Management"},{"location":"Implementation/#the-servicedate-and-expiry-data-fields-in-table-clients","text":"When the TicketMaster adds a new client to the Clients* table, the date the client is entered is recorded in the ServiceDate data field of table Clients . The same date is recorded in the Expiry data field. When a Case Manager user enters a new client into the Clients** table, the ServiceDate data field is set to the date the client is entered, but the Expiry data field is set at least 30 days into the future. This expiry date will be printed on the Case Manager Voucher given to a client by his/her Case Manager. Application OPIDDaily will preload the Dashboard table seen by the TicketMaster with any clients whose Expiry date has not yet past. This will allow a client with a Case Manager Voucher to be tracked on the day he/she appears at Operation ID. It is the client's responsibility to appear at Operation ID on or before the expiry date printed on their voucher.","title":"The ServiceDate and Expiry data fields in table Clients"},{"location":"Implementation/#sessionhelper","text":"The SessionHelper class found on fie DAL/SessionHelper.cs is the key method for managing state in application OPIDDaily. This class is used to store key value pairs in the sesssion context private to each authenticated user. Managing the value of NowServing is a key use of the SessionHelper. Instead of having methods called GetNowServing and SetNowServing, the SharedController uses polymorphism to define two methods called NowServing with different signatures. The NowServing method with zero arguments invokes method SessionHelper.Get and the NowServing method with optional argument nowServing invokes method SessionHelper.Set. These two NowServing methods are invoked by many methods on SharedController to implement editing functionality private to an authenticated user.","title":"SessionHelper"},{"location":"Implementation/#service-tickets","text":"A primary goal of application OPIDDaily is the production of Service Tickets . A service ticket is a single piece of paper that shows the services requested by a given client together with the documents the client is supplying in support of his/her service request. In addition, a service ticket provides a history of service requests from previous visits by the client, if any. Service Tickets are produced by the TicketMaster at the front desk for same-day-service clients. In practice, the TicketMaster only creates a Service Ticket for a client with a history of previous visits. This Service Ticket is attached to the referral letter for a same-day-service client and the client is picked up by an interviewer. The Service Ticket contains a section of supporting documents which an interviewer can use as a worksheet while interviewing a client. The supporting documents section is not filled out by the front desk admin. The interviewer will assist the client in filling out paperwork in support of service(s) sought. It has been suggested that a client's Service Ticket be forwarded to a back office admin before work on a client's paperwork begins. By passing the Service Ticket to a back office admin, a client's service voucher(s) can be cut and recorded in Apricot while the client is busy with paperwork. In effect a client's BackOffice stage can proceed in parallel with his/her Interviewing stage once the Service Ticket has been produced. A Service Ticket is specified by a Case Manager before production of a Case Manager Voucher. See the Remote Operation ID section of the Background tab for information about this. A Service Ticket for Back Office use is printed by an Operation ID employee monitoring the remote service requests that appear on the Back Office service request dashboard display. See the Remote Operation ID section of the Background tab for information about this.","title":"Service Tickets"},{"location":"Implementation/#signalr","text":"Application OPIDDaily uses version 2.4.1 of Microsoft's SignalR framework for real-time communications. SignalR is installed using the NuGet package manager. SignalR is a push-technology used by OPIDDaily to update displays without requiring a page refresh. The SignalR connection hub is defined on file DAL/DailyHub.cs. Invoking a method of DailyHub in the server side code causes a push-notification to go out to all clients registered to the hub. For example, when a case manager adds a new client to the jqGrid managed by the CaseManagerController the method CaseManagerController.AddMyClient will invoke method DailyHub.Refresh. This method executes hubContext.Clients.All.refreshPage(); which sends a push notification to all registered clients of the hub. The registered clients include all other users logged in any role. They will each see their view dashbords update without the need to refresh the dashboard. In this way all grids are kept in synch with the grid to which the client was added. SignalR is also used as the means to update a progress bar display used when processing external Excel files containing check data. The code for doing so is found in this excellent Code Project article . Application OPIDDaily makes use of this code. On the client side, the progress bar is included as a partial view by the statement @Html.Partial(\"_ModalProgressBar\") See for example file ~/Views/BackOffice/Merge.cshtml. On the server side the progress bar is updated by executing a call like DailyHub.SendProgress(\"Merge in progress...\", i, checkCount); found on file DAL/Merger.cs. SignalR is also used by both the special users Client1 and Client2 to refresh the tablet computer displays used by these users. (See the section Managing Users on the Database tab.)","title":"SignalR"},{"location":"Implementation/#exceldatareader","text":"Application OPIDDaily uses version 3.60 of the ExcelDataReader package downloaded using the NuGet package manager. The package is used to read externally generated Excel data files containing check data that is imported into OPIDDaily. The code that makes use of the package is found in files Utils/ExcelData.cs and Utils/MyExcelDataReader.cs","title":"ExcelDataReader"},{"location":"Infrastructure/","text":"Infrastructure The infrastructure of project OPIDDaily refers to the tools and technologies used to develop it, exclusive of the implementation itself. This section will be useful to a developer wanting to maintain and further develop OPIDDaily. It describes both the desktop development environment and the AppHarbor deployment environment for the web application OPIDDaily. Hosting Environments There are 4 hosting environments for OPIDDaily: desktop, training, staging and production. They differ in the database connection string used by each. The connection string is configured as the value of variable OpidDailyConnectionString in the <connectionStrings> section of Web.config. The static value configured there is used by the desktop environment. The static value is overwritten by injection (at AppHarbor) when OPIDDaily is deployed to create a training, staging or production release. The transformation files Web.Staging.config and Web.Release.config play a role in these deployments. The staging deployment at AppHarbor (called stagedaily) has its Environment variable set to Staging to force Web.Staging.config to be used upon deployment. This is done in the Settings section of the deployed application at AppHarbor. (When Web.Staging.config was created, it was necessary to set its build action to Content in Visual Studio to include it in the build at AppHarbor. The same thing happened when Web.Training.config was created.) The production deployment at AppHarbor has its Environment variable set to Release by default. This causes Web.Release.config to be used upon deployment. The training release was the last release created. It was created by use of the Configuration Manager under the Visual Studio Build menu. In the Active solution configuration dropdown <New> was selected and Training was created with settings copied from the Staging configuration. Then Add Config Transform was selected from the context menu of Web.config. (This item is grayed out until a new configuration has been created.) This automatically added file Web.Training.config. The <appSettings> section of Web.Staging.config was copied to Web.Training.config. SEO OPIDDaily is a password protected application that is not intended to be discoverable by search engines. To prevent this it includes the following robots.txt file User-agent:* Disallow:/ This directs all search engines to ignore the entire site. Visual Studio Project To check out application OPIDDaily from GitHub into Visual Studio 2019 (Community Edition), clone https://tmhsplb@appharbor.com/opiddaily.git into a local folder after being made a collaborator on the project. Being a collaborator is necessary in order to commit changes to the GitHub repository. The Visual Studio project representing application OPIDDaily defines a role-based system. It was developed using the ASP.NET Identity 2.0 framework. A sample ASP.NET Identity 2.0 project was developed by Syed Shanu and described in the excellent CodeProject article ASP.NET MVC Security and Creating User Role . The sample project uses the Visual Studio MVC5 project template and makes use of Katana OWIN middleware for user authentication. The use of Katana is built into the ASP.NET Identity 2.0 provider used by the project template, as is explained in the CodeProject article. The OPIDDaily application was built with information taken from this article as well as the technique for maintaining 2 data contexts described in Scott Allen's Pluralsight video . On the Properties page of the Visual Studio project, remember to select Local IIS as the server and click the Create Virtual Directory button to set http://localhost/OpidDaily as the Project Url. These two actions create an application called OpidDaily under the Default Web Site in IIS and enable project OpidDaily to be run in a desktop version of IIS under this Url. Without this, the desktop IIS cannot be used to host the application. See the section on configuring IIS below. New development in the OPIDDaily Visual Studio project will be done in the staging branch and deployed to the stagedaily application at AppHarbor. (See the section on Deployment.) After changes to the staging branch have been tested in the desktop environment, using the Visual Studio GitHub interface they will be committed and then pushed to the staging branch at GitHub. After changes have been tested, they will be merged into the master branch of the project and from there deployed to application OPIDDaily at Appharbor. When the codebase is installed on a developer's Visual Studio instance on his/her machine by cloning the GitHub repository OPIDDaily , the developer must use Visual Studio to create a staging branch and then rebase this branch onto origin/master . This will cause the remote changes to appear in the local staging branch without the need to Fetch and Pull them as is done between a remote master branch and a local master branch. SQL Server Express and SSMS The desktop version of OPIDDaily makes use of a SQL Server Express to store information about clients. The database is managed by v18.0 of SQL Server Management Studio (SSMS). Visual Studio includes the ability to view an installed SQL Server Express database, but it is more convenient to have SQL Server Management Studio available for this purpose. SQL Server Express and SSMS require separate (lengthy) downloads. The SQL Server Express database for OPIDDaily was created by executing the SQL query create database OPIDDailyDB executed inside of SSMS. With this database selected in SSMS, there are two SQL queries that need to be executed to enable IIS to talk to SQL Server Express. The first query is CREATE USER [NT AUTHORITY\\NETWORK SERVICE] FOR LOGIN [NT AUTHORITY\\NETWORK SERVICE] WITH DEFAULT_SCHEMA = dbo; This query creates the database user NT AUTHORITY\\NETWORK SERVICE. The second query is EXEC sp_addrolemember 'db_owner', 'NT AUTHORITY\\NETWORK SERVICE' This query grants user NT AUTHORITY\\NETWORK SERVICE the necessary permissions to communicate with IIS. Finally, go into the security section of database OPIDDaily in SSMS. Expand Users and select the properties for user NT AUTHORITY\\NETWORK SERVICE. Select Membership from the Select a page section of the dialog box and tick the checkbox for db_owner. Performing this final change will permit user NT AUTHORITY\\NETWORK SERVICE to create tables in the database. This ability is necessary for the first the OPIDDaily application is run to enable it to automatically create the ASP.NET Identity tables in the database. Creating and configuring user NT AUTHORITY\\NETWORK SERVICE does not need to be performed at AppHarbor in order to communicate with IIS. See below for information about the AppHarbor deployment of OPIDDaily. It is also necessary to change the application pool identity of application OPIDDaily running under IIS to NETWORKSERVICE. See the section on configuring IIS. There is a bug in SSMS v18.0 that causes it to stop after launch; the splash screen will display and then SSMS will quit. The fix for this is to edit file ssms.exe.config found in folder C:\\\\Program Files (x86)\\Microsoft SQL Server Management Studio 18\\Common7\\IDE and remove (or comment out) the line which has the text: <NgenBind_OptimizeNonGac enabled=\"1\" /> This should be around line 38. Then restart SSMS. SSMS v18.0 does not have the capability to generate database diagrams. Previous versions of SSMS had this capability, but it was removed from v18.0. The capability has been added back to newer version of SSMS. SSMS can be used to connect to a remote database at AppHarbor. The credentials for the remote database can be discovered by clicking on the SQL Server add-on at AppHarbor and then following the Go to SQL Server link on the page that appears. Entity Framework Code First The Visual Studio project OPIDDaily has 2 data contexts called IdentityDB and OpidDailyDB. The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video . Following the video, supporting 2 data contexts in application OPIDDaily was enabled by some manual scaffolding in the codebase. This scaffolding consisted of creating a project folder called DataContexts with two subfolders: IdentityMigrations and OPIDDailyMigrations. Also, a new folder called Entities was added to the OPIDDaily Visual Studio Solution to contain the classes defining the entities used by the solution. Create the top level folder DataContexts and create class IdentityDB in this folder, as specified in Scott Allen's video. Note that it was found necessary to move the class ApplicationDbContext from file Models\\IdentityModels.cs on the video to file DataContexts/IdentityDb.cs to make things work. The class AppStart/IdentityConfig will need to be modified in order for the application to compile. Replace the line var manager = new ApplicationUserManager(new UserStore<ApplicationUser>(context.Get<ApplicationDbContext>())); by var manager = new ApplicationUserManager(new UserStore<ApplicationUser>(context.Get<IdentityDB>())); The class AppStart/StartupAuth will also need to be modified. Replace the line app.CreatePerOwinContext(ApplicationDbContext.Create); by the line app.CreatePerOwinContext(IdentityDB.Create); This will invoke the method IdentityDB.Create at the proper time. Then run the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.IdentityDB -MigrationsDirectory DataContexts\\IdentityMigrations This will create the file DataContexts\\IdentityMigrations\\Configuration.cs Next in method Configuration on this file set AutomaticMigrationsEnabled = true; to allow the ASP.NET Identity tables to be created when application OPIDDaily is run for the first time. Also add the line ContextKey = \"OPIDDaily.DataContexts.IdentityDB\"; to method Configuration. Before running the program for the first time, make sure the database OPIDDailyDB has been created in SQL Server and, if running in the desktop environment, user NT AUTHORITY\\NETWORK SERVICE has been created and configured. See the section SQL Server Express and SSMS above for instructions on how to do this. The CodeProject article (referenced in section Visual Studio Project) gives the Katana middleware code needed to cause the ASP.NET Identity tables to be created and the first user to be entered into them. This middleware code is found on the toplevel file Startup.cs. As specified on this file the first user is the SuperAdmin user, sa. When application OPIDDaily is run for the first time the ASP.NET Identity tables will be created when the middleware is executed. The user sa will then be found in table AspNetUsesrs . File Startup.cs is worth studying. After application OPIDDaily is run for the first time the database will contain not only the ASP.NET Identity table but also an entity framework table called _MigrationHistory. Entity Framework uses this table to record migrations. As a result of the creation of the ASP.NET Identity tables, a migration with MigrationId 201906051504117_InitialCreate was created in the _MigrationHistory table. Next, to initialize the OPIDDaily data context, run the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.OPIDDailyDB -MigrationsDirectory DataContexts\\OPIDDailyMigrations This will create the file DataContexts\\OPIDDailyMigrations\\Configuration.cs Next create the class DataContexts\\OpidDailyDB as per the video. This class is similar to the previously created DataContexts\\IdentityDB.cs. The first entity added to the OPIDDaily project was the class Entities\\Client. This entity was connected to the OPIDDDaily data context by the inclusion of the declaration public DbSet<Client> Clients { get; set; } on file DataContexts\\OpidDailyDB.cs. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Clients\" caused the migration 201906152111225_Clients.cs to be added to DataContexts\\OPIDDailyMigrations. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Clients to be created in database OPIDDailyDB by executing the Up method of the above migration. Notice that the Up method refers to two database columns, ReferralDate and AppearanceDate, which are not in the currently deployed version of table Clients. The data migration 201907122132153_RemoveTwoDates.cs was used to remove these columns when it was realized they would not be needed. Late in the development of OPIDDaily it was realized that the table Visits was not needed. Table Visits had been created when public DbSet<Visit> Visits { get; set; } was added to DataContexts\\OpidDailyDB.cs and class Entities\\Visit.cs was added to the set of database entities. Table Visits was then created by the Up method of migration 201907120006570_History.cs A new migration was created To delete table Visits from the database when it was realized that it was not needed. First the reference to the table was removed from DataContexts\\OpidDailyDB.cs: // public DbSet<Visit> Visits { get; set; } and all references to entity Visit were removed from the code. After re-compilation of the codebase, the command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"DeleteVisitsTable\" was run. The Up method of the resulting migration showed public override void Up() { DropForeignKey(\"dbo.Visits\", \"Client_Id\", \"dbo.Clients\"); DropIndex(\"dbo.Visits\", new[] { \"Client_Id\" }); DropTable(\"dbo.Visits\"); } This method indicates that the table Visits will be removed from the database together with its foreign key relationship to table Clients . Running the command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then removed table Visits from the database as desired. Notice that this is not the same procedure as running the Down method of migration 201907120006570_History.cs because running this Down method of this migration would invalidate all the migrations performed after it. Late in the development of OPIDDaily the entity Entities\\TextMsg.cs was added. This entity was connected to the OPIDDaily data context by the inclusion of the declaration public DbSet<TextMsg> TextMsgs { get; set; } on file DataContexts\\OpidDailyDB.cs. Since table TextMsgs was intended to be related to table Clients in the OPIDDailyDB by a foreign key, the declaration public ICollection<TextMsg> TextMsgs { get; set; } was added to class Entities\\Client.cs. Entity Framework Code First automatically detected this when the \"Conversations\" migration was created. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Conversations\" caused the migration 202005152029597_Conversations.cs to be added to folder DataContexts\\OPIDDailyMigrations. Studying the Up method of this migration, it is seen that the new table TextMsgs to be created will have a foreign key relationship to table Clients . Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table TextMsgs to be created in database OPIDDailyDB by executing the Up method of the above migration. As desired, table TextMsgs has a foreign key relationship to table Clients . To see this, use SSMS to study the columns of table TextMsgs . Each additional database change requires a pair of commands: an add-migration command followed by an update-database command. Executing an add-migration command creates a .cs file in the folder associated with the ConfigurationTypeName. Study this .cs file before executing the update-database command. If the database changes indicated in the .cs file are not correct, simply delete the .cs file before running the update-database command and then try again. See the section on the Database tab on using PowerShell to create script files to execute at AppHarbor. Configuring IIS Development of the OPIDDaily application was performed under IIS on the localhost machine. This was done so that the development environment would match thedeployment environment at AppHarbor as closely as possible. The localhost application server, Internet Information Services (IIS), was not pre-installed on the localhost; however, it is part of the operating system that can easily be activated. To activate IIS, go to the Programs section of the Control Panel and turn on the IIS feature: Programs > Programs and Features > Turn Windows features on or off > Internet Information Services After checking this box, expand it by clicking the plus sign (+) next to it and go to the section World Wide Web Services > Application Development Features In this section, check the checkboxes for ASP ASP.NET 3.5 ASP.NET 4.6 if they are not already checked. This will cause additional Application Pools to be made available to IIS. The OPIDDaily application is installed as an application under the Default Web Site in IIS as described in the section describing the Visual Studio Project. The Basic Settings dialog box for application OPIDDaily (accessible from the Actions pane of IIS), will give the physical path to the folder containing the source code as C:\\VS2019Projects\\OpidDaily\\OpidDaily This folder contains the project solution file, OPIDDaily.sln. Do not change it! Application OPIDDaily must be configured to use the application pool .NET v4.5. in the Basic Settings dialog box. (This application pool became available by enabling the features described above.) Finally, change the application identity of the selected application pool (.NET v4.5) to NetworkService. To do this, highlight Application Pools on the IIS Connections panel. This will cause the available application pools to appear in the IIS body panel. Highlight the .NET v4.5 application pool and then select Set Application Pool Defaults\u2026 to display a dialog box that will enable you to change the identity of the application pool. The dialog box is reachable either from the context menu of the highlighted application pool (under right mouse click) or from the Actions panel on the right of the IIS display. The dialog box contains a section labeled Process Model which contains an entry labeled Identity. Selecting the Identity entry adds an ellipsis next to the bold ApplicationPoolIdentity. Selecting the ellipsis brings up a dialog box with the pre-selected radio button Built-in account. Select NetworkService from the dropdown menu associated with this radio button. After approving this selection, the Identity column of the application pool .NET v4.5 will show NetworkService. See the section on SQL Server and SSMS for how to establish user NetworkService. Git for Windows Visual Studio 2019 (Community Edition) comes with built-in support for GitHub. A new project can be added to Git source control on the desktop by simply selecting Add to Source Control from the context menu of the Solution file in the Solution Explorer. Once a project is under Git source control it can be added to a remote GitHub repository by using tools available through Visual Studio. However, a technique preferred by many developers is to use Git for Windows . Git for Windows provides a BASH shell interface to GitHub which uses the same set of commands available at GitHub itself. Git for Windows integrates with Windows Explorer to allow a BASH shell to be opened on a project that has been added to a desktop Git repository. Simply point Windows Explorer at the folder containing the project solution file and select Git BASH Here from the context menu of the folder to open a Git for Windows BASH shell. As a first git command, try entering git --version in the shell. This will report the version of Git that has been installed by the download. After that simply execute Git commands from this shell window. Git for Windows also offers Git GUI, a graphical version of most Git command line functions. To open Git GUI simply select Git GUI Here from Windows Explorer. GitHub Application OPIDDaily is stored at GitHub as a repository under an account with the email address peter3418@ymail.com and account name tmhsplb. Only user tmhsplb can deploy directly to this repository. Any other user needing to deploy a version of OPIDDaily to this repository must be declared a collaborator on repository OPIDDaily by user tmhsplb. A collaborator is a user associated with a different account established at GitHub. Git for Windows was used to create a remote to save to this GitHub account. The remote was created in the Git BASH shell by opening the shell on the folder which contains the OPIDDaily.sln file (folder C:/VS2019Projects/OPIDDaily/OPIDDaily ) and issuing the command git remote add origin https://github.com/tmhsplb/opiddaily.git Creating this remote only needs to be done once, because Git for Windows stores the remote. To remove a remote use the command git remote rm myremote The need for this may arise if there was a typo in the creation of myremote. AppHarbor AppHarbor (appharbor.com) is a Platform as a Service Provider which uses Amazon Web Services infrastructure for hosting applications and Git as a versioning tool. When an application is defined at AppHarbor, a Git repository is created to manage versions of the application's deployment. The OPIDDaily application is defined as an application at AppHarbor to create the production repository of the desktop application. The staging version of the desktop application is defined by a repository called stagedaily. The remote configured for OPIDDaily at AppHarbor is: https://tmhsplb@appharbor.com/opiddaily.git This remote is configured from a Windows Git BASH shell by the command git remote add opiddaily https://tmhsplb@appharbor.com/opiddaily.git After the remote is configured in the Git BASH shell, issuing the command git push opiddaily master will deploy the master branch of solution opiddaily to AppHarbor as application OPIDDaily, accessible through the URL https://opiddaily.apphb.com If you reset your password at AppHarbor, the 'git push' command will no longer work from the Git BASH shell. You need to have Git prompt you for your new password. To do this on a Windows 10 machine, go to Control Panel > User Accounts > Credential Manager > Windows Credentials and remove the AppHarbor entry under Generic Credentials. The next time you push, you will be prompted for your repository password. Application OPIDDaily is deployed using the free Canoe subscription level at AppHarbor. Under a Canoe subscription, the IIS application pool of application OPIDDaily has a 20 minute timeout, which forces OPIIDDaily to spin up its resources again after 20 minutes of idle time. This has not been a problem at Operation ID, because application OPIDDaily is in continuous use on the days Operation ID is open. However, the 20 minute timeout for the free Canoe version at AppHarbor would become a problem if OPIDDaily were extended to add features suitable for use by agencies that partner with Operation ID. These agencies would require that OPIDDaily be available on demand. On demand service would require the use of a paid subscription level at AppHarbor. The free Yocto version of SQL Server is used as an add-on to the OPIDDaily deployment. The Yocto version has a limit of 20MB of storage space, which is adequate for many days of usage by Operation ID. However, the database usage must be monitored to avoid exceeding the 20MB limit. See the Database Utilization section on the Database tab for how to do this. A paid subscription to a SQL Server at AppHarbor would alleviate this problem. The Staging and Training Versions of OPIDDaily A staging version of application OPIDDaily was created from a Visual Studio staging branch by creating an application called stagedaily at AppHarbor. DO NOT CREATE A SEPARATE REPOSITORY FOR STAGEDAILY AT GITHUB. The remote configured for stagedaily at AppHarbor is: https://tmhsplb@appharbor.com/stagedaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/stagedaily.git After the remote is configured in the Git BASH shell, issuing the command git push stagedaily staging will deploy the staging branch of OPIDDaily to AppHarbor as application stagedaily, accessible through the URL https://stagedaily.apphb.com In the same way, a traindaily application at AppHarbor was created from a training branch in Visual Studio. When the stagedaily application was run for the first time, all the migrations up to the migration ending in 18024_AgencyId in OpidDailyMigrations in Visual Studio were automatically applied and added to the _MigrationHistory table. There were several missing migrations in the _MigrationHistory table because the application of migrations stopped due to migration 18024_AgencyId. This migration could not be applied because it references the Invitations table which was not created by any migration which preceded it. This required adding the Invitations table to the stagedaily database through SSMS by executing the following script: SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [dbo].[Invitations]( [Id] [int] IDENTITY(1,1) NOT NULL, [Extended] [datetime] NOT NULL, [Accepted] [datetime] NOT NULL, [UserName] [nvarchar](max) NULL, [FullName] [nvarchar](max) NULL, [Email] [nvarchar](max) NULL, [Role] [nvarchar](max) NULL, [AgencyId] [int] NOT NULL, CONSTRAINT [PK_dbo.Invitations] PRIMARY KEY CLUSTERED ( [Id] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbo].[Invitations] ADD DEFAULT ((0)) FOR [AgencyId] GO After the Invitations table had been created in the stagedaily database by executing the above script, a script for the missing migrations was created in Visual Studio: PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:AgencyId Executing the created script in SSMS against the stagedaily database updated the database and created the missing migrations in the _MigrationHistory table. The same procedure was followed to create the traindaily database: the Invitations table was created via executing the above script and then the missing migrations were added by executing the script created by the update-database command. Requiring this procedure is an inconvenience caused by unfamiliarity with Entity Framework Code First migrations at the time application OPIDDaily was originally created. It is possible that a migration creating table Invitations was accidentally deleted during the original creation. Roslyn On June 6, 2019 I ran into a problem when I first pushed my OPIDDaily solution from my laptop to its GitHub repository and tried to clone the resulting repository onto my desktop computer. When I tried to run the solution from my desktop it complained about missing part of the path /bin/roslyn/csc.exe. I found a fix that worked at StackOverflow https://stackoverflow.com/questions/32780315/could-not-find-a-part-of-the-path-bin-roslyn-csc-exe There were many proposed fixes, but the one that looked easiest to try was: unload project OPIDDaily and then reload it. This was the first fix I tried and it worked! Deployment This section summarizes deployment to AppHarbor. Much of the information here can be found in the section on AppHarbor. There are three applications at AppHarbor: opiddaily , stagedaily and traindaily . Application opiddaily is the deployment of the Visual Studio master branch of solution OPIDDaily. Application stagedaily is the deployment of the Visual Studio staging branch of solution OPIDDaily and application traindaily is the deployment of branch training in Visual Studio. After configuring the opiddaily remote the Visual Studio production branch can be deployed to AppHarbor by using the Git BASH Shell command git push opiddaily master AppHarbor will automatically deploy application OPIDDaily if the push results in a successful build. After AppHarbor finishes building and deploying the code, application opiddaily can be viewed at https://opiddaily.apphb.com After configuring the staging remote (see above) the Visual Studio staging branch can be deployed to AppHarbor by using the Git BASH Shell command git push stagedaily staging AppHarbor will not automatically deploy application stagedaily even if the build is successful. It is necessary to click on the Deploy button at AppHarbor to deploy a successful build of application stagedaily . This may be by design if application stagedaily is recognized as a GitHub branch of application OPIDDaily. After clicking the Deploy button at AppHarbor to deploy a successful build of application stagedaily , the application can be viewed at https://stagedaily.apphb.com After configuring the training remote (see above) the Visual Studio training branch can be deployed to AppHarbor by using the Git BASH Shell command git push traindaily training Again, the Deploy button must be clicked to actually deploy application traindaily and run it through the URL https://traindaily.apphb.com Although there are three applications at AppHarbor, there is only a single repository at GitHub. The name of this single repository is OPIDDaily. The repository is by default focused on the master branch of the codebase but can be switched to the staging branch or training branch by using the GitHub interface. jqGrid Almost every page of the application OPIDDaily features a grid produced by the jQuery jqGrid component. It was installed into the OPIDDaily project by using the Package Manager command: PM> Install-Package Trirand.jqGrid -Version 4.6.0 There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily. ELMAH Unhandled application errors are caught by ELMAH. Version 2.1.2 of Elamh.Mvc was installed in project OPIDDaily by using the Visual Studio NuGet package manager. By default, the ELMAH log can only be viewed on the server that hosts the application in which ELMAH is installed. To make the ELMAH log visible to a client remotely running the application, add <elmah> <security allowRemoteAccess=\"1\" /> </elmah> to the <configuration> section of file Web.config. To see ELMAH in action, modify the URL in the browser address bar to, for example, opiddaily.apphb.com/Admin/Foo This will generate an unhandled error because the MVC routing system will not be able to resolve the URL. Then go to opiddaily.apphb.com/elmah.axd to see that this error has been caught by ELMAH. On the localhost use localhost/opiddaily/elmah.axd to see the list of ELMAH errors. Installation of the Elmah.Mvc package adds the necessary DLL's and makes the necessary changes to Web.config to configure ELMAH for use. By default ELMAH will write to a database table called ELMAH_Error. The DDL Script definition of this table is found in a separate download . Download the DDL Script for MS SQL Server from the referenced web page. The script is a .SQL file which may be executed as a query inside SSMS to create table ELMAH_Error and 3 stored procedures. The ELMAH log is configured by the connection string named OpidDailyConnectionString on Web.config. The <sytem.web> section of Web.config must configure <httpHandlers> <add verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </httpHandlers> and the <system.webServer> section must configure <handlers> <add name=\"Elmah\" verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </handlers> in order for ELMAH to log both on the local IIS and on the remote server at AppHarbor. log4net Application logging is handled by Version 2.0.8 of log4net by the Apache Software Foundation. Logging is used primarily for reporting of detected errors during application execution. But logging is also very useful when trying to understand the execution of the code. Normally Visual Studio breakpoints can be set in the code to interrupt execution and inspect the value of variables and parameters. But as the code grows larger it becomes increasingly difficult to run it in Visual Studio debug mode. It is, however, always easy to insert logging statements to report on the values of variables and parameters without interrupting execution. The log4net package was installed using the Visual Studio NuGet package manager. The application log for project OPIDDaily is maintained as a database table as described in this article describing the AdoNetAppender for log4net. The article includes a script for creating table Log (renamed AppLog in application OPIDDaily). The script must be executed as a query in SSMS to create table AppLog in the database. Table AppLog is created by the following script: CREATE TABLE[dbo].[AppLog] ( [Id][int] IDENTITY(1, 1) NOT NULL, [Date][datetime] NOT NULL, [Thread][varchar](255) NOT NULL, [Level][varchar](50) NOT NULL, [Logger][varchar](255) NOT NULL, [Message][varchar](max) NOT NULL, [Exception][varchar](max) NULL) The application log is configured by the connection string named OpidDailyConnectionString on Web.config. The value of this connection string is overwritten when the application is deployed to AppHarbor. See the Connection String section of the Database tab. log4net requires some additional configuration in Web.config. In the section add: <section name=\"log4net\" type=\"log4net.Config.Log4NetConfigurationSectionHandler, log4net\" /> After the section add the definition of the AdoNetAppender: <log4net debug=\"true\"> <appender name=\"AdoNetAppender\" type=\"log4net.Appender.AdoNetAppender\"> <!--Change to 10 or MORE. This is critical, after 10 messages then log to database--> <bufferSize value=\"1\" /> <connectionType value=\"System.Data.SqlClient.SqlConnection, System.Data, Version=1.0.3300.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\" /> <connectionStringName value=\"OpidDailyConnectionString\" /> <commandText value=\"INSERT INTO AppLog ([Date],[Thread],[Level],[Logger],[Message],[Exception]) VALUES (@log_date, @thread, @log_level, @logger, @message, @exception)\" /> <commandType value=\"Text\" /> <!--<commmandText value=\"dbo.procLog_Insert\"/> <commandType value=\"StoredProcedure\"/>--> <parameter> <parameterName value=\"@log_date\" /> <dbType value=\"DateTime\" /> <layout type=\"log4net.Layout.RawTimeStampLayout\" /> </parameter> <parameter> <parameterName value=\"@thread\" /> <dbType value=\"String\" /> <size value=\"255\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%thread\" /> </layout> </parameter> <parameter> <parameterName value=\"@log_level\" /> <dbType value=\"String\" /> <size value=\"50\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%level\" /> </layout> </parameter> <parameter> <parameterName value=\"@logger\" /> <dbType value=\"String\" /> <size value=\"255\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%logger\" /> </layout> </parameter> <parameter> <parameterName value=\"@message\" /> <dbType value=\"String\" /> <size value=\"4000\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%message\" /> </layout> </parameter> <parameter> <parameterName value=\"@exception\" /> <dbType value=\"String\" /> <size value=\"2000\" /> <layout type=\"log4net.Layout.ExceptionLayout\" /> </parameter> </appender> <root> <level value=\"DEBUG\" /> <!-- <appender-ref ref=\"RollingLogFileAppender\" /> --> <appender-ref ref=\"AdoNetAppender\" /> </root> </log4net> Notice that the configured value of the bufferSize is 1, despite the comment above the configuration to use a value of 10 or more. The value of 1 is chosen so that buffering of log statements will not occur; each log statement will be written to the log file at the time it is generated. This is important because OPIDDaily does not include many log statements. If the buffer size were set to 10 or more, then log statements would not appear in the log file until 10 log statements had accumulated. Setting the bufferSize to 1 is convenient for Log.Debug statements which may be needed to trace the behavior of the website at AppHarbor where the Visual Studio debugger is not available. It is also at times convenient when debugging in the desktop environment where the Visual Studio debugger is available. Finally, log4net must be initialized on file Global.asax.cs by including the configuration statement log4net.Config.XmlConfigurator.Configure(); Without this statement nothing will work even if log4net is correctly configured on Web.config. MkDocs This document was created using MkDocs as was the MkDocs website itself. MkDocs was installed following the guide on this page . This guide is useful for setting up the environment; however, the syntax for the file mkdocs.yml has changed from that described in the guide. The new syntax can be found at in the User Guide section of this document . An MkDocs document is a static website and can be hosted by any service that supports static sites. This MkDocs document is hosted by GitHub Pages . The Atom open source text editor was used to develop the document on the desktop. An MkDocs document uses HTML Markdown for a desktop development version of a document. GitHub provides a cheatsheet for Markdown syntax . MkDocs provides a built-in preview server. To start this server, open a BASH Shell on the folder containing the mkdoc.yml file of the project and execute mkdocs serve Then go to http://127.0.0.1:8000 in a desktop browser. Pages can be edited and saved while in preview mode. The changes will be reflected in the browser document. When it is time to publish a version of a document, in a Git BASH shell opened on the folder containing the mkdocs.yml file, issue the command mkdocs build to expand the Markdown version of the document into an HTML version in the /site folder. Then open the Git GUI on the folder containing the mkdocs.yml file and use the GUI to create a new Git repository on the local disk. Next at GitHub create repository opiddailydoc to hold the documentation. Then create a repository on the desktop machine to associate with the GitHub repository. Issue the following command in the folder containing the mkdocs.yml file: git init After this, in the folder containing the mkdocs.yml file, define a remote called origin for the document: git remote add origin https://github.com/tmhsplb/opiddailydoc.git This command references the GitHub repository opiddailydoc. The remote only needs to be defined once. It will be remembered by the Git BASH shell. In the shell issue the following commands: git add -A git commit -a -m 'Initial commit' git push origin master This will push the master branch of the document to the repository identified by the remote called origin. Then click on the Settings tab for the newly created repository and scroll down to the GitHub Pages section. Select the master branch source and click on the Save button. Finally, to view the published document go to: https://tmhsplb.github.io/opiddailydoc/site Unless a new file is added to file mkdocs.yml , subsequent edits only require the commands mkdocs build git commit -a -m '<Comment for new commit>' git push origin master to update repository opiddailydoc at GitHub. If a new file is added to mkdocs.yml then git add -A must be run before the mkdocs build command is run. This causes any new files to be added to the local git repository. In either case it may take several minutes before edits are available.","title":"Infrastructure"},{"location":"Infrastructure/#infrastructure","text":"The infrastructure of project OPIDDaily refers to the tools and technologies used to develop it, exclusive of the implementation itself. This section will be useful to a developer wanting to maintain and further develop OPIDDaily. It describes both the desktop development environment and the AppHarbor deployment environment for the web application OPIDDaily.","title":"Infrastructure"},{"location":"Infrastructure/#hosting-environments","text":"There are 4 hosting environments for OPIDDaily: desktop, training, staging and production. They differ in the database connection string used by each. The connection string is configured as the value of variable OpidDailyConnectionString in the <connectionStrings> section of Web.config. The static value configured there is used by the desktop environment. The static value is overwritten by injection (at AppHarbor) when OPIDDaily is deployed to create a training, staging or production release. The transformation files Web.Staging.config and Web.Release.config play a role in these deployments. The staging deployment at AppHarbor (called stagedaily) has its Environment variable set to Staging to force Web.Staging.config to be used upon deployment. This is done in the Settings section of the deployed application at AppHarbor. (When Web.Staging.config was created, it was necessary to set its build action to Content in Visual Studio to include it in the build at AppHarbor. The same thing happened when Web.Training.config was created.) The production deployment at AppHarbor has its Environment variable set to Release by default. This causes Web.Release.config to be used upon deployment. The training release was the last release created. It was created by use of the Configuration Manager under the Visual Studio Build menu. In the Active solution configuration dropdown <New> was selected and Training was created with settings copied from the Staging configuration. Then Add Config Transform was selected from the context menu of Web.config. (This item is grayed out until a new configuration has been created.) This automatically added file Web.Training.config. The <appSettings> section of Web.Staging.config was copied to Web.Training.config.","title":"Hosting Environments"},{"location":"Infrastructure/#seo","text":"OPIDDaily is a password protected application that is not intended to be discoverable by search engines. To prevent this it includes the following robots.txt file User-agent:* Disallow:/ This directs all search engines to ignore the entire site.","title":"SEO"},{"location":"Infrastructure/#visual-studio-project","text":"To check out application OPIDDaily from GitHub into Visual Studio 2019 (Community Edition), clone https://tmhsplb@appharbor.com/opiddaily.git into a local folder after being made a collaborator on the project. Being a collaborator is necessary in order to commit changes to the GitHub repository. The Visual Studio project representing application OPIDDaily defines a role-based system. It was developed using the ASP.NET Identity 2.0 framework. A sample ASP.NET Identity 2.0 project was developed by Syed Shanu and described in the excellent CodeProject article ASP.NET MVC Security and Creating User Role . The sample project uses the Visual Studio MVC5 project template and makes use of Katana OWIN middleware for user authentication. The use of Katana is built into the ASP.NET Identity 2.0 provider used by the project template, as is explained in the CodeProject article. The OPIDDaily application was built with information taken from this article as well as the technique for maintaining 2 data contexts described in Scott Allen's Pluralsight video . On the Properties page of the Visual Studio project, remember to select Local IIS as the server and click the Create Virtual Directory button to set http://localhost/OpidDaily as the Project Url. These two actions create an application called OpidDaily under the Default Web Site in IIS and enable project OpidDaily to be run in a desktop version of IIS under this Url. Without this, the desktop IIS cannot be used to host the application. See the section on configuring IIS below. New development in the OPIDDaily Visual Studio project will be done in the staging branch and deployed to the stagedaily application at AppHarbor. (See the section on Deployment.) After changes to the staging branch have been tested in the desktop environment, using the Visual Studio GitHub interface they will be committed and then pushed to the staging branch at GitHub. After changes have been tested, they will be merged into the master branch of the project and from there deployed to application OPIDDaily at Appharbor. When the codebase is installed on a developer's Visual Studio instance on his/her machine by cloning the GitHub repository OPIDDaily , the developer must use Visual Studio to create a staging branch and then rebase this branch onto origin/master . This will cause the remote changes to appear in the local staging branch without the need to Fetch and Pull them as is done between a remote master branch and a local master branch.","title":"Visual Studio Project"},{"location":"Infrastructure/#sql-server-express-and-ssms","text":"The desktop version of OPIDDaily makes use of a SQL Server Express to store information about clients. The database is managed by v18.0 of SQL Server Management Studio (SSMS). Visual Studio includes the ability to view an installed SQL Server Express database, but it is more convenient to have SQL Server Management Studio available for this purpose. SQL Server Express and SSMS require separate (lengthy) downloads. The SQL Server Express database for OPIDDaily was created by executing the SQL query create database OPIDDailyDB executed inside of SSMS. With this database selected in SSMS, there are two SQL queries that need to be executed to enable IIS to talk to SQL Server Express. The first query is CREATE USER [NT AUTHORITY\\NETWORK SERVICE] FOR LOGIN [NT AUTHORITY\\NETWORK SERVICE] WITH DEFAULT_SCHEMA = dbo; This query creates the database user NT AUTHORITY\\NETWORK SERVICE. The second query is EXEC sp_addrolemember 'db_owner', 'NT AUTHORITY\\NETWORK SERVICE' This query grants user NT AUTHORITY\\NETWORK SERVICE the necessary permissions to communicate with IIS. Finally, go into the security section of database OPIDDaily in SSMS. Expand Users and select the properties for user NT AUTHORITY\\NETWORK SERVICE. Select Membership from the Select a page section of the dialog box and tick the checkbox for db_owner. Performing this final change will permit user NT AUTHORITY\\NETWORK SERVICE to create tables in the database. This ability is necessary for the first the OPIDDaily application is run to enable it to automatically create the ASP.NET Identity tables in the database. Creating and configuring user NT AUTHORITY\\NETWORK SERVICE does not need to be performed at AppHarbor in order to communicate with IIS. See below for information about the AppHarbor deployment of OPIDDaily. It is also necessary to change the application pool identity of application OPIDDaily running under IIS to NETWORKSERVICE. See the section on configuring IIS. There is a bug in SSMS v18.0 that causes it to stop after launch; the splash screen will display and then SSMS will quit. The fix for this is to edit file ssms.exe.config found in folder C:\\\\Program Files (x86)\\Microsoft SQL Server Management Studio 18\\Common7\\IDE and remove (or comment out) the line which has the text: <NgenBind_OptimizeNonGac enabled=\"1\" /> This should be around line 38. Then restart SSMS. SSMS v18.0 does not have the capability to generate database diagrams. Previous versions of SSMS had this capability, but it was removed from v18.0. The capability has been added back to newer version of SSMS. SSMS can be used to connect to a remote database at AppHarbor. The credentials for the remote database can be discovered by clicking on the SQL Server add-on at AppHarbor and then following the Go to SQL Server link on the page that appears.","title":"SQL Server Express and SSMS"},{"location":"Infrastructure/#entity-framework-code-first","text":"The Visual Studio project OPIDDaily has 2 data contexts called IdentityDB and OpidDailyDB. The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video . Following the video, supporting 2 data contexts in application OPIDDaily was enabled by some manual scaffolding in the codebase. This scaffolding consisted of creating a project folder called DataContexts with two subfolders: IdentityMigrations and OPIDDailyMigrations. Also, a new folder called Entities was added to the OPIDDaily Visual Studio Solution to contain the classes defining the entities used by the solution. Create the top level folder DataContexts and create class IdentityDB in this folder, as specified in Scott Allen's video. Note that it was found necessary to move the class ApplicationDbContext from file Models\\IdentityModels.cs on the video to file DataContexts/IdentityDb.cs to make things work. The class AppStart/IdentityConfig will need to be modified in order for the application to compile. Replace the line var manager = new ApplicationUserManager(new UserStore<ApplicationUser>(context.Get<ApplicationDbContext>())); by var manager = new ApplicationUserManager(new UserStore<ApplicationUser>(context.Get<IdentityDB>())); The class AppStart/StartupAuth will also need to be modified. Replace the line app.CreatePerOwinContext(ApplicationDbContext.Create); by the line app.CreatePerOwinContext(IdentityDB.Create); This will invoke the method IdentityDB.Create at the proper time. Then run the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.IdentityDB -MigrationsDirectory DataContexts\\IdentityMigrations This will create the file DataContexts\\IdentityMigrations\\Configuration.cs Next in method Configuration on this file set AutomaticMigrationsEnabled = true; to allow the ASP.NET Identity tables to be created when application OPIDDaily is run for the first time. Also add the line ContextKey = \"OPIDDaily.DataContexts.IdentityDB\"; to method Configuration. Before running the program for the first time, make sure the database OPIDDailyDB has been created in SQL Server and, if running in the desktop environment, user NT AUTHORITY\\NETWORK SERVICE has been created and configured. See the section SQL Server Express and SSMS above for instructions on how to do this. The CodeProject article (referenced in section Visual Studio Project) gives the Katana middleware code needed to cause the ASP.NET Identity tables to be created and the first user to be entered into them. This middleware code is found on the toplevel file Startup.cs. As specified on this file the first user is the SuperAdmin user, sa. When application OPIDDaily is run for the first time the ASP.NET Identity tables will be created when the middleware is executed. The user sa will then be found in table AspNetUsesrs . File Startup.cs is worth studying. After application OPIDDaily is run for the first time the database will contain not only the ASP.NET Identity table but also an entity framework table called _MigrationHistory. Entity Framework uses this table to record migrations. As a result of the creation of the ASP.NET Identity tables, a migration with MigrationId 201906051504117_InitialCreate was created in the _MigrationHistory table. Next, to initialize the OPIDDaily data context, run the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.OPIDDailyDB -MigrationsDirectory DataContexts\\OPIDDailyMigrations This will create the file DataContexts\\OPIDDailyMigrations\\Configuration.cs Next create the class DataContexts\\OpidDailyDB as per the video. This class is similar to the previously created DataContexts\\IdentityDB.cs. The first entity added to the OPIDDaily project was the class Entities\\Client. This entity was connected to the OPIDDDaily data context by the inclusion of the declaration public DbSet<Client> Clients { get; set; } on file DataContexts\\OpidDailyDB.cs. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Clients\" caused the migration 201906152111225_Clients.cs to be added to DataContexts\\OPIDDailyMigrations. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Clients to be created in database OPIDDailyDB by executing the Up method of the above migration. Notice that the Up method refers to two database columns, ReferralDate and AppearanceDate, which are not in the currently deployed version of table Clients. The data migration 201907122132153_RemoveTwoDates.cs was used to remove these columns when it was realized they would not be needed. Late in the development of OPIDDaily it was realized that the table Visits was not needed. Table Visits had been created when public DbSet<Visit> Visits { get; set; } was added to DataContexts\\OpidDailyDB.cs and class Entities\\Visit.cs was added to the set of database entities. Table Visits was then created by the Up method of migration 201907120006570_History.cs A new migration was created To delete table Visits from the database when it was realized that it was not needed. First the reference to the table was removed from DataContexts\\OpidDailyDB.cs: // public DbSet<Visit> Visits { get; set; } and all references to entity Visit were removed from the code. After re-compilation of the codebase, the command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"DeleteVisitsTable\" was run. The Up method of the resulting migration showed public override void Up() { DropForeignKey(\"dbo.Visits\", \"Client_Id\", \"dbo.Clients\"); DropIndex(\"dbo.Visits\", new[] { \"Client_Id\" }); DropTable(\"dbo.Visits\"); } This method indicates that the table Visits will be removed from the database together with its foreign key relationship to table Clients . Running the command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then removed table Visits from the database as desired. Notice that this is not the same procedure as running the Down method of migration 201907120006570_History.cs because running this Down method of this migration would invalidate all the migrations performed after it. Late in the development of OPIDDaily the entity Entities\\TextMsg.cs was added. This entity was connected to the OPIDDaily data context by the inclusion of the declaration public DbSet<TextMsg> TextMsgs { get; set; } on file DataContexts\\OpidDailyDB.cs. Since table TextMsgs was intended to be related to table Clients in the OPIDDailyDB by a foreign key, the declaration public ICollection<TextMsg> TextMsgs { get; set; } was added to class Entities\\Client.cs. Entity Framework Code First automatically detected this when the \"Conversations\" migration was created. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Conversations\" caused the migration 202005152029597_Conversations.cs to be added to folder DataContexts\\OPIDDailyMigrations. Studying the Up method of this migration, it is seen that the new table TextMsgs to be created will have a foreign key relationship to table Clients . Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table TextMsgs to be created in database OPIDDailyDB by executing the Up method of the above migration. As desired, table TextMsgs has a foreign key relationship to table Clients . To see this, use SSMS to study the columns of table TextMsgs . Each additional database change requires a pair of commands: an add-migration command followed by an update-database command. Executing an add-migration command creates a .cs file in the folder associated with the ConfigurationTypeName. Study this .cs file before executing the update-database command. If the database changes indicated in the .cs file are not correct, simply delete the .cs file before running the update-database command and then try again. See the section on the Database tab on using PowerShell to create script files to execute at AppHarbor.","title":"Entity Framework Code First"},{"location":"Infrastructure/#configuring-iis","text":"Development of the OPIDDaily application was performed under IIS on the localhost machine. This was done so that the development environment would match thedeployment environment at AppHarbor as closely as possible. The localhost application server, Internet Information Services (IIS), was not pre-installed on the localhost; however, it is part of the operating system that can easily be activated. To activate IIS, go to the Programs section of the Control Panel and turn on the IIS feature: Programs > Programs and Features > Turn Windows features on or off > Internet Information Services After checking this box, expand it by clicking the plus sign (+) next to it and go to the section World Wide Web Services > Application Development Features In this section, check the checkboxes for ASP ASP.NET 3.5 ASP.NET 4.6 if they are not already checked. This will cause additional Application Pools to be made available to IIS. The OPIDDaily application is installed as an application under the Default Web Site in IIS as described in the section describing the Visual Studio Project. The Basic Settings dialog box for application OPIDDaily (accessible from the Actions pane of IIS), will give the physical path to the folder containing the source code as C:\\VS2019Projects\\OpidDaily\\OpidDaily This folder contains the project solution file, OPIDDaily.sln. Do not change it! Application OPIDDaily must be configured to use the application pool .NET v4.5. in the Basic Settings dialog box. (This application pool became available by enabling the features described above.) Finally, change the application identity of the selected application pool (.NET v4.5) to NetworkService. To do this, highlight Application Pools on the IIS Connections panel. This will cause the available application pools to appear in the IIS body panel. Highlight the .NET v4.5 application pool and then select Set Application Pool Defaults\u2026 to display a dialog box that will enable you to change the identity of the application pool. The dialog box is reachable either from the context menu of the highlighted application pool (under right mouse click) or from the Actions panel on the right of the IIS display. The dialog box contains a section labeled Process Model which contains an entry labeled Identity. Selecting the Identity entry adds an ellipsis next to the bold ApplicationPoolIdentity. Selecting the ellipsis brings up a dialog box with the pre-selected radio button Built-in account. Select NetworkService from the dropdown menu associated with this radio button. After approving this selection, the Identity column of the application pool .NET v4.5 will show NetworkService. See the section on SQL Server and SSMS for how to establish user NetworkService.","title":"Configuring IIS"},{"location":"Infrastructure/#git-for-windows","text":"Visual Studio 2019 (Community Edition) comes with built-in support for GitHub. A new project can be added to Git source control on the desktop by simply selecting Add to Source Control from the context menu of the Solution file in the Solution Explorer. Once a project is under Git source control it can be added to a remote GitHub repository by using tools available through Visual Studio. However, a technique preferred by many developers is to use Git for Windows . Git for Windows provides a BASH shell interface to GitHub which uses the same set of commands available at GitHub itself. Git for Windows integrates with Windows Explorer to allow a BASH shell to be opened on a project that has been added to a desktop Git repository. Simply point Windows Explorer at the folder containing the project solution file and select Git BASH Here from the context menu of the folder to open a Git for Windows BASH shell. As a first git command, try entering git --version in the shell. This will report the version of Git that has been installed by the download. After that simply execute Git commands from this shell window. Git for Windows also offers Git GUI, a graphical version of most Git command line functions. To open Git GUI simply select Git GUI Here from Windows Explorer.","title":"Git for Windows"},{"location":"Infrastructure/#github","text":"Application OPIDDaily is stored at GitHub as a repository under an account with the email address peter3418@ymail.com and account name tmhsplb. Only user tmhsplb can deploy directly to this repository. Any other user needing to deploy a version of OPIDDaily to this repository must be declared a collaborator on repository OPIDDaily by user tmhsplb. A collaborator is a user associated with a different account established at GitHub. Git for Windows was used to create a remote to save to this GitHub account. The remote was created in the Git BASH shell by opening the shell on the folder which contains the OPIDDaily.sln file (folder C:/VS2019Projects/OPIDDaily/OPIDDaily ) and issuing the command git remote add origin https://github.com/tmhsplb/opiddaily.git Creating this remote only needs to be done once, because Git for Windows stores the remote. To remove a remote use the command git remote rm myremote The need for this may arise if there was a typo in the creation of myremote.","title":"GitHub"},{"location":"Infrastructure/#appharbor","text":"AppHarbor (appharbor.com) is a Platform as a Service Provider which uses Amazon Web Services infrastructure for hosting applications and Git as a versioning tool. When an application is defined at AppHarbor, a Git repository is created to manage versions of the application's deployment. The OPIDDaily application is defined as an application at AppHarbor to create the production repository of the desktop application. The staging version of the desktop application is defined by a repository called stagedaily. The remote configured for OPIDDaily at AppHarbor is: https://tmhsplb@appharbor.com/opiddaily.git This remote is configured from a Windows Git BASH shell by the command git remote add opiddaily https://tmhsplb@appharbor.com/opiddaily.git After the remote is configured in the Git BASH shell, issuing the command git push opiddaily master will deploy the master branch of solution opiddaily to AppHarbor as application OPIDDaily, accessible through the URL https://opiddaily.apphb.com If you reset your password at AppHarbor, the 'git push' command will no longer work from the Git BASH shell. You need to have Git prompt you for your new password. To do this on a Windows 10 machine, go to Control Panel > User Accounts > Credential Manager > Windows Credentials and remove the AppHarbor entry under Generic Credentials. The next time you push, you will be prompted for your repository password. Application OPIDDaily is deployed using the free Canoe subscription level at AppHarbor. Under a Canoe subscription, the IIS application pool of application OPIDDaily has a 20 minute timeout, which forces OPIIDDaily to spin up its resources again after 20 minutes of idle time. This has not been a problem at Operation ID, because application OPIDDaily is in continuous use on the days Operation ID is open. However, the 20 minute timeout for the free Canoe version at AppHarbor would become a problem if OPIDDaily were extended to add features suitable for use by agencies that partner with Operation ID. These agencies would require that OPIDDaily be available on demand. On demand service would require the use of a paid subscription level at AppHarbor. The free Yocto version of SQL Server is used as an add-on to the OPIDDaily deployment. The Yocto version has a limit of 20MB of storage space, which is adequate for many days of usage by Operation ID. However, the database usage must be monitored to avoid exceeding the 20MB limit. See the Database Utilization section on the Database tab for how to do this. A paid subscription to a SQL Server at AppHarbor would alleviate this problem.","title":"AppHarbor"},{"location":"Infrastructure/#the-staging-and-training-versions-of-opiddaily","text":"A staging version of application OPIDDaily was created from a Visual Studio staging branch by creating an application called stagedaily at AppHarbor. DO NOT CREATE A SEPARATE REPOSITORY FOR STAGEDAILY AT GITHUB. The remote configured for stagedaily at AppHarbor is: https://tmhsplb@appharbor.com/stagedaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/stagedaily.git After the remote is configured in the Git BASH shell, issuing the command git push stagedaily staging will deploy the staging branch of OPIDDaily to AppHarbor as application stagedaily, accessible through the URL https://stagedaily.apphb.com In the same way, a traindaily application at AppHarbor was created from a training branch in Visual Studio. When the stagedaily application was run for the first time, all the migrations up to the migration ending in 18024_AgencyId in OpidDailyMigrations in Visual Studio were automatically applied and added to the _MigrationHistory table. There were several missing migrations in the _MigrationHistory table because the application of migrations stopped due to migration 18024_AgencyId. This migration could not be applied because it references the Invitations table which was not created by any migration which preceded it. This required adding the Invitations table to the stagedaily database through SSMS by executing the following script: SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [dbo].[Invitations]( [Id] [int] IDENTITY(1,1) NOT NULL, [Extended] [datetime] NOT NULL, [Accepted] [datetime] NOT NULL, [UserName] [nvarchar](max) NULL, [FullName] [nvarchar](max) NULL, [Email] [nvarchar](max) NULL, [Role] [nvarchar](max) NULL, [AgencyId] [int] NOT NULL, CONSTRAINT [PK_dbo.Invitations] PRIMARY KEY CLUSTERED ( [Id] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE [dbo].[Invitations] ADD DEFAULT ((0)) FOR [AgencyId] GO After the Invitations table had been created in the stagedaily database by executing the above script, a script for the missing migrations was created in Visual Studio: PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:AgencyId Executing the created script in SSMS against the stagedaily database updated the database and created the missing migrations in the _MigrationHistory table. The same procedure was followed to create the traindaily database: the Invitations table was created via executing the above script and then the missing migrations were added by executing the script created by the update-database command. Requiring this procedure is an inconvenience caused by unfamiliarity with Entity Framework Code First migrations at the time application OPIDDaily was originally created. It is possible that a migration creating table Invitations was accidentally deleted during the original creation.","title":"The Staging and Training Versions of OPIDDaily"},{"location":"Infrastructure/#roslyn","text":"On June 6, 2019 I ran into a problem when I first pushed my OPIDDaily solution from my laptop to its GitHub repository and tried to clone the resulting repository onto my desktop computer. When I tried to run the solution from my desktop it complained about missing part of the path /bin/roslyn/csc.exe. I found a fix that worked at StackOverflow https://stackoverflow.com/questions/32780315/could-not-find-a-part-of-the-path-bin-roslyn-csc-exe There were many proposed fixes, but the one that looked easiest to try was: unload project OPIDDaily and then reload it. This was the first fix I tried and it worked!","title":"Roslyn"},{"location":"Infrastructure/#deployment","text":"This section summarizes deployment to AppHarbor. Much of the information here can be found in the section on AppHarbor. There are three applications at AppHarbor: opiddaily , stagedaily and traindaily . Application opiddaily is the deployment of the Visual Studio master branch of solution OPIDDaily. Application stagedaily is the deployment of the Visual Studio staging branch of solution OPIDDaily and application traindaily is the deployment of branch training in Visual Studio. After configuring the opiddaily remote the Visual Studio production branch can be deployed to AppHarbor by using the Git BASH Shell command git push opiddaily master AppHarbor will automatically deploy application OPIDDaily if the push results in a successful build. After AppHarbor finishes building and deploying the code, application opiddaily can be viewed at https://opiddaily.apphb.com After configuring the staging remote (see above) the Visual Studio staging branch can be deployed to AppHarbor by using the Git BASH Shell command git push stagedaily staging AppHarbor will not automatically deploy application stagedaily even if the build is successful. It is necessary to click on the Deploy button at AppHarbor to deploy a successful build of application stagedaily . This may be by design if application stagedaily is recognized as a GitHub branch of application OPIDDaily. After clicking the Deploy button at AppHarbor to deploy a successful build of application stagedaily , the application can be viewed at https://stagedaily.apphb.com After configuring the training remote (see above) the Visual Studio training branch can be deployed to AppHarbor by using the Git BASH Shell command git push traindaily training Again, the Deploy button must be clicked to actually deploy application traindaily and run it through the URL https://traindaily.apphb.com Although there are three applications at AppHarbor, there is only a single repository at GitHub. The name of this single repository is OPIDDaily. The repository is by default focused on the master branch of the codebase but can be switched to the staging branch or training branch by using the GitHub interface.","title":"Deployment"},{"location":"Infrastructure/#jqgrid","text":"Almost every page of the application OPIDDaily features a grid produced by the jQuery jqGrid component. It was installed into the OPIDDaily project by using the Package Manager command: PM> Install-Package Trirand.jqGrid -Version 4.6.0 There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily.","title":"jqGrid"},{"location":"Infrastructure/#elmah","text":"Unhandled application errors are caught by ELMAH. Version 2.1.2 of Elamh.Mvc was installed in project OPIDDaily by using the Visual Studio NuGet package manager. By default, the ELMAH log can only be viewed on the server that hosts the application in which ELMAH is installed. To make the ELMAH log visible to a client remotely running the application, add <elmah> <security allowRemoteAccess=\"1\" /> </elmah> to the <configuration> section of file Web.config. To see ELMAH in action, modify the URL in the browser address bar to, for example, opiddaily.apphb.com/Admin/Foo This will generate an unhandled error because the MVC routing system will not be able to resolve the URL. Then go to opiddaily.apphb.com/elmah.axd to see that this error has been caught by ELMAH. On the localhost use localhost/opiddaily/elmah.axd to see the list of ELMAH errors. Installation of the Elmah.Mvc package adds the necessary DLL's and makes the necessary changes to Web.config to configure ELMAH for use. By default ELMAH will write to a database table called ELMAH_Error. The DDL Script definition of this table is found in a separate download . Download the DDL Script for MS SQL Server from the referenced web page. The script is a .SQL file which may be executed as a query inside SSMS to create table ELMAH_Error and 3 stored procedures. The ELMAH log is configured by the connection string named OpidDailyConnectionString on Web.config. The <sytem.web> section of Web.config must configure <httpHandlers> <add verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </httpHandlers> and the <system.webServer> section must configure <handlers> <add name=\"Elmah\" verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </handlers> in order for ELMAH to log both on the local IIS and on the remote server at AppHarbor.","title":"ELMAH"},{"location":"Infrastructure/#log4net","text":"Application logging is handled by Version 2.0.8 of log4net by the Apache Software Foundation. Logging is used primarily for reporting of detected errors during application execution. But logging is also very useful when trying to understand the execution of the code. Normally Visual Studio breakpoints can be set in the code to interrupt execution and inspect the value of variables and parameters. But as the code grows larger it becomes increasingly difficult to run it in Visual Studio debug mode. It is, however, always easy to insert logging statements to report on the values of variables and parameters without interrupting execution. The log4net package was installed using the Visual Studio NuGet package manager. The application log for project OPIDDaily is maintained as a database table as described in this article describing the AdoNetAppender for log4net. The article includes a script for creating table Log (renamed AppLog in application OPIDDaily). The script must be executed as a query in SSMS to create table AppLog in the database. Table AppLog is created by the following script: CREATE TABLE[dbo].[AppLog] ( [Id][int] IDENTITY(1, 1) NOT NULL, [Date][datetime] NOT NULL, [Thread][varchar](255) NOT NULL, [Level][varchar](50) NOT NULL, [Logger][varchar](255) NOT NULL, [Message][varchar](max) NOT NULL, [Exception][varchar](max) NULL) The application log is configured by the connection string named OpidDailyConnectionString on Web.config. The value of this connection string is overwritten when the application is deployed to AppHarbor. See the Connection String section of the Database tab. log4net requires some additional configuration in Web.config. In the section add: <section name=\"log4net\" type=\"log4net.Config.Log4NetConfigurationSectionHandler, log4net\" /> After the section add the definition of the AdoNetAppender: <log4net debug=\"true\"> <appender name=\"AdoNetAppender\" type=\"log4net.Appender.AdoNetAppender\"> <!--Change to 10 or MORE. This is critical, after 10 messages then log to database--> <bufferSize value=\"1\" /> <connectionType value=\"System.Data.SqlClient.SqlConnection, System.Data, Version=1.0.3300.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\" /> <connectionStringName value=\"OpidDailyConnectionString\" /> <commandText value=\"INSERT INTO AppLog ([Date],[Thread],[Level],[Logger],[Message],[Exception]) VALUES (@log_date, @thread, @log_level, @logger, @message, @exception)\" /> <commandType value=\"Text\" /> <!--<commmandText value=\"dbo.procLog_Insert\"/> <commandType value=\"StoredProcedure\"/>--> <parameter> <parameterName value=\"@log_date\" /> <dbType value=\"DateTime\" /> <layout type=\"log4net.Layout.RawTimeStampLayout\" /> </parameter> <parameter> <parameterName value=\"@thread\" /> <dbType value=\"String\" /> <size value=\"255\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%thread\" /> </layout> </parameter> <parameter> <parameterName value=\"@log_level\" /> <dbType value=\"String\" /> <size value=\"50\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%level\" /> </layout> </parameter> <parameter> <parameterName value=\"@logger\" /> <dbType value=\"String\" /> <size value=\"255\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%logger\" /> </layout> </parameter> <parameter> <parameterName value=\"@message\" /> <dbType value=\"String\" /> <size value=\"4000\" /> <layout type=\"log4net.Layout.PatternLayout\"> <conversionPattern value=\"%message\" /> </layout> </parameter> <parameter> <parameterName value=\"@exception\" /> <dbType value=\"String\" /> <size value=\"2000\" /> <layout type=\"log4net.Layout.ExceptionLayout\" /> </parameter> </appender> <root> <level value=\"DEBUG\" /> <!-- <appender-ref ref=\"RollingLogFileAppender\" /> --> <appender-ref ref=\"AdoNetAppender\" /> </root> </log4net> Notice that the configured value of the bufferSize is 1, despite the comment above the configuration to use a value of 10 or more. The value of 1 is chosen so that buffering of log statements will not occur; each log statement will be written to the log file at the time it is generated. This is important because OPIDDaily does not include many log statements. If the buffer size were set to 10 or more, then log statements would not appear in the log file until 10 log statements had accumulated. Setting the bufferSize to 1 is convenient for Log.Debug statements which may be needed to trace the behavior of the website at AppHarbor where the Visual Studio debugger is not available. It is also at times convenient when debugging in the desktop environment where the Visual Studio debugger is available. Finally, log4net must be initialized on file Global.asax.cs by including the configuration statement log4net.Config.XmlConfigurator.Configure(); Without this statement nothing will work even if log4net is correctly configured on Web.config.","title":"log4net"},{"location":"Infrastructure/#mkdocs","text":"This document was created using MkDocs as was the MkDocs website itself. MkDocs was installed following the guide on this page . This guide is useful for setting up the environment; however, the syntax for the file mkdocs.yml has changed from that described in the guide. The new syntax can be found at in the User Guide section of this document . An MkDocs document is a static website and can be hosted by any service that supports static sites. This MkDocs document is hosted by GitHub Pages . The Atom open source text editor was used to develop the document on the desktop. An MkDocs document uses HTML Markdown for a desktop development version of a document. GitHub provides a cheatsheet for Markdown syntax . MkDocs provides a built-in preview server. To start this server, open a BASH Shell on the folder containing the mkdoc.yml file of the project and execute mkdocs serve Then go to http://127.0.0.1:8000 in a desktop browser. Pages can be edited and saved while in preview mode. The changes will be reflected in the browser document. When it is time to publish a version of a document, in a Git BASH shell opened on the folder containing the mkdocs.yml file, issue the command mkdocs build to expand the Markdown version of the document into an HTML version in the /site folder. Then open the Git GUI on the folder containing the mkdocs.yml file and use the GUI to create a new Git repository on the local disk. Next at GitHub create repository opiddailydoc to hold the documentation. Then create a repository on the desktop machine to associate with the GitHub repository. Issue the following command in the folder containing the mkdocs.yml file: git init After this, in the folder containing the mkdocs.yml file, define a remote called origin for the document: git remote add origin https://github.com/tmhsplb/opiddailydoc.git This command references the GitHub repository opiddailydoc. The remote only needs to be defined once. It will be remembered by the Git BASH shell. In the shell issue the following commands: git add -A git commit -a -m 'Initial commit' git push origin master This will push the master branch of the document to the repository identified by the remote called origin. Then click on the Settings tab for the newly created repository and scroll down to the GitHub Pages section. Select the master branch source and click on the Save button. Finally, to view the published document go to: https://tmhsplb.github.io/opiddailydoc/site Unless a new file is added to file mkdocs.yml , subsequent edits only require the commands mkdocs build git commit -a -m '<Comment for new commit>' git push origin master to update repository opiddailydoc at GitHub. If a new file is added to mkdocs.yml then git add -A must be run before the mkdocs build command is run. This causes any new files to be added to the local git repository. In either case it may take several minutes before edits are available.","title":"MkDocs"}]}