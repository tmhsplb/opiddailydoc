{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Background Operation ID is a ministry that exists to serve the need of many individuals to obtain identification documents to support their application for a variety of services from housing to job application. The ministry is volunteer driven and has been in place for over thirty years. During this time thousands of individuals have received help to support their identification needs. Help is provided in the form of vouchers that can be used to pay for services rendererd by either the Texas Department of Public Safety for a Texase ID or by the Bureau of Vital Statistics of Texas or another state to acquire a certified copy of a birth certificate. Over the years of Operation ID's existence various paper driven pipelines of client processing in providing identification services have been used. The currently existing pipeline consists of four stages which are executed sequentially. Screening Check In Interviewing Back Office Voucher Writing In the Screening stage, a client is greeted at the front desk by a volunteer who checks that the client's referral letter is valid and that the documents the client possesses support the services they are seeking. The referral letter is provided by the agency representing the client to Operation ID and this agency performs the first level of screening. If this first level of scrreening is done carefully, the Screening stage at Operation ID can proceed very quickly, allowing the client to proceed to the Check In stage. In the Check In stage (also performed by a volunteer at the front desk) the client is entered into the Operation ID database maintained by the Apricot database driven application software provided by the vendor Social Solutions . Apricot is used to capture a client's demographic data together with a history of a client's previous visits to Operation ID. The demographic data includes a client's name, date of birth and age (a calculated field based on date of birth). The demographic data also includes a link to a household table, if a client is part of a household being processed together. Apricot stores basic demographic data in a table it refers to as the Client Tracking table and stores client visits in a related table called the Client Tracking Document Folder. (In terms of database technology, the Client Tracking Document Folder is related to the Client Tracking table by a foreign key.) Operation ID makes use of the Client Tracking Document Folder to record services provided to a client during previous visits. Whenever a service is provided to a client (example: writing a check for a birth certificate), the service is recorded by date together with the number of any issued check. At a date after the date of service, the disposition of an issued check is recorded. Most often this disposition will be either Cleared, indicating that the check has been used by the client or Voided, indicating that the check is no longer valid. During the Check In stage at the front desk, a client's history of previous visits (if any) is manually copied onto a small slip of paper which is then stapled to the client's referral letter. Previous visits may make a client ineligible for a service being requested. Operation ID enforces a twice-in-a-lifetime policy for birth certificate or ID services. If a client's history of previous visits reveals that they have twice previously been issued checks for a birth certificate and both checks have been used, then the client is not eligible for another check for a birth certificate. Similarly, if a client has twice previously been issued checks for an ID (either a Texas ID or a Texas Driver's License) and both checks have been used, then they are not eligible for another check for an ID. If a client is ineligible for a requested service, then they are so informed at the front desk. A client ineligible for a requested service may appeal their ineligibilty and may, at the discretion of Operation ID management, be granted an exception. Most clients are either first time clients (no previous visit history) or clients who have not yet exhausted eligibility for a requested service. In either case, such a client is sent to the Interviewing stage. A client ready to be interviewed may have to wait for a (volunteer) interviewer to become available. Clients are processed on a first-come-first-served basis and frequently have to wait for an available interviewer. When an interviewer picks up a waiting client, the client's requested services are reviewed and paperwork in support of these services is completed. If the client is representing themselve alone, the paperwork generation may go fairly quickly. If the client is requesting an out-of-state birth certificate, more time is required, because the paperwork required to acquire a certified birth certificate vaires from state to state. If the client represents a household, then paperwork for each member of the household is required and this may slow down the interviewing process considerably. In any event, under the current client processing pipeline, all paperwork required is completed during the interviewing process before the back office can generate any voucher for a client. When paperwork filled out during the Interviewing stage is sent to the back office, the Back Office Voucher Writing stage may begin. This stage may not begin immediately upon the delivery of the completed paperwork for a given client, because a backlog of previous clients may need to be processed first. When a client's paperwork is eventually processed by the back office, what remains to be done is straightforward. A voucher for each service requested by a client is first generated by use of Quickbooks (operated by a volunteer). Then the voucher number of each generated voucher is recorded in the Apricot database (by another volunteer) in the client's visit history. This record keeping step is what enables enforcement of the twice-in-a-lifetime service policy mentioned above. After vouchers have been generated and recorded, they are deliverd to the client and the pipeline of client processing is completed. OPID Daily The OPID Daily application was designed to partially automate the pipeline of client processing at Operation ID by allowing the back office production of vouchers to proceed in parallel with the interviewing of clients to generate paperwork supporting their needs. OPID Daily takes advantage of the fact that a client's voucher needs are already known during the Screening and Check In stages. This allows the front desk volunteers to send the back office volunteers information which allows the Back Office Voucher Writing stage to begin, possibly even before the Interviewing Stage has started. Using OPID Daily, a Service Ticket describing a client's previous visits (if any) together with the services the client is seeking is printed on a printer in the back office. The back office volunteers can begin processing the Requested Services document in advance of the Interviewing stage being completed. In effect, the Interviewing stage and the Back Office Voucher Writing stage can be run in parallel. This has the potential of delivering substantial time savings over the currently existing sequential pipeline. The Screening and Check In stages will require a small amount of additional time in order to generate the Requested Services document for a client, but this extra time will be more than compensated for by the ability to run the Interviewing and Back Office Voucher Writing stages in parallel. OPID Daily will be available as a password protected website to registered users. This document describes the design and implementation of OPID Daily. The Infrastructure tab describes how project OPID Daily is maintained on a desktop host and how it is deployed to the .NET hosting service AppHarbor. The Database tab describes how the OPID Daily database is managed on the desktop and at AppHarbor. The Implementation tab provides some details concerning the implementation of OPID Daily. Users OPID Daily is a role-based system. Each registered user will be assignd a user role by the OPID Daily administrator. The role that a user is assigned will determine the OPID Daily features available to the user. A user's assigned role will depend upon whether the user volunteers at the front desk, is a volunteer interviewer or is a back office volunteer. In addition, there will be a manager's role to be assigned to Operation ID managers who want to watch the client processing flow during a day of operation. The OPID Daily administrator will be in the role of Superadmin and will have access to features necessary for the maintenance of application OPID Daily. There will be only one Superadmin user, but the credentials for this account will be shared among maintainers of the application.","title":"Background"},{"location":"#background","text":"Operation ID is a ministry that exists to serve the need of many individuals to obtain identification documents to support their application for a variety of services from housing to job application. The ministry is volunteer driven and has been in place for over thirty years. During this time thousands of individuals have received help to support their identification needs. Help is provided in the form of vouchers that can be used to pay for services rendererd by either the Texas Department of Public Safety for a Texase ID or by the Bureau of Vital Statistics of Texas or another state to acquire a certified copy of a birth certificate. Over the years of Operation ID's existence various paper driven pipelines of client processing in providing identification services have been used. The currently existing pipeline consists of four stages which are executed sequentially. Screening Check In Interviewing Back Office Voucher Writing In the Screening stage, a client is greeted at the front desk by a volunteer who checks that the client's referral letter is valid and that the documents the client possesses support the services they are seeking. The referral letter is provided by the agency representing the client to Operation ID and this agency performs the first level of screening. If this first level of scrreening is done carefully, the Screening stage at Operation ID can proceed very quickly, allowing the client to proceed to the Check In stage. In the Check In stage (also performed by a volunteer at the front desk) the client is entered into the Operation ID database maintained by the Apricot database driven application software provided by the vendor Social Solutions . Apricot is used to capture a client's demographic data together with a history of a client's previous visits to Operation ID. The demographic data includes a client's name, date of birth and age (a calculated field based on date of birth). The demographic data also includes a link to a household table, if a client is part of a household being processed together. Apricot stores basic demographic data in a table it refers to as the Client Tracking table and stores client visits in a related table called the Client Tracking Document Folder. (In terms of database technology, the Client Tracking Document Folder is related to the Client Tracking table by a foreign key.) Operation ID makes use of the Client Tracking Document Folder to record services provided to a client during previous visits. Whenever a service is provided to a client (example: writing a check for a birth certificate), the service is recorded by date together with the number of any issued check. At a date after the date of service, the disposition of an issued check is recorded. Most often this disposition will be either Cleared, indicating that the check has been used by the client or Voided, indicating that the check is no longer valid. During the Check In stage at the front desk, a client's history of previous visits (if any) is manually copied onto a small slip of paper which is then stapled to the client's referral letter. Previous visits may make a client ineligible for a service being requested. Operation ID enforces a twice-in-a-lifetime policy for birth certificate or ID services. If a client's history of previous visits reveals that they have twice previously been issued checks for a birth certificate and both checks have been used, then the client is not eligible for another check for a birth certificate. Similarly, if a client has twice previously been issued checks for an ID (either a Texas ID or a Texas Driver's License) and both checks have been used, then they are not eligible for another check for an ID. If a client is ineligible for a requested service, then they are so informed at the front desk. A client ineligible for a requested service may appeal their ineligibilty and may, at the discretion of Operation ID management, be granted an exception. Most clients are either first time clients (no previous visit history) or clients who have not yet exhausted eligibility for a requested service. In either case, such a client is sent to the Interviewing stage. A client ready to be interviewed may have to wait for a (volunteer) interviewer to become available. Clients are processed on a first-come-first-served basis and frequently have to wait for an available interviewer. When an interviewer picks up a waiting client, the client's requested services are reviewed and paperwork in support of these services is completed. If the client is representing themselve alone, the paperwork generation may go fairly quickly. If the client is requesting an out-of-state birth certificate, more time is required, because the paperwork required to acquire a certified birth certificate vaires from state to state. If the client represents a household, then paperwork for each member of the household is required and this may slow down the interviewing process considerably. In any event, under the current client processing pipeline, all paperwork required is completed during the interviewing process before the back office can generate any voucher for a client. When paperwork filled out during the Interviewing stage is sent to the back office, the Back Office Voucher Writing stage may begin. This stage may not begin immediately upon the delivery of the completed paperwork for a given client, because a backlog of previous clients may need to be processed first. When a client's paperwork is eventually processed by the back office, what remains to be done is straightforward. A voucher for each service requested by a client is first generated by use of Quickbooks (operated by a volunteer). Then the voucher number of each generated voucher is recorded in the Apricot database (by another volunteer) in the client's visit history. This record keeping step is what enables enforcement of the twice-in-a-lifetime service policy mentioned above. After vouchers have been generated and recorded, they are deliverd to the client and the pipeline of client processing is completed.","title":"Background"},{"location":"#opid-daily","text":"The OPID Daily application was designed to partially automate the pipeline of client processing at Operation ID by allowing the back office production of vouchers to proceed in parallel with the interviewing of clients to generate paperwork supporting their needs. OPID Daily takes advantage of the fact that a client's voucher needs are already known during the Screening and Check In stages. This allows the front desk volunteers to send the back office volunteers information which allows the Back Office Voucher Writing stage to begin, possibly even before the Interviewing Stage has started. Using OPID Daily, a Service Ticket describing a client's previous visits (if any) together with the services the client is seeking is printed on a printer in the back office. The back office volunteers can begin processing the Requested Services document in advance of the Interviewing stage being completed. In effect, the Interviewing stage and the Back Office Voucher Writing stage can be run in parallel. This has the potential of delivering substantial time savings over the currently existing sequential pipeline. The Screening and Check In stages will require a small amount of additional time in order to generate the Requested Services document for a client, but this extra time will be more than compensated for by the ability to run the Interviewing and Back Office Voucher Writing stages in parallel. OPID Daily will be available as a password protected website to registered users. This document describes the design and implementation of OPID Daily. The Infrastructure tab describes how project OPID Daily is maintained on a desktop host and how it is deployed to the .NET hosting service AppHarbor. The Database tab describes how the OPID Daily database is managed on the desktop and at AppHarbor. The Implementation tab provides some details concerning the implementation of OPID Daily.","title":"OPID Daily"},{"location":"#users","text":"OPID Daily is a role-based system. Each registered user will be assignd a user role by the OPID Daily administrator. The role that a user is assigned will determine the OPID Daily features available to the user. A user's assigned role will depend upon whether the user volunteers at the front desk, is a volunteer interviewer or is a back office volunteer. In addition, there will be a manager's role to be assigned to Operation ID managers who want to watch the client processing flow during a day of operation. The OPID Daily administrator will be in the role of Superadmin and will have access to features necessary for the maintenance of application OPID Daily. There will be only one Superadmin user, but the credentials for this account will be shared among maintainers of the application.","title":"Users"},{"location":"Infrastructure/","text":"Infrastructure The infrastructure of project OPIDDaily refers to the tools and technologies used to develop OPIDDaily, exclusive of the implementation itself. This section will be useful to a developer wanting to maintain and further develop OPID Daily. It describes both the desktop development environment and the AppHarbor deployment environment for application OPIDDaily. Hosting Environments There are 3 hosting environments for OPIDDaily: desktop, staging and production. They differ in the database connection string used by each. The connection string is configured as the value of variable SQLSERVER_CONNECTION_STRING in the <appSettings> section of Web.config. The static value configured there is used by the desktop environment. The static value is overwritten by injection (at AppHarbor) when OPIDDaily is deployed to create either a staging or production release. The transformation files Web.Staging.config and Web.Release.config play a role in these deployments. The staging deployment at AppHarbor has its Environment variable set to Staging to force Web.Staging.config to be used upon deployment. This is done in the Settings section of the deployed application. The production deployment at AppHarbor has its Environment variable set to Release by default. This causes Web.Release.config to be used upon deployment. Visual Studio Project The Visual Studio 2019 (Community Edition) project representing application OPIDDaily was developed using an ASP.NET Identity 2.0 sample project developed by Syed Shanu as a starting point. The project is described in the excellent CodeProject article ASP.NET MVC Security and Creating User Role . The sample project uses the Visual Studio MVC5 project template and makes use of Katana OWIN middleware for user authentication. The use of Katana is built into the ASP.NET Identity 2.0 provider used by the project template, as is explained in the CodeProject article. On the Properties page of the Visual Studio project remember to select Local IIS as the server and click the Create Virtual Directory button to set http://localhost/OpidDaily as the Project Url. These two actions create an application called OpidDaily under the Default Web Site in IIS and enable project OpidDaily to be run in a desktop version of IIS under this Url. Without this, the desktop IIS cannot be used to host the application. See the section on configuring IIS below. Visual Studio includes the ability to view an installed SQL Server Express database, but it is more convenient to have SQL Server Management Studio (SSMS) available for this purpose. This requires a (lengthy) download. The version of SSMS used for the development of the OPIDDaily application is v18.0. Note that SQL Server Express is a separate download from SSMS; it does not come bundled with SSMS. New development in the OPIDDaily Visual Studio project will be done in the staging branch and deployed to the stagedaily application at AppHarbor. (See the section on Deployment.) After changes to the staging branch have been tested in the desktop environment, using the Visual Studio GutHub interface they will be commited and then pushed to the staging branch at GitHub. After changes have been tested, they will be merged into the master branch of the project and from there deployed to application OPIDDaily at Appharbor. When the codebase is installed on a developer's Visual Studio instance on his/her machine by cloning the GitHub repository OPIDDaily , the developer must use Visual Sutdio to create a staging branch and then rebase this branch to origin/staging . This will allow the developer to see the contents of the staging branch at GitHub. To pick up changes commited and pushed to GitHub in a remote staging branch, it is necessary to rebase the local staging branch to origin/staging . This will cause the remote changes to appear in the local staging branch without the need to Fetch and Pull them as is done between a remote master branch and a local master branch. SQL Server Express The desktop version of OPID Daily makes use of a SQL Server Express to store information about clients. The SQL Server Express database for OPIDDaily was created by executing the SQL query create database OPIDDailyDB executed inside of SSMS. With this database selected in SSMS, there are two SQL queries that need to be executed to enable IIS to talk to SQL Server Express. The first query is CREATE USER [NT AUTHORITY\\NETWORK SERVICE] FOR LOGIN [NT AUTHORITY\\NETWORK SERVICE] WITH DEFAULT_SCHEMA = dbo; This query creates the database user NT AUTHORITY\\NETWORK SERVICE. The second query is EXEC sp_addrolemember 'db_owner', 'NT AUTHORITY\\NETWORK SERVICE' This query grants user NT AUTHORITY\\NETWORK SERVICE the necessary permissions to communicate with IIS. These same two queries do not need to be executed in the AppHarbor database to prepare it to communicate with IIS. See below for information about the AppHarbor deployment of OPIDDaily. It is also necessary to change the application pool identity of application OPIDDaily running under IIS to NETWORKSERVICE. See the section on configuring IIS. Entity Framework Code First An application based on Entity Framework Code First may have multiple data contexts referencing a single database, as is the case for application OPIDDaily. In application OPIDDaily a data context is reserved for database migrations used by the ASP.NET Identity subsystem. Supporting multiple data contexts was enabled by some manual scaffolding in the codebase. In the case of the OPIDDaily application this scaffolding consisted of creating a project folder called DataContexts with two subfolders: IdentityMigrations and OPIDDailyMigrations. Also, a new folder called Entities was added to the OPIDDaily Visual Studio Solution to contain the classes defining the entities used by the solution. Before the code was run for the first time, the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.IdentityDb -MigrationsDirectory DataContexts\\IdentityMigrations was executed. This created the two files DataContexts\\IdentityMigrations\\Configuration.cs DataContexts\\IdentityMigrations\\IdentityDB.cs which initiaized the IdentityDB data context. It is worth taking a look at these two files. In particular, the file IdentityDB.cs was edited to point to the application connection string through Config.ConnectionString. Executing the above PowerShell command allows the ASP.NET Identity system to automatically update the OPIDDailyDB with the ASP.NET Identity tables the first time the program is run. Running the program for the first time on the local IIS also automatically created the migration DataContexts\\IdentityMigrations\\201906051504117_InitialCreate.cs which specifes the code used to create the ASP.NET Identity tables. It is worth taking a look at this file. The first time the program was run, the Superadmin user, sa, was created in database OPIDDailyDB. This was done by method Startup.Configuration on the toplevel file OPIDDaily\\Startup.cs. This file specifies the user sa as the first user in role SuperAdmin (created on the file). It also points to the toplevel file Config.cs through the reference Config.SuperadminPassword, where the password of user sa is configured. This project used as its starting point the excellent CodeProject article ASP.NET MVC Security and Creating User Role It was necessary to move the class ApplicationDbContext from file Models\\IdentityModels.cs on the sample project to file DataContexts/IdentityDb.cs to make things work. The PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.OPIDDailyDB -MigrationsDirectory DataContexts\\OPIDDailyMigrations was executed to initialize the OPIDDaily data context. This created the two files DataContexts\\OPIDDailyMigrations\\Configuration.cs DataContexts\\OPIDDailyMigrations\\OPIDDailyDB.cs It is worth studying these two files. The first entity added to the OPIDDaily project was the class Entities\\Client.cs. This entity was connected to the OPIDDDaily data context by the inclusion of the declaration public DbSet<Client> Clients { get; set; } on file DataContexts\\OpidDailyDB.cs. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Clients\" caused the migration 201906152111225_Clients.cs to be added to DataContexts\\OPIDDailyMigrations. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Clients to be created in database OPIDDailyDB by executing the Up method of the above migration. Notice that the Up method refers to two database columns, ReferralDate and AppearanceDate, which are not in the currently deployed version of table Clients. The data migration 201907122132153_RemoveToDates.cs was used to remove these columns when it was realized they would not be needed. The second entity to be added to project OPIDDaily was the class Entities\\Visit.cs. This entity was connected to the OPIDDaily data context by the inclusion of the declaration public DbSet<Visit> Visits { get; set; } on file DataContexts\\OpidDailyDB.cs. Since table Visits was intended to be related to table Clients in the OPIDDailyDB by a foreign key, the declaration public ICollection<Visit> Visits { get; set; } was added to class Entities\\Client.cs. Entity Framework Code First automatically detected this when the \"History\" migration (described next) was created. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"History\" then caused the migration 2019071220006570_History.cs to be added to DataContexts\\OPIDDailyMigrations. Studying the Up method of this migration, it is seen that the new table Visits to be created will have a foreign key relationship to table Clients. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Visits to be created in database OPIDDailyDB by executing the Up method of the above migration. As desired, table Visits has a foreign key relationship to table Clients. To see this, use SSMS to study the columns of table Visits. Although the desired foreign key relationship exists between tables Cients and Visits, deleting a client from the Clients table does not by default perform a cascading deleted of any related records in the Visits table. The cascading delete must be performed manualy. To see how this is done, see method RemoveClients in the SuperadminController. Each additional database change requires a pair of commands: an add-migration command followed by an update-database command. Executing an add-migration command creates a .cs file in the folder associated with the ConfigurationTypeName. Study this .cs file before executing the update-database command. If the database changes indicated in the .cs file are not correct, simply delete the .cs file before running the update-database command and then try again. To generate a script for the most recent migration, go back one migration in the migration history. For example, the migration preceding the migration ExpressClient was the migration PXXA. Therefore, to get a script for migration ExpressClient, execute the command update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:PXXA The generated script can be run against the database at AppHarbor using SSMS. The script should be run before the code is updated at AppHarbor. Configuring IIS Development of the OPIDDaily application was performed under IIS on the localhost machine. This was done so that the development environment would match the deployment environment at AppHarbor as closely as possible. The localhost application server, Internet Information Services (IIS), was not pre-installed on the localhost; however, it is part of the operating system that can easily be activated. To activate IIS, go to the Programs section of the Control Panel and turn on the IIS feature: Programs > Programs and Features > Turn Windows features on or off > Internet Information Services After checking this box, expand it by clicking the plus sign (+) next to it and go to the section World Wide Web Services > Application Development Features In this section, check the checkboxes for ASP ASP.NET 3.5 ASP.NET 4.6 if they are not already checked. This will cause additional Application Pools to be made available to IIS. The OPIDDaily application is installed as an application under the Default Web Site in IIS as described in the section describing the Visual Studio Project. The Basic Settings dialog box for application Eref (accessible from the Actions pane of IIS), will give the physical path to the folder containing the source code as C:\\Projects\\OpidDaily\\OpidDaily This folder contains the project solution file, OPIDaily.sln. Do not change it! Application OPIDaily must be configured to use the application pool .NET v4.5. in the Basic Settings dialog box. (This application pool became available by enabling the features described above.) Finally, change the application identity of the selected application pool (.NET v4.5) to NetworkService. To do this, highlight Application Pools on the IIS Connections panel. This will cause the available application pools to appear in the IIS body panel. Highlight the .NET v4.5 application pool and then select Set Application Pool Defaults\u2026 to display a dialog box that will enable you to change the identity of the application pool. The dialog box is reachable either from the context menu of the highlighted application pool (under right mouse click) or from the Actions panel on the right of the IIS display. The dialog box contains a section labeled Process Model which contains an entry labeled Identity. Selecting the Identity entry adds an ellipsis next to the bold ApplicationPoolIdentity. Selecting the ellipsis brings up a dialog box with the pre-selected radio button Built-in account. Select NetworkService from the dropdown menu associated with this radio button. After approving this selection, the Identity column of the application pool .NET v4.5 will show NetworkService. See the section on SQL Server Express for how to establish user NetworkService. Git for Windows Visual Studio 2019 (Community Edition) comes with built-in support for GitHub. A new project can be added to Git source control on the desktop by simply selecting Add to Source Control from the context menu of the Solution file in the Solution Explorer. Once a project is under Git source control it can be added to a remote GitHub repository by using tools available through Visual Studio. However, a technique preferred by many developers is to use Git for Windows . Git for Windows provides a BASH shell interface to GitHub which uses the same set of commands available at GitHub itself. Git for Windows integrates with Windows Explorer to allow a BASH shell to be opened on a project that has been added to a desktop Git repository. Simply point Windows Explorer at the folder containing the project solution file and select Git BASH Here from the context menu of the folder to open a Git for Windows BASH shell. Then execute Git commands from this shell window. Git for Windows also offers Git GUI, a graphical version of most Git command line functions. To open Git GUI simply select Git GUI Here from Windows Explorer. GitHub Application OPIDDaily is stored at GitHub as a repository under an account with the email address peter3418@ymail.com and account name tmhsplb. Only user tmhsplb can deploy directly to this repository. Any other user needing to deploy a version of OPIDDaily to this repository must be declared a collaborator on repository OPIDDaily by user tmhsplb. A collaborator is a user associated with a different account established at GitHub. Git for Windows was used to create a remote to save to this GitHub account. The remote was created in the Git BASH shell by opening the shell on the folder which contains the OPIDDaily.sln file (folder C:/Projects/OPIDDaily ) and issuing the command git remote add origin https://github.com/tmhsplb/opiddaily.git Creating this remote only needs to be done once, because Git for Windows stores the remote. To remove a remote use the command git remote rm <remote> The need for this may arise if there was a typo in the creation of . AppHarbor AppHarbor (appharbor.com) is a Platform as a Service Provider which uses Amazon Web Services infrastructure for hosting applications and Git as a versioning tool. When an application is defined at AppHarbor, a Git repository is created to manage versions of the application's deployment. The OPIDaily application is defined as an application at AppHarbor to create the production repository of the desktop application. The staging version of the desktop application is defined by a repository called stagedaily. The remote configured for OPIDDaily at AppHarbor is: https://tmhsplbt@appharbor.com/opiddaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/opiddaily.git After the remote is configured in the Git BASH shell, issuing the command git push opiddaily master will deploy the master branch of opiddaily to AppHarbor as application OPIDDaily, accessible through the URL https://opiddaily.apphb.com If you reset your password at AppHarbor, the 'git push' command will no longer work from the Git BASH shell. You need to have git prompt you for your new password. To do this on a Windows 10 machine, go to Control Panel > User Accounts > Credential Manager > Windows Credentials and remove the AppHarbor entry under Generic Credentials. The next time you push, you will be prompted for your repository password. An application such as OPIDDaily deployed using the free Canoe subscription level at AppHarbor has limitations that make it unsuitable for production use. Under the Canoe subscription, the IIS application pool of application OPIDDaily has a 20 minute timeout, which forces OPIIDDaily to spin up its resources again after 20 minutes of idle time. The free Yocto version of SQL Server is used as an add-on to the OPIDDaily deployment. The Yocto version is free but has a limit of 20MB of storage space, which is adequate for development purposes. A staging version of application OPIDDaily was created by creating an application called stagedaily at AppHarbor. DO NOT CREATE A SEPARATE REPOSITORY FOR STAGEDAILY AT GITHUB. The remote configured for stagedaily at AppHarbor is: https://tmhsplbt@appharbor.com/stagedaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/stagedaily.git After the remote is configured in the Git BASH shell, issuing the command git push stagedaily staging will deploy the staging branch of OPIDDaily to AppHarbor as application stagedaily, accessible through the URL https://stagedaily.apphb.com All the tables created by Entity Framework migrations magically appeared in the staging version. The magic was probably caused by deployment of the codebase of the staging branch to AppHarbor. This branch contains all the migrations used by the master branch. However, there was one table missing:the Invitations table. This table is not included in any migration, so it has to be added manually to the staging database. This is a simple matter of using SSMS to script the table and executing the script (in SSMS) against the staging database. The scripts that needed to run on the desktop to establish the connection between Visual Studio and the desktop SQL Server did not need to be run against the staging database to establish communication with the AppHarbor server. It is possible to use AppHarbor to generate a custom domain name for an application, but this has not been done for the OPIDDaily application. On June 6, 2019 I ran into a problem when I first pushed my OPIDDaily solution from my laptop to its GitHub repository and tried to pull the resulting solution onto my desktop computer. When I tried to run the solution from my desktop it complained about missing part of the path /bin/roslyn/csc.exe. I found a fix that worked at StackOverflow https://stackoverflow.com/questions/32780315/could-not-find-a-part-of-the-path-bin-roslyn-csc-exe There were many proposed fixes, but the one that looked easiest to try was: unload project OPIDDaily and then reload it. This was the first fix I tried and it worked! Deployment This section summarizes deployment to AppHarbor. Most of the information here can be found in the section on AppHarbor. There are two applications at AppHarbor: opiddaily and stagedaily. Application opiddaily is the deployment of the Visual Studio master branch of solution OPIDDaily. Application stagedaily is the deployment of the Visual Studio staging branch of solution OPIDDaily. After configuring the master remote the Visual Studio production branch can be deployed to AppHarbor by using the Git BASH Shell command git push opiddaily master AppHarbor willl automatically deploy application OPIDaily if the push results in a successful build. After AppHarbor finishes building and deploying the code, application OPIDDaily can be viewed at https://opiddaily.apphb.com After configuring the staging remote (see above) the Visual Studio staging branch can be deployed to AppHarbor by using the Git BASH Shell command git push stagedaily staging AppHarbor will not automatically deploy application stagedaily even if the build is successful. It is necessary to click on the Deploy button at AppHarbor to deploy a successful build of application stagedaily. This may be by design. After clicking the Deploy button at AppHarbor to deploy a successful build of application stagedaily, the application can be viewed at https://stagedaily.apphb.com Although there are two applications at AppHarbor, there is only a single repository at GitHub. The name of this single repository is OPIDDaily. The repository is by default focused on the master branch of the codebase but can be switched to the staging branch by using the GitHub interface. jqGrid Almost every page of the application OPIDDaily features a grid produced by the jQuery jqGrid component. It was installed into the OPIDDaily project by using the Package Manager command: PM> Install-Package Trirand.jqGrid -Version 4.6.0 There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily. ELMAH Unhandled application errors are caught by ELMAH. Version 2.1.2 of Elamh.Mvc was installed in project OPIDDaily by using the Visual Studio NuGet package manager. By default, the ELMAH log can only be viewed on the server that hosts the application in which ELMAH is installed. To make the ELMAH log visible to a client remotely running the application, add <elmah> <security allowRemoteAccess=\"1\" /> </elmah> to the <configuration> section of file Web.config. To see ELMAH in action, modify the URL in the browser address bar to, for example, opiddaily.apphb.com/Admin/Foo This will generate an unhandled error because the MVC routing system will not be able to resolve the URL. Then go to opiddaily.apphb.com/elmah.axd to see that this error has been caught by ELMAH. On the localhost use localhost/opiddaily/elmah.axd to see the list of ELMAH errors. Installation of the Elmah.Mvc package adds the necessary DLL's and makes the necessary changes to Web.config to configure ELMAH for use. By default ELMAH will write to a database table called ELMAH_Error. The DDL Script definition of this table is found in a separate download . Download the DDL Script for MS SQL Server from the referenced web page. The script is a .SQL file which may be executed as a query inside SSMS to create table ELMAH_Error. The ELMAH log is configured by the connection string named OPidDailyConnectionString on Web.config. The value of this connection string is overwritten when the application is deployed to AppHarbor. See the Connection String section of the Database tab. The <sytem.web> section of Web.config must configure <httpHandlers> <add verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </httpHandlers> and the <system.webServer> section must configure <handlers> <add name=\"Elmah\" verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </handlers> in order for ELMAH to log both on the local IIS and on the remote server at AppHarbor. It is also necessary to set the connection string alias as described in the Connection String section of the Database tab. MkDocs This document was created using MkDocs as was the MkDocs website itself. MkDocs was installed following the guide on this page . This guide is useful for setting up the environment; however, the syntax for the file mkdocs.yml has changed from that described in the guide. The new syntax can be found at in the User Guide section of this document . An MkDocs document is a static website and can hosted by any service that supports static sites. This MkDocs document is hosted by GitHub Pages . The Brackets open source text editor was used to develop the document on the desktop. An MkDocs document uses HTML Markdown for a desktop development version of a document. GitHub provides a cheatsheet for Markdown syntax . MkDocs provides a built-in preview server. To start this server, open a BASH Shell on the folder containing the mkdoc.yml file of the project and execute mkdocs serve Then go to http://127.0.0.1:8000 in a desktop browser. Pages can be edited and saved while in preview mode. The changes will be reflected in the browser document. When it is time to publish a version of a document, in a Git BASH shell opened on the folder containing the mkdocs.yml file, issue the command mkdocs build to expand the Markdown version of the document into an HTML version into the /site folder. Then open the Git GUI on the folder containing the mkdocs.yml file and use the GUI to create a new Git repository on the local disk. Next create repository opiddailydoc to hold the documentation at GitHub. After this, in the folder containing the mkdocs.yml file, define a remote called origin for the document: git remote add origin https://github.com/tmhsplb/opiddailydoc This command references the GitHub repository opiddailydoc. The remote only needs to be defined once. It will be remembered by the Git BASH shell. In the shell issue the following commands: git add -A git commit -a -m 'Initial commit' git push origin master This will push the master branch of the document to the repository identified by the remote called origin. Then click on the Settings tab for the newly created repository and scroll down to the GitHub Pages section. Select the master branch source and click on the Save button. Finally, to view the published document go to: https://tmhsplb.github.io/opiddailydoc/site Subsequent edits only require the commands mkdocs build git commit -a -m '<Comment for new commit>' git push origin master to update repository opiddailydoc at GitHub. It may take several minutes before edits are available.","title":"Infrastructure"},{"location":"Infrastructure/#infrastructure","text":"The infrastructure of project OPIDDaily refers to the tools and technologies used to develop OPIDDaily, exclusive of the implementation itself. This section will be useful to a developer wanting to maintain and further develop OPID Daily. It describes both the desktop development environment and the AppHarbor deployment environment for application OPIDDaily.","title":"Infrastructure"},{"location":"Infrastructure/#hosting-environments","text":"There are 3 hosting environments for OPIDDaily: desktop, staging and production. They differ in the database connection string used by each. The connection string is configured as the value of variable SQLSERVER_CONNECTION_STRING in the <appSettings> section of Web.config. The static value configured there is used by the desktop environment. The static value is overwritten by injection (at AppHarbor) when OPIDDaily is deployed to create either a staging or production release. The transformation files Web.Staging.config and Web.Release.config play a role in these deployments. The staging deployment at AppHarbor has its Environment variable set to Staging to force Web.Staging.config to be used upon deployment. This is done in the Settings section of the deployed application. The production deployment at AppHarbor has its Environment variable set to Release by default. This causes Web.Release.config to be used upon deployment.","title":"Hosting Environments"},{"location":"Infrastructure/#visual-studio-project","text":"The Visual Studio 2019 (Community Edition) project representing application OPIDDaily was developed using an ASP.NET Identity 2.0 sample project developed by Syed Shanu as a starting point. The project is described in the excellent CodeProject article ASP.NET MVC Security and Creating User Role . The sample project uses the Visual Studio MVC5 project template and makes use of Katana OWIN middleware for user authentication. The use of Katana is built into the ASP.NET Identity 2.0 provider used by the project template, as is explained in the CodeProject article. On the Properties page of the Visual Studio project remember to select Local IIS as the server and click the Create Virtual Directory button to set http://localhost/OpidDaily as the Project Url. These two actions create an application called OpidDaily under the Default Web Site in IIS and enable project OpidDaily to be run in a desktop version of IIS under this Url. Without this, the desktop IIS cannot be used to host the application. See the section on configuring IIS below. Visual Studio includes the ability to view an installed SQL Server Express database, but it is more convenient to have SQL Server Management Studio (SSMS) available for this purpose. This requires a (lengthy) download. The version of SSMS used for the development of the OPIDDaily application is v18.0. Note that SQL Server Express is a separate download from SSMS; it does not come bundled with SSMS. New development in the OPIDDaily Visual Studio project will be done in the staging branch and deployed to the stagedaily application at AppHarbor. (See the section on Deployment.) After changes to the staging branch have been tested in the desktop environment, using the Visual Studio GutHub interface they will be commited and then pushed to the staging branch at GitHub. After changes have been tested, they will be merged into the master branch of the project and from there deployed to application OPIDDaily at Appharbor. When the codebase is installed on a developer's Visual Studio instance on his/her machine by cloning the GitHub repository OPIDDaily , the developer must use Visual Sutdio to create a staging branch and then rebase this branch to origin/staging . This will allow the developer to see the contents of the staging branch at GitHub. To pick up changes commited and pushed to GitHub in a remote staging branch, it is necessary to rebase the local staging branch to origin/staging . This will cause the remote changes to appear in the local staging branch without the need to Fetch and Pull them as is done between a remote master branch and a local master branch.","title":"Visual Studio Project"},{"location":"Infrastructure/#sql-server-express","text":"The desktop version of OPID Daily makes use of a SQL Server Express to store information about clients. The SQL Server Express database for OPIDDaily was created by executing the SQL query create database OPIDDailyDB executed inside of SSMS. With this database selected in SSMS, there are two SQL queries that need to be executed to enable IIS to talk to SQL Server Express. The first query is CREATE USER [NT AUTHORITY\\NETWORK SERVICE] FOR LOGIN [NT AUTHORITY\\NETWORK SERVICE] WITH DEFAULT_SCHEMA = dbo; This query creates the database user NT AUTHORITY\\NETWORK SERVICE. The second query is EXEC sp_addrolemember 'db_owner', 'NT AUTHORITY\\NETWORK SERVICE' This query grants user NT AUTHORITY\\NETWORK SERVICE the necessary permissions to communicate with IIS. These same two queries do not need to be executed in the AppHarbor database to prepare it to communicate with IIS. See below for information about the AppHarbor deployment of OPIDDaily. It is also necessary to change the application pool identity of application OPIDDaily running under IIS to NETWORKSERVICE. See the section on configuring IIS.","title":"SQL Server Express"},{"location":"Infrastructure/#entity-framework-code-first","text":"An application based on Entity Framework Code First may have multiple data contexts referencing a single database, as is the case for application OPIDDaily. In application OPIDDaily a data context is reserved for database migrations used by the ASP.NET Identity subsystem. Supporting multiple data contexts was enabled by some manual scaffolding in the codebase. In the case of the OPIDDaily application this scaffolding consisted of creating a project folder called DataContexts with two subfolders: IdentityMigrations and OPIDDailyMigrations. Also, a new folder called Entities was added to the OPIDDaily Visual Studio Solution to contain the classes defining the entities used by the solution. Before the code was run for the first time, the PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.IdentityDb -MigrationsDirectory DataContexts\\IdentityMigrations was executed. This created the two files DataContexts\\IdentityMigrations\\Configuration.cs DataContexts\\IdentityMigrations\\IdentityDB.cs which initiaized the IdentityDB data context. It is worth taking a look at these two files. In particular, the file IdentityDB.cs was edited to point to the application connection string through Config.ConnectionString. Executing the above PowerShell command allows the ASP.NET Identity system to automatically update the OPIDDailyDB with the ASP.NET Identity tables the first time the program is run. Running the program for the first time on the local IIS also automatically created the migration DataContexts\\IdentityMigrations\\201906051504117_InitialCreate.cs which specifes the code used to create the ASP.NET Identity tables. It is worth taking a look at this file. The first time the program was run, the Superadmin user, sa, was created in database OPIDDailyDB. This was done by method Startup.Configuration on the toplevel file OPIDDaily\\Startup.cs. This file specifies the user sa as the first user in role SuperAdmin (created on the file). It also points to the toplevel file Config.cs through the reference Config.SuperadminPassword, where the password of user sa is configured. This project used as its starting point the excellent CodeProject article ASP.NET MVC Security and Creating User Role It was necessary to move the class ApplicationDbContext from file Models\\IdentityModels.cs on the sample project to file DataContexts/IdentityDb.cs to make things work. The PowerShell command PM> Enable-Migrations -ContextTypeName OPIDDaily.DataContexts.OPIDDailyDB -MigrationsDirectory DataContexts\\OPIDDailyMigrations was executed to initialize the OPIDDaily data context. This created the two files DataContexts\\OPIDDailyMigrations\\Configuration.cs DataContexts\\OPIDDailyMigrations\\OPIDDailyDB.cs It is worth studying these two files. The first entity added to the OPIDDaily project was the class Entities\\Client.cs. This entity was connected to the OPIDDDaily data context by the inclusion of the declaration public DbSet<Client> Clients { get; set; } on file DataContexts\\OpidDailyDB.cs. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"Clients\" caused the migration 201906152111225_Clients.cs to be added to DataContexts\\OPIDDailyMigrations. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Clients to be created in database OPIDDailyDB by executing the Up method of the above migration. Notice that the Up method refers to two database columns, ReferralDate and AppearanceDate, which are not in the currently deployed version of table Clients. The data migration 201907122132153_RemoveToDates.cs was used to remove these columns when it was realized they would not be needed. The second entity to be added to project OPIDDaily was the class Entities\\Visit.cs. This entity was connected to the OPIDDaily data context by the inclusion of the declaration public DbSet<Visit> Visits { get; set; } on file DataContexts\\OpidDailyDB.cs. Since table Visits was intended to be related to table Clients in the OPIDDailyDB by a foreign key, the declaration public ICollection<Visit> Visits { get; set; } was added to class Entities\\Client.cs. Entity Framework Code First automatically detected this when the \"History\" migration (described next) was created. Running the PowerShell command PM> add-migration -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration \"History\" then caused the migration 2019071220006570_History.cs to be added to DataContexts\\OPIDDailyMigrations. Studying the Up method of this migration, it is seen that the new table Visits to be created will have a foreign key relationship to table Clients. Running the PowerShell command PM> update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration then caused table Visits to be created in database OPIDDailyDB by executing the Up method of the above migration. As desired, table Visits has a foreign key relationship to table Clients. To see this, use SSMS to study the columns of table Visits. Although the desired foreign key relationship exists between tables Cients and Visits, deleting a client from the Clients table does not by default perform a cascading deleted of any related records in the Visits table. The cascading delete must be performed manualy. To see how this is done, see method RemoveClients in the SuperadminController. Each additional database change requires a pair of commands: an add-migration command followed by an update-database command. Executing an add-migration command creates a .cs file in the folder associated with the ConfigurationTypeName. Study this .cs file before executing the update-database command. If the database changes indicated in the .cs file are not correct, simply delete the .cs file before running the update-database command and then try again. To generate a script for the most recent migration, go back one migration in the migration history. For example, the migration preceding the migration ExpressClient was the migration PXXA. Therefore, to get a script for migration ExpressClient, execute the command update-database -ConfigurationTypeName OPIDDaily.DataContexts.OPIDDailyMigrations.Configuration -Script -SourceMigration:PXXA The generated script can be run against the database at AppHarbor using SSMS. The script should be run before the code is updated at AppHarbor.","title":"Entity Framework Code First"},{"location":"Infrastructure/#configuring-iis","text":"Development of the OPIDDaily application was performed under IIS on the localhost machine. This was done so that the development environment would match the deployment environment at AppHarbor as closely as possible. The localhost application server, Internet Information Services (IIS), was not pre-installed on the localhost; however, it is part of the operating system that can easily be activated. To activate IIS, go to the Programs section of the Control Panel and turn on the IIS feature: Programs > Programs and Features > Turn Windows features on or off > Internet Information Services After checking this box, expand it by clicking the plus sign (+) next to it and go to the section World Wide Web Services > Application Development Features In this section, check the checkboxes for ASP ASP.NET 3.5 ASP.NET 4.6 if they are not already checked. This will cause additional Application Pools to be made available to IIS. The OPIDDaily application is installed as an application under the Default Web Site in IIS as described in the section describing the Visual Studio Project. The Basic Settings dialog box for application Eref (accessible from the Actions pane of IIS), will give the physical path to the folder containing the source code as C:\\Projects\\OpidDaily\\OpidDaily This folder contains the project solution file, OPIDaily.sln. Do not change it! Application OPIDaily must be configured to use the application pool .NET v4.5. in the Basic Settings dialog box. (This application pool became available by enabling the features described above.) Finally, change the application identity of the selected application pool (.NET v4.5) to NetworkService. To do this, highlight Application Pools on the IIS Connections panel. This will cause the available application pools to appear in the IIS body panel. Highlight the .NET v4.5 application pool and then select Set Application Pool Defaults\u2026 to display a dialog box that will enable you to change the identity of the application pool. The dialog box is reachable either from the context menu of the highlighted application pool (under right mouse click) or from the Actions panel on the right of the IIS display. The dialog box contains a section labeled Process Model which contains an entry labeled Identity. Selecting the Identity entry adds an ellipsis next to the bold ApplicationPoolIdentity. Selecting the ellipsis brings up a dialog box with the pre-selected radio button Built-in account. Select NetworkService from the dropdown menu associated with this radio button. After approving this selection, the Identity column of the application pool .NET v4.5 will show NetworkService. See the section on SQL Server Express for how to establish user NetworkService.","title":"Configuring IIS"},{"location":"Infrastructure/#git-for-windows","text":"Visual Studio 2019 (Community Edition) comes with built-in support for GitHub. A new project can be added to Git source control on the desktop by simply selecting Add to Source Control from the context menu of the Solution file in the Solution Explorer. Once a project is under Git source control it can be added to a remote GitHub repository by using tools available through Visual Studio. However, a technique preferred by many developers is to use Git for Windows . Git for Windows provides a BASH shell interface to GitHub which uses the same set of commands available at GitHub itself. Git for Windows integrates with Windows Explorer to allow a BASH shell to be opened on a project that has been added to a desktop Git repository. Simply point Windows Explorer at the folder containing the project solution file and select Git BASH Here from the context menu of the folder to open a Git for Windows BASH shell. Then execute Git commands from this shell window. Git for Windows also offers Git GUI, a graphical version of most Git command line functions. To open Git GUI simply select Git GUI Here from Windows Explorer.","title":"Git for Windows"},{"location":"Infrastructure/#github","text":"Application OPIDDaily is stored at GitHub as a repository under an account with the email address peter3418@ymail.com and account name tmhsplb. Only user tmhsplb can deploy directly to this repository. Any other user needing to deploy a version of OPIDDaily to this repository must be declared a collaborator on repository OPIDDaily by user tmhsplb. A collaborator is a user associated with a different account established at GitHub. Git for Windows was used to create a remote to save to this GitHub account. The remote was created in the Git BASH shell by opening the shell on the folder which contains the OPIDDaily.sln file (folder C:/Projects/OPIDDaily ) and issuing the command git remote add origin https://github.com/tmhsplb/opiddaily.git Creating this remote only needs to be done once, because Git for Windows stores the remote. To remove a remote use the command git remote rm <remote> The need for this may arise if there was a typo in the creation of .","title":"GitHub"},{"location":"Infrastructure/#appharbor","text":"AppHarbor (appharbor.com) is a Platform as a Service Provider which uses Amazon Web Services infrastructure for hosting applications and Git as a versioning tool. When an application is defined at AppHarbor, a Git repository is created to manage versions of the application's deployment. The OPIDaily application is defined as an application at AppHarbor to create the production repository of the desktop application. The staging version of the desktop application is defined by a repository called stagedaily. The remote configured for OPIDDaily at AppHarbor is: https://tmhsplbt@appharbor.com/opiddaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/opiddaily.git After the remote is configured in the Git BASH shell, issuing the command git push opiddaily master will deploy the master branch of opiddaily to AppHarbor as application OPIDDaily, accessible through the URL https://opiddaily.apphb.com If you reset your password at AppHarbor, the 'git push' command will no longer work from the Git BASH shell. You need to have git prompt you for your new password. To do this on a Windows 10 machine, go to Control Panel > User Accounts > Credential Manager > Windows Credentials and remove the AppHarbor entry under Generic Credentials. The next time you push, you will be prompted for your repository password. An application such as OPIDDaily deployed using the free Canoe subscription level at AppHarbor has limitations that make it unsuitable for production use. Under the Canoe subscription, the IIS application pool of application OPIDDaily has a 20 minute timeout, which forces OPIIDDaily to spin up its resources again after 20 minutes of idle time. The free Yocto version of SQL Server is used as an add-on to the OPIDDaily deployment. The Yocto version is free but has a limit of 20MB of storage space, which is adequate for development purposes. A staging version of application OPIDDaily was created by creating an application called stagedaily at AppHarbor. DO NOT CREATE A SEPARATE REPOSITORY FOR STAGEDAILY AT GITHUB. The remote configured for stagedaily at AppHarbor is: https://tmhsplbt@appharbor.com/stagedaily.git This remote is configured from a Windows Git BASH shell by the command git remote add stagedaily https://tmhsplb@appharbor.com/stagedaily.git After the remote is configured in the Git BASH shell, issuing the command git push stagedaily staging will deploy the staging branch of OPIDDaily to AppHarbor as application stagedaily, accessible through the URL https://stagedaily.apphb.com All the tables created by Entity Framework migrations magically appeared in the staging version. The magic was probably caused by deployment of the codebase of the staging branch to AppHarbor. This branch contains all the migrations used by the master branch. However, there was one table missing:the Invitations table. This table is not included in any migration, so it has to be added manually to the staging database. This is a simple matter of using SSMS to script the table and executing the script (in SSMS) against the staging database. The scripts that needed to run on the desktop to establish the connection between Visual Studio and the desktop SQL Server did not need to be run against the staging database to establish communication with the AppHarbor server. It is possible to use AppHarbor to generate a custom domain name for an application, but this has not been done for the OPIDDaily application. On June 6, 2019 I ran into a problem when I first pushed my OPIDDaily solution from my laptop to its GitHub repository and tried to pull the resulting solution onto my desktop computer. When I tried to run the solution from my desktop it complained about missing part of the path /bin/roslyn/csc.exe. I found a fix that worked at StackOverflow https://stackoverflow.com/questions/32780315/could-not-find-a-part-of-the-path-bin-roslyn-csc-exe There were many proposed fixes, but the one that looked easiest to try was: unload project OPIDDaily and then reload it. This was the first fix I tried and it worked!","title":"AppHarbor"},{"location":"Infrastructure/#deployment","text":"This section summarizes deployment to AppHarbor. Most of the information here can be found in the section on AppHarbor. There are two applications at AppHarbor: opiddaily and stagedaily. Application opiddaily is the deployment of the Visual Studio master branch of solution OPIDDaily. Application stagedaily is the deployment of the Visual Studio staging branch of solution OPIDDaily. After configuring the master remote the Visual Studio production branch can be deployed to AppHarbor by using the Git BASH Shell command git push opiddaily master AppHarbor willl automatically deploy application OPIDaily if the push results in a successful build. After AppHarbor finishes building and deploying the code, application OPIDDaily can be viewed at https://opiddaily.apphb.com After configuring the staging remote (see above) the Visual Studio staging branch can be deployed to AppHarbor by using the Git BASH Shell command git push stagedaily staging AppHarbor will not automatically deploy application stagedaily even if the build is successful. It is necessary to click on the Deploy button at AppHarbor to deploy a successful build of application stagedaily. This may be by design. After clicking the Deploy button at AppHarbor to deploy a successful build of application stagedaily, the application can be viewed at https://stagedaily.apphb.com Although there are two applications at AppHarbor, there is only a single repository at GitHub. The name of this single repository is OPIDDaily. The repository is by default focused on the master branch of the codebase but can be switched to the staging branch by using the GitHub interface.","title":"Deployment"},{"location":"Infrastructure/#jqgrid","text":"Almost every page of the application OPIDDaily features a grid produced by the jQuery jqGrid component. It was installed into the OPIDDaily project by using the Package Manager command: PM> Install-Package Trirand.jqGrid -Version 4.6.0 There is a collection of jqGrid Demos that was very helpful during the development of OPIDDaily.","title":"jqGrid"},{"location":"Infrastructure/#elmah","text":"Unhandled application errors are caught by ELMAH. Version 2.1.2 of Elamh.Mvc was installed in project OPIDDaily by using the Visual Studio NuGet package manager. By default, the ELMAH log can only be viewed on the server that hosts the application in which ELMAH is installed. To make the ELMAH log visible to a client remotely running the application, add <elmah> <security allowRemoteAccess=\"1\" /> </elmah> to the <configuration> section of file Web.config. To see ELMAH in action, modify the URL in the browser address bar to, for example, opiddaily.apphb.com/Admin/Foo This will generate an unhandled error because the MVC routing system will not be able to resolve the URL. Then go to opiddaily.apphb.com/elmah.axd to see that this error has been caught by ELMAH. On the localhost use localhost/opiddaily/elmah.axd to see the list of ELMAH errors. Installation of the Elmah.Mvc package adds the necessary DLL's and makes the necessary changes to Web.config to configure ELMAH for use. By default ELMAH will write to a database table called ELMAH_Error. The DDL Script definition of this table is found in a separate download . Download the DDL Script for MS SQL Server from the referenced web page. The script is a .SQL file which may be executed as a query inside SSMS to create table ELMAH_Error. The ELMAH log is configured by the connection string named OPidDailyConnectionString on Web.config. The value of this connection string is overwritten when the application is deployed to AppHarbor. See the Connection String section of the Database tab. The <sytem.web> section of Web.config must configure <httpHandlers> <add verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </httpHandlers> and the <system.webServer> section must configure <handlers> <add name=\"Elmah\" verb=\"POST,GET,HEAD\" path=\"elmah.axd\" type=\"Elmah.ErrorLogPageFactory, Elmah\" /> </handlers> in order for ELMAH to log both on the local IIS and on the remote server at AppHarbor. It is also necessary to set the connection string alias as described in the Connection String section of the Database tab.","title":"ELMAH"},{"location":"Infrastructure/#mkdocs","text":"This document was created using MkDocs as was the MkDocs website itself. MkDocs was installed following the guide on this page . This guide is useful for setting up the environment; however, the syntax for the file mkdocs.yml has changed from that described in the guide. The new syntax can be found at in the User Guide section of this document . An MkDocs document is a static website and can hosted by any service that supports static sites. This MkDocs document is hosted by GitHub Pages . The Brackets open source text editor was used to develop the document on the desktop. An MkDocs document uses HTML Markdown for a desktop development version of a document. GitHub provides a cheatsheet for Markdown syntax . MkDocs provides a built-in preview server. To start this server, open a BASH Shell on the folder containing the mkdoc.yml file of the project and execute mkdocs serve Then go to http://127.0.0.1:8000 in a desktop browser. Pages can be edited and saved while in preview mode. The changes will be reflected in the browser document. When it is time to publish a version of a document, in a Git BASH shell opened on the folder containing the mkdocs.yml file, issue the command mkdocs build to expand the Markdown version of the document into an HTML version into the /site folder. Then open the Git GUI on the folder containing the mkdocs.yml file and use the GUI to create a new Git repository on the local disk. Next create repository opiddailydoc to hold the documentation at GitHub. After this, in the folder containing the mkdocs.yml file, define a remote called origin for the document: git remote add origin https://github.com/tmhsplb/opiddailydoc This command references the GitHub repository opiddailydoc. The remote only needs to be defined once. It will be remembered by the Git BASH shell. In the shell issue the following commands: git add -A git commit -a -m 'Initial commit' git push origin master This will push the master branch of the document to the repository identified by the remote called origin. Then click on the Settings tab for the newly created repository and scroll down to the GitHub Pages section. Select the master branch source and click on the Save button. Finally, to view the published document go to: https://tmhsplb.github.io/opiddailydoc/site Subsequent edits only require the commands mkdocs build git commit -a -m '<Comment for new commit>' git push origin master to update repository opiddailydoc at GitHub. It may take several minutes before edits are available.","title":"MkDocs"},{"location":"database/","text":"Database OpidDaily is a database driven application built using SQL Server technology. In the desktop environment OpidDaily is built using the Sql Server Express database engine. In the online environment at AppHarbor a full SQL Server is used. The two versions are compatible with each other with respect to the database features used. SQL Server Management Studio v18.0 (SSMS) was used to manage both database engines. In the desktop environment, Windows Authentication is used to connect to the Sql Server Express database. In the online enviroment, SQL Server Authentication is used to connect to the SQL Server database. When application OpidDaily was created at AppHarbor, a free version of SQL Server was added on through the AppHarbor interface. After application OpidDaily was deployed at AppHarbor it was run for the first time without knowledge of the database connection string. This worked because the connection string configured as the value of SQLSERVER_CONNECTION_STRING configured in the <appSettings> section of Web.config is overwritten with the connection string for the add-on SQL Server at application deploy time. When OPIDDaily was accessed through https://opiddaily.apphb.com the ASP.NET Idenetity 2.0 tables were created in the database. The Login screen that appeared was then used to login the SuperAdmin (sa) with the password configured on Startup.cs. This served up the Home view of the SuperadminController, which reported the connection string. Connection String In the desktop environment, SSMS was used to create an empty project database by executing the SQL query create database OpidDailyDB The Visual Studio Server Explorer (found under the Eref project View menu) was then used to discover the connection string to database OpidDailyDB by creating a new Data Connection to it and copying the Connection String property of the data connection as the value of the variable SQLSERVER_CONNECTION_STRING in the <appSettings> section of Web.config. This setting is read programatically by the constructors on IdentityDb.cs and OpidDailyDB.cs. See the code base. Getting the value of the connection string programatically eliminates the need to configure the connection string in the usual place, the <connectionStrings> section of Web.config. The online version of OpidDaily is hosted as an application at AppHarbor and it uses a database server provided as an add-on. The add-on database server includes a database which serves as the application database, so it is not necessary to create the application database as was done above for the desktop version. The connection string of the SQL Server instance at AppHarbor is found under the Configuration Variables section for the OPIDDaily application at AppHarbor; however, the Configuration Variables are not fully visble and cannot be copied. To discover the full values of the Configuration Variables for the connection string for application OPIDDaily, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link \"Go to SQL Server\" on the page that appears. The Configuration Variables section states that the configuration variables should be accessed programatically, since the values may be updated by the add-on provider without notice. An AppHarbor knowledge base article explains that the connection string is injected as the value of SQLSERVER_CONNECTION_STRING into the <appSettings> section of Web.config at application deploy time. This injection overwrites the statically configured value used by the desktop version as mentioned above. To report the value of the connection string in the staging and production environments, the SuperadminController was modified to report the value stored in <appSettings> . Each time the SuperAdmin logs in, the value will be displayed by view ~/Views/Superadmin/Home.cshtml. Knowing this value allows a connection to the OpidDaily database to be made through SSMS using SQL Server Authentication. ELMAH uses the configuration string used by the OPIDDaily application. This is accomplished by configuring the connection string OpidDailyConnectionString in the <connectionStrings> section on Web.config and setting the connection string alias for the SQL Server add-on at AppHarbor to be OPidDailyConnectionString . To set this alias, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link \"Go to SQL Server\" on the page that appears. Click the button labeled \"Edit database configuration\" to set OpidDailyConnectionString as the alias value for the connection string. When this is done, OpidDailyConnectionString will appear as the value of SQLSERVER_CONNECTION_STRING_ALIAS in the Configuration variables section of application OPIDDaily at AppHarbor. When application OPIDDaily is deployed to AppHarbor, this alias will overwrite the configured value on file Web.config by the value the connection string for the AppHarbor database. This is explained in the same knowledge base article referenced above. Database Diagram The diagram was created by SSMS and then pasted into an MS Word Document. It is represented as a picture in the Word document. Clicking on the picture in the Word document selects it. When the picture is selected, the context menu (right click) will contain the item Save as Picture ... which enables the picture to be saved as a .PNG file. The 3 tables in the upper left of the above diagram are created by ASP.NET Identity 2.0 to manage registered users of Eref. The 3 tables are managed by their own data context which cannot be augmented by additional tables. However, data fields can be added to table AspNetUsers to connect it to tables created in a separate data context. This is what has been done for project Eref. The data fields AgencyID and NowServing have been added to table AspNetUsers to connect it to the 4 tables Agencies , Invitations , Clients and Visits , which are the data tables used to manage Eref referrals. These 4 tables are managed by their own data context. The NowServing field of the AspNetUsers table is used to keep track of the client currently being served by an Eref user. The usage of this data field is explained in the Implementation tab. The 2 data contexts of project Eref are referred to as IdentityDb and ReferralsDB. (See the section Entity Framework Code First of the Infrastructure tab.) The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video . The tables in the diagram used to manage Eref referrals were created using a script file. The technique for creating a script file is described in the Getting a SQL Script section of this article . The link is broken. Maybe it said to run: PM> update-database -Script -SourceMigration:0 to generate a SQL script to be run at AppHarbor. This was found in this article at StackOverflow. The generated script includes the Entity Framework MigrationHistory table. Using SSMS to run the generated script against the AppHarbor database worked application OpID! This technique is used to keep the deployd versions of the Eref database in synch with the development version. According to this article, the command PM> update-database -ConfigurationTypeName Eref.DataContexts.ReferralMigrations.Configuration -Script -SourceMigration $InitialDatabase will create a script file necessary to create the tables for Eref referrals using all the migrations applied since the initial migration. Notice that there is no colon (:) following -SourceMigration in this command. To generate a script to run the Down methods of migrations do, for example, PM> Update-Database -ConfigurationTypeName Eref.DataContexts.ReferralMigrations.Configuration -Script -TargetMigration: AgencyInfo Notice that there is a colon (:) following -TargetMigration in this command. This command will create a script to run the Down methods of all migrations since (and not including) migration AgencyInfo. It is important to be able to generate this script if changes need to be backed out, because the deployed versions of application Eref do not use Entity Framework to manage the database. Managing Users Eref is a role based database application administered by a Superadmin. The Superadmin has the responsibilty of establishing a login account for each Eref user to prevent a user from specifying his/her own role. Most users will be in the role of Client Advocate, which has been established by the Superadmin. The Superadmin will be given a user name for a new user together with the agency the user works for. For example, if Mary Atwood who works for the Salvation Army would like to become an Eref Client Advocate under the user name Mary, this request would be presented to the Superadmin user at Main Street Ministries. Provided that the user name Mary is not already in use, the Superadmin user would use a private interface to enter Mary Atwood in the Invitations table under UserName Mary with FullName Mary Atwood. Through the private interface the Superadmin would also enter an email address for Mary and select the Salvation Army as the agency that she represents and select Client Advocate as her role. When saved through the private interface, this will create a record in the Invitations table which is in effect an invitation for Mary Atwood to register under user name Mary. The Superadmin will notify Mary that her account has been created and that she may register for its use using user name Mary, the email address she has provided and a password of her choosing. When Mary registers, the user name and email address she provides will be checked against the Invitations table. If this pair of identifiers is not found in the Invitations table, Mary's attempt to register will be rejected. IF they are found, a record will be created for her in the AspNetUsers table using the password she has specified and using the role of Client Advocate from the Invitations table. The AgencyID field of the created record (obtained from the Invitations table) will point to the record representing the Salvation Army in the Agencies table. On subsequent visits to Eref, Mary may simply login with the credentials established by her registration. When logged in she will be recognized as a Client Advocate for the Salvation Army and will see only records of clients served by the Salvation Army. Database Utilization SSMS can be used to check on the utilization of a datbase. To do so: Right click a database name Navigate to Reports > Standard Reports > Disk Usage This is important with respect to a free AppHarbor database to make sure the disk limiits are not exceeded. When restoring the database of checks for OpID from a backup file, 21,500 checks consumd 5.25MB out of an allowed 13.44MB. So there is room for many more checks. Restoration of this file seemd to generate a timeout error. But despite the error, the file was eventually restored. Don't know how long it actually took to restore. The timeout occurred within a couple of minutes of starting the restore.","title":"Database"},{"location":"database/#database","text":"OpidDaily is a database driven application built using SQL Server technology. In the desktop environment OpidDaily is built using the Sql Server Express database engine. In the online environment at AppHarbor a full SQL Server is used. The two versions are compatible with each other with respect to the database features used. SQL Server Management Studio v18.0 (SSMS) was used to manage both database engines. In the desktop environment, Windows Authentication is used to connect to the Sql Server Express database. In the online enviroment, SQL Server Authentication is used to connect to the SQL Server database. When application OpidDaily was created at AppHarbor, a free version of SQL Server was added on through the AppHarbor interface. After application OpidDaily was deployed at AppHarbor it was run for the first time without knowledge of the database connection string. This worked because the connection string configured as the value of SQLSERVER_CONNECTION_STRING configured in the <appSettings> section of Web.config is overwritten with the connection string for the add-on SQL Server at application deploy time. When OPIDDaily was accessed through https://opiddaily.apphb.com the ASP.NET Idenetity 2.0 tables were created in the database. The Login screen that appeared was then used to login the SuperAdmin (sa) with the password configured on Startup.cs. This served up the Home view of the SuperadminController, which reported the connection string.","title":"Database"},{"location":"database/#connection-string","text":"In the desktop environment, SSMS was used to create an empty project database by executing the SQL query create database OpidDailyDB The Visual Studio Server Explorer (found under the Eref project View menu) was then used to discover the connection string to database OpidDailyDB by creating a new Data Connection to it and copying the Connection String property of the data connection as the value of the variable SQLSERVER_CONNECTION_STRING in the <appSettings> section of Web.config. This setting is read programatically by the constructors on IdentityDb.cs and OpidDailyDB.cs. See the code base. Getting the value of the connection string programatically eliminates the need to configure the connection string in the usual place, the <connectionStrings> section of Web.config. The online version of OpidDaily is hosted as an application at AppHarbor and it uses a database server provided as an add-on. The add-on database server includes a database which serves as the application database, so it is not necessary to create the application database as was done above for the desktop version. The connection string of the SQL Server instance at AppHarbor is found under the Configuration Variables section for the OPIDDaily application at AppHarbor; however, the Configuration Variables are not fully visble and cannot be copied. To discover the full values of the Configuration Variables for the connection string for application OPIDDaily, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link \"Go to SQL Server\" on the page that appears. The Configuration Variables section states that the configuration variables should be accessed programatically, since the values may be updated by the add-on provider without notice. An AppHarbor knowledge base article explains that the connection string is injected as the value of SQLSERVER_CONNECTION_STRING into the <appSettings> section of Web.config at application deploy time. This injection overwrites the statically configured value used by the desktop version as mentioned above. To report the value of the connection string in the staging and production environments, the SuperadminController was modified to report the value stored in <appSettings> . Each time the SuperAdmin logs in, the value will be displayed by view ~/Views/Superadmin/Home.cshtml. Knowing this value allows a connection to the OpidDaily database to be made through SSMS using SQL Server Authentication. ELMAH uses the configuration string used by the OPIDDaily application. This is accomplished by configuring the connection string OpidDailyConnectionString in the <connectionStrings> section on Web.config and setting the connection string alias for the SQL Server add-on at AppHarbor to be OPidDailyConnectionString . To set this alias, select the SQL Server add-on for application OPIDDaily at AppHarbor and then follow the link \"Go to SQL Server\" on the page that appears. Click the button labeled \"Edit database configuration\" to set OpidDailyConnectionString as the alias value for the connection string. When this is done, OpidDailyConnectionString will appear as the value of SQLSERVER_CONNECTION_STRING_ALIAS in the Configuration variables section of application OPIDDaily at AppHarbor. When application OPIDDaily is deployed to AppHarbor, this alias will overwrite the configured value on file Web.config by the value the connection string for the AppHarbor database. This is explained in the same knowledge base article referenced above.","title":"Connection String"},{"location":"database/#database-diagram","text":"The diagram was created by SSMS and then pasted into an MS Word Document. It is represented as a picture in the Word document. Clicking on the picture in the Word document selects it. When the picture is selected, the context menu (right click) will contain the item Save as Picture ... which enables the picture to be saved as a .PNG file. The 3 tables in the upper left of the above diagram are created by ASP.NET Identity 2.0 to manage registered users of Eref. The 3 tables are managed by their own data context which cannot be augmented by additional tables. However, data fields can be added to table AspNetUsers to connect it to tables created in a separate data context. This is what has been done for project Eref. The data fields AgencyID and NowServing have been added to table AspNetUsers to connect it to the 4 tables Agencies , Invitations , Clients and Visits , which are the data tables used to manage Eref referrals. These 4 tables are managed by their own data context. The NowServing field of the AspNetUsers table is used to keep track of the client currently being served by an Eref user. The usage of this data field is explained in the Implementation tab. The 2 data contexts of project Eref are referred to as IdentityDb and ReferralsDB. (See the section Entity Framework Code First of the Infrastructure tab.) The technique for establishing a single connection string over 2 data contexts is described in Scott Allen's Pluralsight video . The tables in the diagram used to manage Eref referrals were created using a script file. The technique for creating a script file is described in the Getting a SQL Script section of this article . The link is broken. Maybe it said to run: PM> update-database -Script -SourceMigration:0 to generate a SQL script to be run at AppHarbor. This was found in this article at StackOverflow. The generated script includes the Entity Framework MigrationHistory table. Using SSMS to run the generated script against the AppHarbor database worked application OpID! This technique is used to keep the deployd versions of the Eref database in synch with the development version. According to this article, the command PM> update-database -ConfigurationTypeName Eref.DataContexts.ReferralMigrations.Configuration -Script -SourceMigration $InitialDatabase will create a script file necessary to create the tables for Eref referrals using all the migrations applied since the initial migration. Notice that there is no colon (:) following -SourceMigration in this command. To generate a script to run the Down methods of migrations do, for example, PM> Update-Database -ConfigurationTypeName Eref.DataContexts.ReferralMigrations.Configuration -Script -TargetMigration: AgencyInfo Notice that there is a colon (:) following -TargetMigration in this command. This command will create a script to run the Down methods of all migrations since (and not including) migration AgencyInfo. It is important to be able to generate this script if changes need to be backed out, because the deployed versions of application Eref do not use Entity Framework to manage the database.","title":"Database Diagram"},{"location":"database/#managing-users","text":"Eref is a role based database application administered by a Superadmin. The Superadmin has the responsibilty of establishing a login account for each Eref user to prevent a user from specifying his/her own role. Most users will be in the role of Client Advocate, which has been established by the Superadmin. The Superadmin will be given a user name for a new user together with the agency the user works for. For example, if Mary Atwood who works for the Salvation Army would like to become an Eref Client Advocate under the user name Mary, this request would be presented to the Superadmin user at Main Street Ministries. Provided that the user name Mary is not already in use, the Superadmin user would use a private interface to enter Mary Atwood in the Invitations table under UserName Mary with FullName Mary Atwood. Through the private interface the Superadmin would also enter an email address for Mary and select the Salvation Army as the agency that she represents and select Client Advocate as her role. When saved through the private interface, this will create a record in the Invitations table which is in effect an invitation for Mary Atwood to register under user name Mary. The Superadmin will notify Mary that her account has been created and that she may register for its use using user name Mary, the email address she has provided and a password of her choosing. When Mary registers, the user name and email address she provides will be checked against the Invitations table. If this pair of identifiers is not found in the Invitations table, Mary's attempt to register will be rejected. IF they are found, a record will be created for her in the AspNetUsers table using the password she has specified and using the role of Client Advocate from the Invitations table. The AgencyID field of the created record (obtained from the Invitations table) will point to the record representing the Salvation Army in the Agencies table. On subsequent visits to Eref, Mary may simply login with the credentials established by her registration. When logged in she will be recognized as a Client Advocate for the Salvation Army and will see only records of clients served by the Salvation Army.","title":"Managing Users"},{"location":"database/#database-utilization","text":"SSMS can be used to check on the utilization of a datbase. To do so: Right click a database name Navigate to Reports > Standard Reports > Disk Usage This is important with respect to a free AppHarbor database to make sure the disk limiits are not exceeded. When restoring the database of checks for OpID from a backup file, 21,500 checks consumd 5.25MB out of an allowed 13.44MB. So there is room for many more checks. Restoration of this file seemd to generate a timeout error. But despite the error, the file was eventually restored. Don't know how long it actually took to restore. The timeout occurred within a couple of minutes of starting the restore.","title":"Database Utilization"}]}